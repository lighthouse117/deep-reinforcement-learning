{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# フードバンクにおける食品分配問題\n",
    "深層強化学習の適用\n",
    "\n",
    "DQN (Deep Q-Network)\n",
    "\n",
    "ニューラルネットワークにはPyTorchを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 状態定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 状態定義\n",
    "from enum import IntEnum\n",
    "\n",
    "# 在庫の残り\n",
    "class StockRemaining(IntEnum):\n",
    "    FULL = 0\n",
    "    MANY = 1\n",
    "    FEW = 2\n",
    "    NONE = 3\n",
    "\n",
    "# 在庫の変動\n",
    "class StockChange(IntEnum):\n",
    "    NONE = 0\n",
    "    SLIGHTLY = 1\n",
    "    SOMEWHAT = 2\n",
    "    GREATLY = 3\n",
    "\n",
    "# エージェントの充足度\n",
    "class Satisfaction(IntEnum):\n",
    "    HARDLY = 0\n",
    "    SOMEWHAT = 1\n",
    "    COMLETELY = 2\n",
    "    OVERLY = 3\n",
    "\n",
    "# 終端状態の区別用\n",
    "class Progress(IntEnum):\n",
    "    ONGOING = 0\n",
    "    DONE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用パラメータ\n",
    "NUM_EPISODES = 1001\n",
    "MAX_STEPS = 30\n",
    "GREEDY_CYCLE = 50\n",
    "\n",
    "GAMMA = 0.98\n",
    "\n",
    "INITIAL_EPSILON = 1.0\n",
    "MINIMUM_EPSILON = 1.0\n",
    "EPSILON_DELTA = (INITIAL_EPSILON - MINIMUM_EPSILON) / (NUM_EPISODES * 0.95)\n",
    "\n",
    "ALPHA = 0.0001\n",
    "# INITIAL_ALPHA = 0.1\n",
    "# MINIMUM_ALPHA = 0.0001\n",
    "# ALPHA_DELTA = (INITIAL_ALPHA - MINIMUM_ALPHA) / (NUM_EPISODES * 0.95)\n",
    "\n",
    "HIDDEN_SIZE = 32\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 100\n",
    "\n",
    "\n",
    "\n",
    "# 環境設定\n",
    "AGENTS_COUNT = 1\n",
    "FOODS = [5, 5, 5]\n",
    "NUM_FOODS = len(FOODS)\n",
    "REQUESTS = [\n",
    "    [1, 3, 5],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 経験再生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 経験を保存するメモリクラスを定義\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    Experience Replay(経験再生)を実装するクラス\n",
    "    学習データを一定数メモリに保存し、ランダムにサンプリングすることで\n",
    "    時系列の相関をなくすことができる\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"メモリを初期化\"\"\"\n",
    "        # 状態遷移を保持するバッファ\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        # deque: double-ended queue （両端キュー）\n",
    "        # 両サイドからデータを取り出したり追加できる\n",
    "        # リストに比べて先頭・末尾の操作がO(1)の計算量で済む\n",
    "        # 最大要素数のcapacityを超えると古い要素から削除される\n",
    "\n",
    "    def push(self, state, action, state_next, reward):\n",
    "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
    "        \n",
    "        tensor_state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        tensor_state_next = torch.tensor(state_next, dtype=torch.float).unsqueeze(0)\n",
    "        tensor_action = torch.tensor([action], dtype=torch.long).unsqueeze(0)\n",
    "        tensor_reward = torch.tensor([reward], dtype=torch.long)\n",
    "\n",
    "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
    "        t = Transition(tensor_state, tensor_action, tensor_state_next, tensor_reward)\n",
    "        self.memory.append(t)\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントが持つ脳\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions, f):\n",
    "        self.f = f\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('fc1', nn.Linear(num_states, HIDDEN_SIZE))\n",
    "        self.model.add_module('relu1', nn.ReLU())\n",
    "        self.model.add_module('fc2', nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE))\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(HIDDEN_SIZE, num_actions))\n",
    "\n",
    "        print(self.model)  # ネットワークの形を出力\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=ALPHA)\n",
    "\n",
    "    \n",
    "\n",
    "    def replay(self, greedy):\n",
    "        '''Experience Replayでネットワークの結合パラメータをミニバッチ学習'''\n",
    "\n",
    "        # メモリサイズがミニバッチより小さい間は何もしない\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # 各変数をミニバッチに対応する形に変形\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        # print(f\"batch.state: {batch.state}\")\n",
    "        # print(f\"batch.action: {batch.action}\")\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # 終端状態でない次状態のマスク（終端でない所がTrueになる）\n",
    "        # non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "        # 終端状態でない次状態を抜き出す\n",
    "        # non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        # 終端状態でない次状態のマスク（終端でない所がTrueになる）\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s.numpy()[0][-1] != Progress.DONE, batch.next_state)))\n",
    "        # 終端状態でない次状態を抜き出す\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s.numpy()[0][-1] != Progress.DONE])\n",
    "\n",
    "        # print(test_states)\n",
    "        # if len(test_states) != BATCH_SIZE:\n",
    "        #     print(len(test_states))\n",
    "\n",
    "        # ネットワークを推論モードに切り替える\n",
    "        self.model.eval()\n",
    "\n",
    "        # print(f\"state_batch: {state_batch}\")\n",
    "        # print(f\"出力: {self.model(state_batch)}\")\n",
    "        # print(f\"action_batch: {action_batch}\")\n",
    "        # print(f\"reward_batch: {reward_batch}\")\n",
    "\n",
    "        # 現在のQ値（NNの出力）\n",
    "        current_q_values = torch.gather(self.model(state_batch), 1, action_batch)\n",
    "        # print(f\" current_q_values: {current_q_values.max()}\")\n",
    "\n",
    "        \n",
    "        # 次状態のQ値の最大値\n",
    "        next_max_q_values = torch.zeros(BATCH_SIZE) # すべて0にしておく\n",
    "        # 終端状態でないインデックスに、次状態の最大Q値を挿入\n",
    "        next_max_q_values[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # print(f\"次状態の出力: {self.model(state_next_batch)}\")\n",
    "        # print(f\"次状態の最大Q値: {next_max_q_values}\")\n",
    "\n",
    "        # 教師データとなるQ値の目標値\n",
    "        # （報酬 + 割引率 * 次状態の最大Q値）\n",
    "        target_q_values = (reward_batch + GAMMA * next_max_q_values).unsqueeze(1)\n",
    "\n",
    "        # next_states = torch.cat(batch.next_state).numpy()\n",
    "        # test_index = np.where(next_states[-1] != Progress.DONE)\n",
    "        # target_final = target_q_values[test_index]\n",
    "        # if target_final.max() > 0:\n",
    "        #     print(target_final.max())\n",
    "\n",
    "        # max_value = target_q_values.max()\n",
    "        # if max_value > 0:\n",
    "        #     print(max_value)\n",
    "\n",
    "        # if (max_value > 100):\n",
    "        #     print(f\"state_batch: {state_batch}\")\n",
    "        #     print(f\"action_batch: {action_batch}\")\n",
    "        #     print(f\"reward_batch: {reward_batch}\")\n",
    "        #     print(f\"current_q_values: {current_q_values}\")\n",
    "        #     print(f\"次状態の最大Q値: {next_max_q_values}\")\n",
    "        #     print(f\"教師データ: {target_q_values}\")\n",
    "\n",
    "        # ネットワークを訓練モードに切り替える\n",
    "        self.model.train()\n",
    "\n",
    "        # if greedy:\n",
    "        #     diff = target_q_values - current_q_values\n",
    "        #     print(f\"TD誤差: {diff}\", file=self.f)\n",
    "\n",
    "        # 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(current_q_values, target_q_values)\n",
    "\n",
    "        # if greedy:\n",
    "        #     print(f\"TD: {target_q_values - current_q_values}\")\n",
    "            # print(f\"current_q_values: {current_q_values}\")\n",
    "            # print(f\"target_q_values: {target_q_values}\")\n",
    "\n",
    "\n",
    "        # 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "\n",
    "        \n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()  # 結合パラメータを更新\n",
    "\n",
    "    def decide_action(self, state, options, greedy, epsilon):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        # epsilon = 0.5 * (1 / (episode + 1))\n",
    "        greedy_print = greedy\n",
    "\n",
    "\n",
    "        if greedy is False:\n",
    "            # ε-greedyで行動を決定する\n",
    "            if np.random.rand() >= epsilon:\n",
    "                greedy = True\n",
    "\n",
    "        if greedy:\n",
    "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
    "\n",
    "            tensor_state = torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "            # print(state)\n",
    "            # print(tensor_state)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                q = self.model(tensor_state).numpy()\n",
    "                # print(f\"Q値: {q}\")\n",
    "                options_q = q[options]\n",
    "                max_arg = np.argmax(options_q)\n",
    "                action = options[max_arg]\n",
    "                # print(f\"action: {action}\")\n",
    "                if greedy_print:\n",
    "                    print(f\"Q値: {q} 選択した行動: {action}\", file=f)\n",
    "                \n",
    "            # ネットワークの出力の最大値のindexを取り出す\n",
    "\n",
    "        else:\n",
    "            action = random.choice(options)\n",
    "            # print(f\"ランダムな行動: {action}\")\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentクラス\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, request, num_states, num_actions,  f):\n",
    "        self.name = name\n",
    "        self.REQUEST = request\n",
    "        self.brain = Brain(num_states, num_actions, f)\n",
    "        self.f = f\n",
    "\n",
    "    def reset(self, env_stock, greedy):\n",
    "        self.current_requests = self.REQUEST.copy()\n",
    "        self.stock = np.zeros(NUM_FOODS, dtype=np.int64)\n",
    "\n",
    "        self.food_done = False\n",
    "        self.learning_done = False\n",
    "        self.old_env_stock = env_stock.copy()\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, greedy):\n",
    "        # if state == state_next:\n",
    "        #     if greedy:\n",
    "        #         print(f\"{self.name}: 状態が変化していません\", file=self.f)\n",
    "        # else:\n",
    "        #     self.brain.replay()\n",
    "        self.brain.replay(greedy)\n",
    "        \n",
    "\n",
    "\n",
    "    # 行動（どの食品を取得するか）を決定\n",
    "    def get_action(self, state, env_stock, greedy, epsilon):\n",
    "        # 行動の候補\n",
    "        action_options = []\n",
    "\n",
    "        # 本部に在庫がある食品を候補に入れる\n",
    "        for food in range(len(env_stock)):\n",
    "            if env_stock[food] != 0:\n",
    "                action_options.append(food)\n",
    "\n",
    "\n",
    "        # 「何もしない」という選択肢も候補に加える\n",
    "        action_options.append(NUM_FOODS)\n",
    "\n",
    "\n",
    "        # 行動を決定\n",
    "        action = self.brain.decide_action(state, action_options, greedy, epsilon)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward, greedy):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        if state == state_next:\n",
    "            if greedy:\n",
    "                print(f\"{self.name}: 状態が変化していません\", file=self.f)\n",
    "        else:\n",
    "            if greedy:\n",
    "                print(\"--- メモリに保存 ---\", file=self.f)\n",
    "                print(\"現在の状態: \", file=self.f)\n",
    "                self.print_state(state)\n",
    "                print(f\"とった行動: {action}\", file=self.f)\n",
    "                print(\"次の状態: \", file=self.f)\n",
    "                self.print_state(state_next)\n",
    "                print(f\"獲得した報酬: {reward}\", file=self.f)\n",
    "                print(\"-----------------\", file=self.f)\n",
    "            self.brain.memory.push(state, action, state_next, reward)\n",
    "\n",
    "\n",
    "    def observe_state(self, env_stock, episode_terminal):\n",
    "        remainings = []\n",
    "        changes = []\n",
    "        satisfactions = []\n",
    "        progress = []\n",
    "\n",
    "        granularity = len(StockRemaining) - 2\n",
    "        for amount, original in zip(env_stock, FOODS):\n",
    "            section = round(original / granularity)\n",
    "            if amount == 0:\n",
    "                remainings.append(StockRemaining.NONE)\n",
    "            elif amount < section:\n",
    "                remainings.append(StockRemaining.FEW)\n",
    "            elif amount < original:\n",
    "                remainings.append(StockRemaining.MANY)\n",
    "            else:\n",
    "                remainings.append(StockRemaining.FULL)\n",
    "\n",
    "        difference = self.old_env_stock - env_stock\n",
    "        for diff in difference:\n",
    "            if diff == 0:\n",
    "                changes.append(StockChange.NONE)\n",
    "            elif diff == 1:\n",
    "                changes.append(StockChange.SLIGHTLY)\n",
    "            elif diff == 2:\n",
    "                changes.append(StockChange.SOMEWHAT)\n",
    "            else:\n",
    "                changes.append(StockChange.GREATLY)\n",
    "\n",
    "        satisfaction_rates = self.stock / self.REQUEST\n",
    "        for rate in satisfaction_rates:\n",
    "            if rate < 0.5:\n",
    "                satisfactions.append(Satisfaction.HARDLY)\n",
    "            elif rate < 1:\n",
    "                satisfactions.append(Satisfaction.SOMEWHAT)\n",
    "            elif rate == 1:\n",
    "                satisfactions.append(Satisfaction.COMLETELY)\n",
    "            else:\n",
    "                satisfactions.append(Satisfaction.OVERLY)\n",
    "\n",
    "        if episode_terminal:\n",
    "            progress.append(Progress.DONE)\n",
    "        else:\n",
    "            progress.append(Progress.ONGOING)\n",
    "\n",
    "        state = tuple(remainings + changes + satisfactions + progress)\n",
    "\n",
    "        # state = tuple(remainings + satisfactions + progress)\n",
    "\n",
    "        self.old_env_stock = env_stock.copy()\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    def grab_food(self, food):\n",
    "        # 手元の在庫が1つ増える\n",
    "        self.stock[food] += 1\n",
    "        # 要求リストから1つ減らす\n",
    "        if (self.current_requests[food] != 0):\n",
    "            self.current_requests[food] -= 1\n",
    "\n",
    "        self.check_satisfied()\n",
    "\n",
    "    def check_satisfied(self):\n",
    "        # print(f\"要求: {self.current_requests}\")\n",
    "        if np.all(self.current_requests == 0):\n",
    "            # 要求がすべて満たされたことを記録\n",
    "            self.food_done = True\n",
    "            # print(f\"{self.name} 要求がすべて満たされました\")\n",
    "\n",
    "        # TODO: 食品ごとに調べる（要求があっても食品の在庫がない場合）\n",
    "\n",
    "    def get_violation(self):\n",
    "        diffs = self.REQUEST - self.stock\n",
    "        diff_rates = diffs / self.REQUEST * 10\n",
    "        abs_diffs = np.absolute(diff_rates)\n",
    "        violation = np.sum(abs_diffs)\n",
    "\n",
    "        # if (abs(violation) > 100):\n",
    "        #     print(f\"violation: {violation}\")\n",
    "\n",
    "        self.violation = violation\n",
    "        return violation\n",
    "\n",
    "\n",
    "    def print_state(self, state):\n",
    "        num = NUM_FOODS\n",
    "        print(f\"{self.name} State: \", end=\"\", file=self.f)\n",
    "\n",
    "        print(\"Remaining[ \", end=\"\", file=self.f)\n",
    "        for i in range(num):\n",
    "            print(f\"{state[i].name} \", end=\"\", file=self.f)\n",
    "        print(\"], Change[ \", end=\"\", file=self.f)\n",
    "        for i in range(num, num * 2):\n",
    "            print(f\"{state[i].name} \", end=\"\", file=self.f)\n",
    "        print(\"], Satisfaction[ \", end=\"\", file=self.f)\n",
    "        for i in range(num * 2, num * 3):\n",
    "            print(f\"{state[i].name} \", end=\"\", file=self.f)\n",
    "\n",
    "        print(f\"] Progress[{state[num * 3].name}]\", file=self.f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "        self.agents = self.init_agents()\n",
    "\n",
    "    def init_agents(self):\n",
    "        state_size = pow(len(StockRemaining), NUM_FOODS) * pow(len(StockChange),\n",
    "                                                                  NUM_FOODS) * pow(len(Satisfaction), NUM_FOODS) * len(Progress)\n",
    "        num_states = NUM_FOODS * 3 + 1\n",
    "        num_actions = NUM_FOODS + 1\n",
    "\n",
    "        print(\"========= 各エージェントのもつ状態行動空間 =========\", file=self.f)\n",
    "        print(f\"状態数: {state_size:,}\", file=self.f)\n",
    "        print(f\"行動数: {num_actions}\", file=self.f)\n",
    "        print(f\"状態 × 行動の組み合わせ: {(state_size * num_actions):,}\", file=self.f)\n",
    "        print(\"\\n\\n\", file=self.f)\n",
    "\n",
    "        agents: List[Agent] = []\n",
    "        for i in range(AGENTS_COUNT):\n",
    "            name = f\"Agent{i + 1}\"\n",
    "            agent = Agent(\n",
    "                name, np.array(REQUESTS[i]), num_states, num_actions, self.f)\n",
    "            agents.append(agent)\n",
    "        return agents\n",
    "\n",
    "    def reset(self, greedy):\n",
    "        self.stock = np.array(FOODS, dtype=np.int64)\n",
    "        states = None\n",
    "\n",
    "        for agent in self.agents:\n",
    "            agent.reset(self.stock, greedy)\n",
    "\n",
    "        return states\n",
    "\n",
    "    def get_actions(self, states):\n",
    "        actions = []\n",
    "        # すべてのエージェントに対して\n",
    "        for agent, state in zip(self.agents, states):\n",
    "            # 行動を決定\n",
    "            action = agent.get_action(state)\n",
    "            actions.append(action)\n",
    "            if action == NUM_FOODS:\n",
    "                # print(f\"{agent.name} 行動: 何もしない\")\n",
    "                pass\n",
    "            else:\n",
    "                # print(f\"{agent.name} 行動: 食品{action}を１つ取る\")\n",
    "                pass\n",
    "        return actions\n",
    "\n",
    "    def check_food_run_out(self):\n",
    "        # 全ての在庫が0になったかチェック\n",
    "        return np.all(self.stock == 0)\n",
    "\n",
    "    def check_agents_food_done(self):\n",
    "        # 全てのエージェントが終了条件を満たしているかチェック\n",
    "        all_done = True\n",
    "        for agent in self.agents:\n",
    "            if not agent.food_done:\n",
    "                all_done = False\n",
    "                break\n",
    "        # 全エージェントの取れる行動がなくなったか\n",
    "        return all_done\n",
    "\n",
    "    def check_agents_learning_done(self):\n",
    "        all_done = True\n",
    "        for agent in self.agents:\n",
    "            if not agent.learning_done:\n",
    "                all_done = False\n",
    "                break\n",
    "        return all_done\n",
    "\n",
    "    # def learn(self, states, actions, rewards, states_next, alpha):\n",
    "    #     for agent, state, action, reward, state_next in zip(self.agents, states, actions, rewards, states_next):\n",
    "    #         agent.learn(state, action, reward, state_next, alpha)\n",
    "\n",
    "    def get_reward(self, target_agent: Agent, terminal, greedy):\n",
    "        if terminal:\n",
    "            # - |制約違反度の平均からの偏差|\n",
    "            # violations = []\n",
    "            # for agent in self.agents:\n",
    "            #     v = agent.get_violation()\n",
    "            #     violations.append(v)\n",
    "            # mean = np.mean(violations)\n",
    "            # abs_deviation = np.absolute(mean - target_agent.get_violation())\n",
    "            # reward = - abs_deviation\n",
    "\n",
    "            # - (制約違反度 + 制約違反度の標準偏差)\n",
    "            violations = []\n",
    "            for agent in self.agents:\n",
    "                v = agent.get_violation()\n",
    "                # print(f\"violation: {v}\")\n",
    "                violations.append(v)\n",
    "            std = np.std(violations)\n",
    "            # print(f\"std: {std}\")\n",
    "            \n",
    "            reward = - (target_agent.get_violation() + std)\n",
    "            # print(f\"reward: {reward}\")\n",
    "\n",
    "            # - (制約違反度の平均+標準偏差)　統一\n",
    "            # violations = []\n",
    "            # for agent in self.agents:\n",
    "            #     v = agent.get_violation()\n",
    "            #     violations.append(v)\n",
    "            # mean = np.mean(violations)\n",
    "            # std = np.std(violations)\n",
    "            # reward = - (mean + std)\n",
    "\n",
    "            if greedy:\n",
    "                print(\n",
    "                    f\"{target_agent.name}: 報酬{reward:.3f}  要求{target_agent.REQUEST} 在庫{target_agent.stock}\", file=self.f)\n",
    "\n",
    "        else:\n",
    "            # reward = -1\n",
    "            reward = 0\n",
    "       \n",
    "        return reward\n",
    "\n",
    "    def print_env_state(self):\n",
    "        print(\"Env State: [\", end=\"\", file=self.f)\n",
    "        for status in self.env_state:\n",
    "            print(f\"{status.name} \", end=\"\", file=self.f)\n",
    "\n",
    "        print(\"]\", file=self.f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メイン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=10, out_features=32, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=32, out_features=32, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=32, out_features=4, bias=True)\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n",
      "-------------- Episode:0 (greedy) --------------\n",
      "reward: -22.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-396-331fb8007b3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mold_states\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_actions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgreedy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mepisode_terminal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-394-778471065dfb>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, greedy)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#     self.brain.replay()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgreedy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-393-ce716f6beb88>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, greedy)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# バックプロパゲーションを計算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.set_printoptions(precision=5, floatmode='maxprec_equal')\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "DIR_PATH = \"D:\\\\Lighthouse\\\\Documents\\\\Reinforcement Learning\\\\Food Distribution\\\\results\"\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "file_name_time = \"{0:%Y-%m-%d_%H%M%S}\".format(start_time)\n",
    "\n",
    "log_name = f\"log_{file_name_time}.txt\"\n",
    "log_path = os.path.join(DIR_PATH, \"logs\", log_name)\n",
    "f = open(log_path, mode=\"w\", encoding=\"UTF-8\")\n",
    "\n",
    "print(\"フードバンク食品分配 Deep Q-Network\\n\", file=f)\n",
    "\n",
    "print(\"開始時刻: {0:%Y/%m/%d %H:%M:%S}\\n\".format(start_time), file=f)\n",
    "\n",
    "print(\"====================== 環境設定 ======================\", file=f)\n",
    "print(f\"食品の種類: {NUM_FOODS}\", file=f)\n",
    "print(f\"本部の在庫: {FOODS}\", file=f)\n",
    "print(f\"エージェント数: {AGENTS_COUNT}\", file=f)\n",
    "print(f\"エージェントの要求リスト: {REQUESTS}\", file=f)\n",
    "print(\"\\n\\n\", file=f)\n",
    "\n",
    "print(\"==================== 学習パラメーター ====================\", file=f)\n",
    "print(f\"エピソード数: {NUM_EPISODES:,}\", file=f)\n",
    "print(f\"最大ステップ数: {MAX_STEPS:,}\\n\", file=f)\n",
    "print(f\"割引率: {GAMMA}\\n\", file=f)\n",
    "print(f\"学習率: {ALPHA}\", file=f)\n",
    "print(f\"εの初期値: {INITIAL_EPSILON}\", file=f)\n",
    "print(f\"εの最終値: {MINIMUM_EPSILON}\", file=f)\n",
    "print(f\"1エピソードごとのεの減少値: {EPSILON_DELTA:.7f}\", file=f)\n",
    "print(\"\\n\\n\", file=f)\n",
    "\n",
    "print(\"====================== 報酬設定 ======================\", file=f)\n",
    "print(\"- (制約違反度 + 制約違反度の標準偏差)\", file=f)\n",
    "# print(\"- |制約違反度の平均からの偏差|\", file=f)\n",
    "# print(\"- (制約違反度の平均 + 標準偏差) 統一\", file=f)\n",
    "print(\"\\n\\n\", file=f)\n",
    "\n",
    "env = Environment(f)\n",
    "\n",
    "result_episodes = []\n",
    "results_agents_y = [[], [], []]\n",
    "\n",
    "result_ave = []\n",
    "result_dev = []\n",
    "\n",
    "epsilon = INITIAL_EPSILON\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(3, 1, 1)\n",
    "ax2 = fig.add_subplot(3, 1, 2)\n",
    "ax3 = fig.add_subplot(3, 1, 3)\n",
    "\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax2.set_xlabel(\"Episode\")\n",
    "ax3.set_xlabel(\"Episode\")\n",
    "\n",
    "ax1.set_ylabel(\"Constraint Violations\")\n",
    "ax2.set_ylabel(\"Mean\")\n",
    "ax3.set_ylabel(\"Variance\")\n",
    "\n",
    "line1, = ax1.plot([], [], label=\"Agent1\")\n",
    "line2, = ax1.plot([], [], label=\"Agent2\")\n",
    "line3, = ax1.plot([], [], label=\"Agent3\")\n",
    "\n",
    "line_ave, = ax2.plot([], [])\n",
    "line_dev, = ax3.plot([], [])\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "lines = [line1, line2, line3]\n",
    "\n",
    "# 各エピソード\n",
    "for episode in range(NUM_EPISODES):\n",
    "\n",
    "    if episode % GREEDY_CYCLE == 0:\n",
    "        greedy = True\n",
    "        print(\n",
    "            f\"-------------- Episode:{episode} (greedy) --------------\")\n",
    "        print(\n",
    "            f\"\\n\\n-------------- Episode:{episode} (greedy) ----------------------------------------------------------------------\", file=f)\n",
    "    else:\n",
    "        greedy = False\n",
    "        # print(f\"-------------- Episode:{episode} --------------\")\n",
    "    states = [[], [], []]\n",
    "    actions = [[], [], []]\n",
    "    rewards = [0, 0, 0]\n",
    "\n",
    "    old_states = env.reset(greedy)\n",
    "    old_actions = []\n",
    "\n",
    "    learning_complete = False\n",
    "    step = 0\n",
    "\n",
    "    if greedy:\n",
    "        # print(f\"学習率: {alpha:.5f}\", file=f)\n",
    "        print(f\"ε: {epsilon:.5f}\", file=f)\n",
    "\n",
    "    # 各ステップ\n",
    "    while True:\n",
    "        if greedy:\n",
    "            print(f\"\\n------- Step:{step} -------\", file=f)\n",
    "\n",
    "        # 各エージェント\n",
    "        for index, agent in enumerate(env.agents):\n",
    "\n",
    "            # 終了条件確認\n",
    "            food_done = env.check_food_run_out()\n",
    "            all_agents_done = env.check_agents_food_done()\n",
    "            exceed_max_step = step == MAX_STEPS - 1\n",
    "\n",
    "            episode_terminal = food_done or all_agents_done or exceed_max_step\n",
    "\n",
    "            if greedy:\n",
    "                print(f\"\\n本部の在庫: {env.stock}\", file=f)\n",
    "                print(f\"{agent.name}の在庫:{agent.stock}\", file=f)\n",
    "                diff = agent.old_env_stock - env.stock\n",
    "                print(f\"{agent.name}視点の本部在庫の変化:{diff}\", file=f)\n",
    "                if episode_terminal:\n",
    "                    print(\"*** 終了条件を満たしています ***\", file=f)\n",
    "\n",
    "            # 状態を観測\n",
    "            state = agent.observe_state(env.stock, episode_terminal)\n",
    "            states[index] = state\n",
    "\n",
    "            if greedy:\n",
    "                agent.print_state(state)\n",
    "\n",
    "            # 行動を決定\n",
    "            if not episode_terminal:\n",
    "                action = agent.get_action(\n",
    "                    state, env.stock, greedy, epsilon)\n",
    "                actions[index] = action\n",
    "\n",
    "                if greedy:\n",
    "                    if action == NUM_FOODS:\n",
    "                        action_string = \"何もしない\"\n",
    "                    else:\n",
    "                        action_string = f\"食品{action}\"\n",
    "\n",
    "                    print(\n",
    "                        f\"{agent.name} 選択した行動: {action_string}\", file=f)\n",
    "\n",
    "                # 行動をとる\n",
    "                if action != NUM_FOODS:\n",
    "                    # エージェントが食品を1つとる\n",
    "                    agent.grab_food(action)\n",
    "                    # 本部の在庫が1つ減る\n",
    "                    env.stock[action] -= 1\n",
    "\n",
    "            reward = env.get_reward(agent, episode_terminal, greedy)\n",
    "\n",
    "            if old_states is not None:\n",
    "                agent.memorize(old_states[index], old_actions[index], state, reward, greedy)\n",
    "                agent.learn(greedy)\n",
    "\n",
    "            if episode_terminal:\n",
    "                if greedy:\n",
    "                    print(f\"{agent.name} 学習終了\\n\", file=f)\n",
    "\n",
    "                rewards[index] = reward\n",
    "                agent.learning_done = True\n",
    "                learning_complete = env.check_agents_learning_done()\n",
    "                if learning_complete:\n",
    "                    break\n",
    "\n",
    "        if exceed_max_step and greedy:\n",
    "            print(\"最大ステップ数を超えました\", file=f)\n",
    "\n",
    "        if learning_complete:\n",
    "            break\n",
    "\n",
    "        old_states = copy.deepcopy(states)\n",
    "        old_actions = copy.deepcopy(actions)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "\n",
    "    if epsilon > MINIMUM_EPSILON:\n",
    "        epsilon = epsilon - EPSILON_DELTA\n",
    "\n",
    "    if greedy:\n",
    "        print(\n",
    "            f\"************ Episode{episode}の結果 ************\", file=f)\n",
    "        print(f\"要したステップ数: {step}\", file=f)\n",
    "        print(f\"食品の残り個数: {np.sum(env.stock)}\", file=f)\n",
    "        violations = []\n",
    "        for agent, reward in zip(env.agents, rewards):\n",
    "            print(\n",
    "                f\"{agent.name}の在庫: {agent.stock}  制約違反度: {agent.violation:.3f}  報酬: {reward:.3f}\", file=f)\n",
    "            print(f\"reward: {reward}\")\n",
    "            violations.append(agent.violation)\n",
    "\n",
    "        mean = np.average(violations)\n",
    "        var = np.std(violations)\n",
    "\n",
    "        print(f\"制約違反度の平均: {mean:.3f}  分散: {var:.3f}\", file=f)\n",
    "        # print(f\"現在のエピソード: {episode}\")\n",
    "        # result_reward_x.append(episode)\n",
    "        # result_reward_y.append(rewards[0])\n",
    "        # result_optimal_reward_x.append(episode)\n",
    "        # result_optimal_reward_y.append(optimal_reward)\n",
    "        # lines1.set_data(result_reward_x, result_reward_y)\n",
    "        result_episodes.append(episode)\n",
    "        result_ave.append(mean)\n",
    "        result_dev.append(var)\n",
    "\n",
    "        for i, agent, line, result in zip(range(AGENTS_COUNT), env.agents, lines, results_agents_y):\n",
    "            # result.append(rewards[i])\n",
    "            result.append(violations[i])\n",
    "            line.set_data(result_episodes, result)\n",
    "\n",
    "        line_ave.set_data(result_episodes, result_ave)\n",
    "        line_dev.set_data(result_episodes, result_dev)\n",
    "\n",
    "        # lines1.set_data(result_agents_x, result_agent1_y)\n",
    "        # lines2.set_data(result_agents_x, result_agent2_y)\n",
    "        # lines3.set_data(result_agents_x, result_agent3_y)\n",
    "\n",
    "        ax1.relim()\n",
    "        ax1.autoscale_view()\n",
    "\n",
    "        ax2.relim()\n",
    "        ax2.autoscale_view()\n",
    "\n",
    "        ax3.relim()\n",
    "        ax3.autoscale_view()\n",
    "\n",
    "        # lines1.set_data(result_satisfaction_x, result_satisfaction_y)\n",
    "\n",
    "        # lines2.set_data(result_optimal_reward_x, result_optimal_reward_y)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "# print(\"\\n---- 最適解 ----\")\n",
    "# for agent, loss, stock, request in zip(env.agents, optimal_loss_values, optimal_agent_stock, REQUESTS):\n",
    "#     diff = stock - request\n",
    "#     print(f\"{agent.name}: 損失{loss:.1f} 要求との差{diff} 在庫{stock}\")\n",
    "# print(f\"報酬: {optimal_reward:.4f}\")\n",
    "# print(f\"発見したエピソード: {optimal_episode}\")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"\\n終了時刻: {0:%Y/%m/%d %H:%M:%S}\".format(end_time), file=f)\n",
    "\n",
    "figure_name = f\"figure_{file_name_time}.png\"\n",
    "figure_path = os.path.join(DIR_PATH, \"figures\", figure_name)\n",
    "plt.savefig(figure_path)\n",
    "\n",
    "# csv_name = f\"data_{file_name_time}.csv\"\n",
    "# csv_path = os.path.join(DIR_PATH, \"data\", csv_name)\n",
    "# with open(csv_path, \"w\", newline='', encoding=\"UTF-8\") as csv_f:\n",
    "#     writer = csv.writer(csv_f)\n",
    "#     writer.writerow([\"Episode\", \"Distance\"])\n",
    "#     writer.writerows(self.result)\n",
    "\n",
    "print(\"\\n\\n終了\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3627c14751eaa7c73cc8d47219c6adb280d3451f32f6ace3fb2ffdf3eb5f2175"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
