{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# フードバンクにおける食品分配問題\n",
    "深層強化学習の適用\n",
    "\n",
    "DDQN (Double Deep Q-Network)\n",
    "\n",
    "ニューラルネットワークにはPyTorchを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 状態定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 状態定義\n",
    "from enum import IntEnum\n",
    "\n",
    "# 在庫の残り\n",
    "class StockRemaining(IntEnum):\n",
    "    FULL = 0\n",
    "    MANY = 1\n",
    "    FEW = 2\n",
    "    NONE = 3\n",
    "\n",
    "# 在庫の変動\n",
    "class StockChange(IntEnum):\n",
    "    NONE = 0\n",
    "    SLIGHTLY = 1\n",
    "    SOMEWHAT = 2\n",
    "    GREATLY = 3\n",
    "\n",
    "# エージェントの充足度\n",
    "class Satisfaction(IntEnum):\n",
    "    HARDLY = 0\n",
    "    SOMEWHAT = 1\n",
    "    COMLETELY = 2\n",
    "    OVERLY = 3\n",
    "\n",
    "# 終端状態の区別用\n",
    "class Progress(IntEnum):\n",
    "    ONGOING = 0\n",
    "    DONE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用パラメータ\n",
    "NUM_EPISODES = 2001\n",
    "MAX_STEPS = 30\n",
    "GREEDY_CYCLE = 100\n",
    "\n",
    "GAMMA = 0.98\n",
    "\n",
    "INITIAL_EPSILON = 1.0\n",
    "MINIMUM_EPSILON = 1.0\n",
    "EPSILON_DELTA = (INITIAL_EPSILON - MINIMUM_EPSILON) / (NUM_EPISODES * 0.95)\n",
    "\n",
    "ALPHA = 0.0001\n",
    "# INITIAL_ALPHA = 0.1\n",
    "# MINIMUM_ALPHA = 0.0001\n",
    "# ALPHA_DELTA = (INITIAL_ALPHA - MINIMUM_ALPHA) / (NUM_EPISODES * 0.95)\n",
    "\n",
    "HIDDEN_SIZE = 32\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "CAPACITY = 10000\n",
    "\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "\n",
    "\n",
    "# 環境設定\n",
    "AGENTS_COUNT = 1\n",
    "FOODS = [5, 5, 5]\n",
    "NUM_FOODS = len(FOODS)\n",
    "REQUESTS = [\n",
    "    [1, 3, 5],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 経験再生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 経験を保存するメモリクラスを定義\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    Experience Replay(経験再生)を実装するクラス\n",
    "    学習データを一定数メモリに保存し、ランダムにサンプリングすることで\n",
    "    時系列の相関をなくすことができる\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"メモリを初期化\"\"\"\n",
    "        # 状態遷移を保持するバッファ\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        # deque: double-ended queue （両端キュー）\n",
    "        # 両サイドからデータを取り出したり追加できる\n",
    "        # リストに比べて先頭・末尾の操作がO(1)の計算量で済む\n",
    "        # 最大要素数のcapacityを超えると古い要素から削除される\n",
    "\n",
    "    def push(self, state, action, state_next, reward):\n",
    "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
    "        \n",
    "        tensor_state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        tensor_state_next = torch.tensor(state_next, dtype=torch.float).unsqueeze(0)\n",
    "        tensor_action = torch.tensor([action], dtype=torch.long).unsqueeze(0)\n",
    "        tensor_reward = torch.tensor([reward], dtype=torch.long)\n",
    "\n",
    "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
    "        t = Transition(tensor_state, tensor_action, tensor_state_next, tensor_reward)\n",
    "        self.memory.append(t)\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディープ・ニューラルネットワークの構築\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.fc3 = nn.Linear(n_mid, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        output = self.fc3(h2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントが持つ脳\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions, f):\n",
    "        self.f = f\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        n_in, n_mid, n_out = num_states, 32, num_actions\n",
    "        self.main_q_network = Net(n_in, n_mid, n_out)  # Netクラスを使用\n",
    "        self.target_q_network = Net(n_in, n_mid, n_out)  # Netクラスを使用\n",
    "        print(self.main_q_network)  # ネットワークの形を出力\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.main_q_network.parameters(), lr=ALPHA)\n",
    "\n",
    "        # 損失関数\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        # self.model.apply(init_weights)\n",
    "    \n",
    "    def replay(self, greedy):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "\n",
    "        # メモリに十分にたまるまで待つ\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # ミニバッチの作成\n",
    "        self.batch, self.state_batch, self.action_batch, self.reward_batch, self.non_final_next_states = self.make_minibatch()\n",
    "\n",
    "        # 教師信号となるQ(s_t, a_t)値を求める\n",
    "        self.target_q_values = self.get_target_q_values()\n",
    "\n",
    "        # 結合パラメータの更新\n",
    "        self.update_main_q_network()\n",
    "\n",
    "\n",
    "    def decide_action(self, state, options, greedy, epsilon):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        # epsilon = 0.5 * (1 / (episode + 1))\n",
    "        greedy_print = greedy\n",
    "\n",
    "\n",
    "        if greedy is False:\n",
    "            # ε-greedyで行動を決定する\n",
    "            if np.random.rand() >= epsilon:\n",
    "                greedy = True\n",
    "        \n",
    "        greedy = False\n",
    "\n",
    "        if greedy:\n",
    "            self.main_q_network.eval()  # ネットワークを推論モードに切り替える\n",
    "\n",
    "            tensor_state = torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "            # print(state)\n",
    "            # print(tensor_state)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                q = self.main_q_network(tensor_state).numpy()\n",
    "                # print(f\"Q値: {q}\")\n",
    "                options_q = q[options]\n",
    "                max_arg = np.argmax(options_q)\n",
    "                action = options[max_arg]\n",
    "                # print(f\"action: {action}\")\n",
    "                if greedy_print:\n",
    "                    print(f\"Q値: {q} 選択した行動: {action}\", file=f)\n",
    "                \n",
    "            # ネットワークの出力の最大値のindexを取り出す\n",
    "\n",
    "        else:\n",
    "            action = random.choice(options)\n",
    "            # print(f\"ランダムな行動: {action}\")\n",
    "\n",
    "        return action\n",
    "\n",
    "    def make_minibatch(self):\n",
    "        '''ミニバッチの作成'''\n",
    "\n",
    "        # メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # 各変数をミニバッチに対応する形に変形\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # 次状態のうち、終端状態でない部分を抜き出す\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s.numpy()[0][-1] != Progress.DONE])\n",
    "\n",
    "        return batch, state_batch, action_batch, reward_batch, non_final_next_states\n",
    "\n",
    "    def get_target_q_values(self):\n",
    "        '''教師信号となるQ(s_t, a_t)値を求める'''\n",
    "\n",
    "        # ネットワークを推論モードに切り替える\n",
    "        self.main_q_network.eval()\n",
    "        self.target_q_network.eval()\n",
    "\n",
    "        # 現在のQ値（メインネットワークの出力）\n",
    "        self.current_q_values = torch.gather(self.main_q_network(self.state_batch), 1, self.action_batch)\n",
    "\n",
    "\n",
    "        # 終端状態でない次状態のマスク（終端でない所がTrueになる）\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s.numpy()[0][-1] != Progress.DONE, self.batch.next_state)))\n",
    "\n",
    "        # print(self.non_final_next_states)\n",
    "\n",
    "        non_final_q_values  = self.target_q_network(self.non_final_next_states).detach()\n",
    "        non_final_possible_q_values =  torch.tensor([ [non_final_q_values[si, a] if s[a] != StockRemaining.NONE else float(\"-inf\") for a in range(NUM_FOODS + 1)  ] for si, s in enumerate(self.non_final_next_states.detach())])\n",
    "\n",
    "\n",
    "        \n",
    "        next_max_q_values = torch.zeros(BATCH_SIZE) # すべて0にしておく\n",
    "        # 次状態のQ値の最大値（ターゲットネットワークの出力）\n",
    "        # 終端状態でないインデックスに、次状態の最大Q値を挿入\n",
    "        next_max_q_values[non_final_mask] = non_final_possible_q_values.max(1)[0].detach()\n",
    "\n",
    "        \n",
    "\n",
    "        # if greedy:\n",
    "            # print(non_final_q_values)\n",
    "            # print(possible_action_index)\n",
    "            # print(self.main_q_network(self.state_batch))\n",
    "            # test_batch  = torch.tensor([[0, 1]] * len(self.non_final_next_states))\n",
    "            # print(test_batch)\n",
    "            # print(self.current_q_values)\n",
    "            \n",
    "            # print(non_final_q_values)\n",
    "            # print(self.non_final_next_states)\n",
    "            # print(non_final_possible_q_values)\n",
    "            # print(test_batch)\n",
    "            # print(torch.tensor(possible_action_index))\n",
    "            # print(torch.gather(non_final_q_values, 1, test_batch))\n",
    "        \n",
    "\n",
    "        # next_Q = []\n",
    "        # for i in range(es.NUM_FOODS):\n",
    "        #     if state_next[i] != StockRemaining.NONE:\n",
    "        #         next_Q.append(self.Q[state_next][i])\n",
    "        # next_Q.append(self.Q[state_next][-1])\n",
    "\n",
    "        # max_Q = max(next_Q)\n",
    "\n",
    "        # print(next_max_q_values)\n",
    "\n",
    "\n",
    "        # 教師データとなるQ値の目標値\n",
    "        # （報酬 + 割引率 * 次状態の最大Q値）\n",
    "        target_q_values = (self.reward_batch + GAMMA * next_max_q_values).unsqueeze(1)\n",
    "\n",
    "        return target_q_values\n",
    "    \n",
    "    def update_main_q_network(self):\n",
    "        '''4. 結合パラメータの更新'''\n",
    "\n",
    "        # ネットワークを訓練モードに切り替える\n",
    "        self.main_q_network.train()\n",
    "\n",
    "        # 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = self.criterion(self.current_q_values,\n",
    "                                self.target_q_values)\n",
    "\n",
    "        # 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "\n",
    "        nn.utils.clip_grad_norm_(self.main_q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()  # 結合パラメータを更新\n",
    "\n",
    "    def update_target_q_network(self):  # DDQNで追加\n",
    "        '''Target Q-NetworkをMainと同じにする'''\n",
    "        self.target_q_network.load_state_dict(self.main_q_network.state_dict())\n",
    "        # print(\"Target Q-Networkを更新\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentクラス\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, request, num_states, num_actions,  f):\n",
    "        self.name = name\n",
    "        self.REQUEST = request\n",
    "        self.brain = Brain(num_states, num_actions, f)\n",
    "        self.f = f\n",
    "\n",
    "    def reset(self, env_stock, greedy):\n",
    "        self.current_requests = self.REQUEST.copy()\n",
    "        self.stock = np.zeros(NUM_FOODS, dtype=np.int64)\n",
    "\n",
    "        self.food_complete = False\n",
    "        self.learning_done = False\n",
    "        self.old_env_stock = env_stock.copy()\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, greedy):\n",
    "        self.brain.replay(greedy)\n",
    "\n",
    "    def update_target_q_function(self):\n",
    "        '''Target Q-NetworkをMain Q-Networkと同じに更新'''\n",
    "        \n",
    "        self.brain.update_target_q_network()        \n",
    "\n",
    "\n",
    "    # 行動（どの食品を取得するか）を決定\n",
    "    def get_action(self, state, env_stock, greedy, epsilon):\n",
    "        # 行動の候補\n",
    "        action_options = []\n",
    "\n",
    "        # 本部に在庫がある食品を候補に入れる\n",
    "        for food in range(len(env_stock)):\n",
    "            if env_stock[food] != 0:\n",
    "                action_options.append(food)\n",
    "\n",
    "\n",
    "        # 「何もしない」という選択肢も候補に加える\n",
    "        action_options.append(NUM_FOODS)\n",
    "\n",
    "\n",
    "        # 行動を決定\n",
    "        action = self.brain.decide_action(state, action_options, greedy, epsilon)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward, greedy):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        if state == state_next:\n",
    "            if greedy:\n",
    "                print(f\"{self.name}: 状態が変化していません\", file=self.f)\n",
    "        else:\n",
    "            # if greedy:\n",
    "            #     print(\"--- メモリに保存 ---\", file=self.f)\n",
    "            #     print(\"現在の状態: \", file=self.f)\n",
    "            #     self.print_state(state)\n",
    "            #     print(f\"とった行動: {action}\", file=self.f)\n",
    "            #     print(\"次の状態: \", file=self.f)\n",
    "            #     self.print_state(state_next)\n",
    "            #     print(f\"獲得した報酬: {reward}\", file=self.f)\n",
    "            #     print(\"-----------------\", file=self.f)\n",
    "            if not greedy:\n",
    "                self.brain.memory.push(state, action, state_next, reward)\n",
    "\n",
    "\n",
    "    def observe_state(self, env_stock, episode_terminal):\n",
    "        remainings = []\n",
    "        changes = []\n",
    "        satisfactions = []\n",
    "        progress = []\n",
    "\n",
    "        granularity = len(StockRemaining) - 2\n",
    "        for amount, original in zip(env_stock, FOODS):\n",
    "            section = round(original / granularity)\n",
    "            if amount == 0:\n",
    "                remainings.append(StockRemaining.NONE)\n",
    "            elif amount < section:\n",
    "                remainings.append(StockRemaining.FEW)\n",
    "            elif amount < original:\n",
    "                remainings.append(StockRemaining.MANY)\n",
    "            else:\n",
    "                remainings.append(StockRemaining.FULL)\n",
    "\n",
    "        difference = self.old_env_stock - env_stock\n",
    "        for diff in difference:\n",
    "            if diff == 0:\n",
    "                changes.append(StockChange.NONE)\n",
    "            elif diff == 1:\n",
    "                changes.append(StockChange.SLIGHTLY)\n",
    "            elif diff == 2:\n",
    "                changes.append(StockChange.SOMEWHAT)\n",
    "            else:\n",
    "                changes.append(StockChange.GREATLY)\n",
    "\n",
    "        satisfaction_rates = self.stock / self.REQUEST\n",
    "        for rate in satisfaction_rates:\n",
    "            if rate < 0.5:\n",
    "                satisfactions.append(Satisfaction.HARDLY)\n",
    "            elif rate < 1:\n",
    "                satisfactions.append(Satisfaction.SOMEWHAT)\n",
    "            elif rate == 1:\n",
    "                satisfactions.append(Satisfaction.COMLETELY)\n",
    "            else:\n",
    "                satisfactions.append(Satisfaction.OVERLY)\n",
    "\n",
    "        if episode_terminal:\n",
    "            progress.append(Progress.DONE)\n",
    "        else:\n",
    "            progress.append(Progress.ONGOING)\n",
    "\n",
    "        state = tuple(remainings + changes + satisfactions + progress)\n",
    "\n",
    "        # state = tuple(remainings + satisfactions + progress)\n",
    "\n",
    "        self.old_env_stock = env_stock.copy()\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    def grab_food(self, food):\n",
    "        # 手元の在庫が1つ増える\n",
    "        self.stock[food] += 1\n",
    "        # 要求リストから1つ減らす\n",
    "        if (self.current_requests[food] != 0):\n",
    "            self.current_requests[food] -= 1\n",
    "\n",
    "        # self.check_food_complete()\n",
    "\n",
    "    def check_food_complete(self, env_stock):\n",
    "        # print(f\"要求: {self.current_requests}\")\n",
    "        if np.all(self.current_requests == 0):\n",
    "            # 要求がすべて満たされたことを記録\n",
    "            self.food_complete= True\n",
    "            # print(f\"{self.name} 要求がすべて満たされました\")\n",
    "        else:\n",
    "            # 要求が満たされてない場合、食品がまだ余っているか在庫をチェック\n",
    "            # 欲しい食品の在庫\n",
    "            required_food_stock =  env_stock[self.current_requests > 0]\n",
    "            # 欲しい食品の在庫がすべてない\n",
    "            if np.all(required_food_stock == 0):\n",
    "                self.food_complete = True\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "       \n",
    "\n",
    "    def get_violation(self):\n",
    "        diffs = self.REQUEST - self.stock\n",
    "        diff_rates = diffs / self.REQUEST * 10\n",
    "        abs_diffs = np.absolute(diff_rates)\n",
    "        violation = np.sum(abs_diffs)\n",
    "\n",
    "        # if (abs(violation) > 100):\n",
    "        #     print(f\"violation: {violation}\")\n",
    "\n",
    "        self.violation = violation\n",
    "        return violation\n",
    "\n",
    "\n",
    "    def print_state(self, state):\n",
    "        num = NUM_FOODS\n",
    "        print(f\"{self.name} State: \", end=\"\", file=self.f)\n",
    "\n",
    "        print(\"Remaining[ \", end=\"\", file=self.f)\n",
    "        for i in range(num):\n",
    "            print(f\"{state[i].name} \", end=\"\", file=self.f)\n",
    "        print(\"], Change[ \", end=\"\", file=self.f)\n",
    "        for i in range(num, num * 2):\n",
    "            print(f\"{state[i].name} \", end=\"\", file=self.f)\n",
    "        print(\"], Satisfaction[ \", end=\"\", file=self.f)\n",
    "        for i in range(num * 2, num * 3):\n",
    "            print(f\"{state[i].name} \", end=\"\", file=self.f)\n",
    "\n",
    "        print(f\"] Progress[{state[num * 3].name}]\", file=self.f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "        self.agents = self.init_agents()\n",
    "\n",
    "    def init_agents(self):\n",
    "        state_size = pow(len(StockRemaining), NUM_FOODS) * pow(len(StockChange),\n",
    "                                                                  NUM_FOODS) * pow(len(Satisfaction), NUM_FOODS) * len(Progress)\n",
    "        num_states = NUM_FOODS * 3 + 1\n",
    "        num_actions = NUM_FOODS + 1\n",
    "\n",
    "        print(\"========= 各エージェントのもつ状態行動空間 =========\", file=self.f)\n",
    "        print(f\"状態数: {state_size:,}\", file=self.f)\n",
    "        print(f\"行動数: {num_actions}\", file=self.f)\n",
    "        print(f\"状態 × 行動の組み合わせ: {(state_size * num_actions):,}\", file=self.f)\n",
    "        print(\"\\n\\n\", file=self.f)\n",
    "\n",
    "        agents: List[Agent] = []\n",
    "        for i in range(AGENTS_COUNT):\n",
    "            name = f\"Agent{i + 1}\"\n",
    "            agent = Agent(\n",
    "                name, np.array(REQUESTS[i]), num_states, num_actions, self.f)\n",
    "            agents.append(agent)\n",
    "        return agents\n",
    "\n",
    "    def reset(self, greedy):\n",
    "        self.stock = np.array(FOODS, dtype=np.int64)\n",
    "        states = None\n",
    "\n",
    "        for agent in self.agents:\n",
    "            agent.reset(self.stock, greedy)\n",
    "\n",
    "        return states\n",
    "\n",
    "    def get_actions(self, states):\n",
    "        actions = []\n",
    "        # すべてのエージェントに対して\n",
    "        for agent, state in zip(self.agents, states):\n",
    "            # 行動を決定\n",
    "            action = agent.get_action(state)\n",
    "            actions.append(action)\n",
    "            if action == NUM_FOODS:\n",
    "                # print(f\"{agent.name} 行動: 何もしない\")\n",
    "                pass\n",
    "            else:\n",
    "                # print(f\"{agent.name} 行動: 食品{action}を１つ取る\")\n",
    "                pass\n",
    "        return actions\n",
    "\n",
    "    def check_food_run_out(self):\n",
    "        # 全ての在庫が0になったかチェック\n",
    "        return np.all(self.stock == 0)\n",
    "\n",
    "    def check_agents_food_complete(self):\n",
    "        # 全てのエージェントが終了条件を満たしているかチェック\n",
    "        all_done = True\n",
    "        for agent in self.agents:\n",
    "            if not agent.check_food_complete(self.stock):\n",
    "                all_done = False\n",
    "                break\n",
    "        # 全エージェントの取れる行動がなくなったか\n",
    "        return all_done\n",
    "\n",
    "    def check_agents_learning_done(self):\n",
    "        all_done = True\n",
    "        for agent in self.agents:\n",
    "            if not agent.learning_done:\n",
    "                all_done = False\n",
    "                break\n",
    "        return all_done\n",
    "\n",
    "    # def learn(self, states, actions, rewards, states_next, alpha):\n",
    "    #     for agent, state, action, reward, state_next in zip(self.agents, states, actions, rewards, states_next):\n",
    "    #         agent.learn(state, action, reward, state_next, alpha)\n",
    "\n",
    "    def get_reward(self, target_agent: Agent, terminal, greedy):\n",
    "        if terminal:\n",
    "            # - |制約違反度の平均からの偏差|\n",
    "            # violations = []\n",
    "            # for agent in self.agents:\n",
    "            #     v = agent.get_violation()\n",
    "            #     violations.append(v)\n",
    "            # mean = np.mean(violations)\n",
    "            # abs_deviation = np.absolute(mean - target_agent.get_violation())\n",
    "            # reward = - abs_deviation\n",
    "\n",
    "            # - (制約違反度 + 制約違反度の標準偏差)\n",
    "            violations = []\n",
    "            for agent in self.agents:\n",
    "                v = agent.get_violation()\n",
    "                # print(f\"violation: {v}\")\n",
    "                violations.append(v)\n",
    "            std = np.std(violations)\n",
    "            # print(f\"std: {std}\")\n",
    "            \n",
    "            reward = - (target_agent.get_violation() + std)\n",
    "            # reward = 0\n",
    "            # print(f\"reward: {reward}\")\n",
    "\n",
    "            # - (制約違反度の平均+標準偏差)　統一\n",
    "            # violations = []\n",
    "            # for agent in self.agents:\n",
    "            #     v = agent.get_violation()\n",
    "            #     violations.append(v)\n",
    "            # mean = np.mean(violations)\n",
    "            # std = np.std(violations)\n",
    "            # reward = - (mean + std)\n",
    "\n",
    "            if greedy:\n",
    "                print(\n",
    "                    f\"{target_agent.name}: 報酬{reward:.3f}  要求{target_agent.REQUEST} 在庫{target_agent.stock}\", file=self.f)\n",
    "\n",
    "        else:\n",
    "            reward = -1\n",
    "            # reward = 0\n",
    "       \n",
    "        return reward\n",
    "\n",
    "    def print_env_state(self):\n",
    "        print(\"Env State: [\", end=\"\", file=self.f)\n",
    "        for status in self.env_state:\n",
    "            print(f\"{status.name} \", end=\"\", file=self.f)\n",
    "\n",
    "        print(\"]\", file=self.f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メイン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n",
      "-------------- Episode:0 (greedy) --------------\n",
      "要したステップ数: 30\n",
      "reward: -46.666666666666664\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Episode:100 (greedy) --------------\n",
      "要したステップ数: 30\n",
      "reward: -46.666666666666664\n",
      "-------------- Episode:200 (greedy) --------------\n",
      "要したステップ数: 30\n",
      "reward: -46.666666666666664\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-4b057728640e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mold_states\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_actions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgreedy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mepisode_terminal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-606ac46cb852>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, greedy)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgreedy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_target_q_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-47253cf6d280>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, greedy)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# 教師信号となるQ(s_t, a_t)値を求める\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_target_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# 結合パラメータの更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-47253cf6d280>\u001b[0m in \u001b[0;36mget_target_q_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# print(self.non_final_next_states)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mnon_final_q_values\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_q_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mnon_final_possible_q_values\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnon_final_q_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mStockRemaining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNONE\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-inf\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_FOODS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-86-fe1a096ae0aa>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.set_printoptions(precision=5, floatmode='maxprec_equal')\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "DIR_PATH = \"D:\\\\Lighthouse\\\\Documents\\\\Reinforcement Learning\\\\Food Distribution\\\\results\"\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "file_name_time = \"{0:%Y-%m-%d_%H%M%S}\".format(start_time)\n",
    "\n",
    "log_name = f\"log_{file_name_time}.txt\"\n",
    "log_path = os.path.join(DIR_PATH, \"logs\", log_name)\n",
    "f = open(log_path, mode=\"w\", encoding=\"UTF-8\")\n",
    "\n",
    "print(\"フードバンク食品分配 DDQN\\n\", file=f)\n",
    "\n",
    "print(\"開始時刻: {0:%Y/%m/%d %H:%M:%S}\\n\".format(start_time), file=f)\n",
    "\n",
    "print(\"====================== 環境設定 ======================\", file=f)\n",
    "print(f\"食品の種類: {NUM_FOODS}\", file=f)\n",
    "print(f\"本部の在庫: {FOODS}\", file=f)\n",
    "print(f\"エージェント数: {AGENTS_COUNT}\", file=f)\n",
    "print(f\"エージェントの要求リスト: {REQUESTS}\", file=f)\n",
    "print(\"\\n\\n\", file=f)\n",
    "\n",
    "print(\"==================== 学習パラメーター ====================\", file=f)\n",
    "print(f\"エピソード数: {NUM_EPISODES:,}\", file=f)\n",
    "print(f\"最大ステップ数: {MAX_STEPS:,}\\n\", file=f)\n",
    "print(f\"割引率: {GAMMA}\\n\", file=f)\n",
    "print(f\"学習率: {ALPHA}\", file=f)\n",
    "print(f\"εの初期値: {INITIAL_EPSILON}\", file=f)\n",
    "print(f\"εの最終値: {MINIMUM_EPSILON}\", file=f)\n",
    "print(f\"1エピソードごとのεの減少値: {EPSILON_DELTA:.7f}\", file=f)\n",
    "print(\"\\n\\n\", file=f)\n",
    "\n",
    "print(\"====================== 報酬設定 ======================\", file=f)\n",
    "print(\"- (制約違反度 + 制約違反度の標準偏差)\", file=f)\n",
    "# print(\"- |制約違反度の平均からの偏差|\", file=f)\n",
    "# print(\"- (制約違反度の平均 + 標準偏差) 統一\", file=f)\n",
    "print(\"\\n\\n\", file=f)\n",
    "\n",
    "env = Environment(f)\n",
    "\n",
    "result_episodes = []\n",
    "results_agents_y = [[], [], []]\n",
    "\n",
    "result_ave = []\n",
    "result_dev = []\n",
    "\n",
    "epsilon = INITIAL_EPSILON\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(3, 1, 1)\n",
    "ax2 = fig.add_subplot(3, 1, 2)\n",
    "ax3 = fig.add_subplot(3, 1, 3)\n",
    "\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax2.set_xlabel(\"Episode\")\n",
    "ax3.set_xlabel(\"Episode\")\n",
    "\n",
    "ax1.set_ylabel(\"Constraint Violations\")\n",
    "ax2.set_ylabel(\"Mean\")\n",
    "ax3.set_ylabel(\"Variance\")\n",
    "\n",
    "line1, = ax1.plot([], [], label=\"Agent1\")\n",
    "line2, = ax1.plot([], [], label=\"Agent2\")\n",
    "line3, = ax1.plot([], [], label=\"Agent3\")\n",
    "\n",
    "line_ave, = ax2.plot([], [])\n",
    "line_dev, = ax3.plot([], [])\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "lines = [line1, line2, line3]\n",
    "\n",
    "# 各エピソード\n",
    "for episode in range(NUM_EPISODES):\n",
    "\n",
    "    if episode % GREEDY_CYCLE == 0:\n",
    "        greedy = True\n",
    "        print(\n",
    "            f\"-------------- Episode:{episode} (greedy) --------------\")\n",
    "        print(\n",
    "            f\"\\n\\n-------------- Episode:{episode} (greedy) ----------------------------------------------------------------------\", file=f)\n",
    "    else:\n",
    "        greedy = False\n",
    "        # print(f\"-------------- Episode:{episode} --------------\")\n",
    "    states = [[], [], []]\n",
    "    actions = [[], [], []]\n",
    "    rewards = [0, 0, 0]\n",
    "\n",
    "    old_states = env.reset(greedy)\n",
    "    old_actions = []\n",
    "\n",
    "    learning_complete = False\n",
    "    step = 0\n",
    "\n",
    "    if greedy:\n",
    "        # print(f\"学習率: {alpha:.5f}\", file=f)\n",
    "        print(f\"ε: {epsilon:.5f}\", file=f)\n",
    "\n",
    "    # 各ステップ\n",
    "    while True:\n",
    "        if greedy:\n",
    "            print(f\"\\n------- Step:{step} -------\", file=f)\n",
    "\n",
    "        # 各エージェント\n",
    "        for index, agent in enumerate(env.agents):\n",
    "\n",
    "            # 終了条件確認\n",
    "            # food_run_out = env.check_food_run_out()\n",
    "            all_agents_food_compolete = env.check_agents_food_complete()\n",
    "            exceed_max_step = step == MAX_STEPS - 1\n",
    "\n",
    "            episode_terminal = all_agents_food_compolete or exceed_max_step\n",
    "\n",
    "            if greedy:\n",
    "                print(f\"\\n本部の在庫: {env.stock}\", file=f)\n",
    "                print(f\"{agent.name}の在庫:{agent.stock} 要求:{agent.current_requests}\", file=f)\n",
    "                diff = agent.old_env_stock - env.stock\n",
    "                print(f\"{agent.name}視点の本部在庫の変化:{diff}\", file=f)\n",
    "                if episode_terminal:\n",
    "                    print(\"*** 終了条件を満たしています ***\", file=f)\n",
    "\n",
    "            # 状態を観測\n",
    "            state = agent.observe_state(env.stock, episode_terminal)\n",
    "            states[index] = state\n",
    "\n",
    "            if greedy:\n",
    "                agent.print_state(state)\n",
    "\n",
    "            # 行動を決定\n",
    "            if not episode_terminal:\n",
    "                action = agent.get_action(\n",
    "                    state, env.stock, greedy, epsilon)\n",
    "                actions[index] = action\n",
    "\n",
    "                if greedy:\n",
    "                    if action == NUM_FOODS:\n",
    "                        action_string = \"何もしない\"\n",
    "                    else:\n",
    "                        action_string = f\"食品{action}\"\n",
    "\n",
    "                    print(\n",
    "                        f\"{agent.name} 選択した行動: {action_string}\", file=f)\n",
    "\n",
    "                # 行動をとる\n",
    "                if action != NUM_FOODS:\n",
    "                    # エージェントが食品を1つとる\n",
    "                    agent.grab_food(action)\n",
    "                    # 本部の在庫が1つ減る\n",
    "                    env.stock[action] -= 1\n",
    "\n",
    "            reward = env.get_reward(agent, episode_terminal, greedy)\n",
    "\n",
    "            if old_states is not None:\n",
    "                agent.memorize(old_states[index], old_actions[index], state, reward, greedy)\n",
    "                agent.learn(greedy)\n",
    "\n",
    "            if episode_terminal:\n",
    "                if greedy:\n",
    "                    print(f\"{agent.name} 学習終了\\n\", file=f)\n",
    "\n",
    "                rewards[index] = reward\n",
    "                agent.learning_done = True\n",
    "                learning_complete = env.check_agents_learning_done()\n",
    "                if learning_complete:\n",
    "                    break\n",
    "\n",
    "        if exceed_max_step and greedy:\n",
    "            print(\"最大ステップ数を超えました\", file=f)\n",
    "\n",
    "        if learning_complete:\n",
    "            break\n",
    "\n",
    "        old_states = copy.deepcopy(states)\n",
    "        old_actions = copy.deepcopy(actions)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "\n",
    "    if epsilon > MINIMUM_EPSILON:\n",
    "        epsilon = epsilon - EPSILON_DELTA\n",
    "\n",
    "    if (episode % TARGET_UPDATE == 0):\n",
    "        for agent in env.agents:\n",
    "            agent.update_target_q_function()\n",
    "\n",
    "    if greedy:\n",
    "        print(\n",
    "            f\"************ Episode{episode}の結果 ************\", file=f)\n",
    "        print(f\"要したステップ数: {step + 1}\", file=f)\n",
    "        print(f\"要したステップ数: {step + 1}\")\n",
    "        print(f\"食品の残り個数: {np.sum(env.stock)}\", file=f)\n",
    "        violations = []\n",
    "        for agent, reward in zip(env.agents, rewards):\n",
    "            print(\n",
    "                f\"{agent.name}の在庫: {agent.stock}  制約違反度: {agent.violation:.3f}  報酬: {reward:.3f}\", file=f)\n",
    "            print(f\"reward: {reward}\")\n",
    "            violations.append(agent.violation)\n",
    "\n",
    "        mean = np.average(violations)\n",
    "        var = np.std(violations)\n",
    "\n",
    "        print(f\"制約違反度の平均: {mean:.3f}  分散: {var:.3f}\", file=f)\n",
    "        # print(f\"現在のエピソード: {episode}\")\n",
    "        # result_reward_x.append(episode)\n",
    "        # result_reward_y.append(rewards[0])\n",
    "        # result_optimal_reward_x.append(episode)\n",
    "        # result_optimal_reward_y.append(optimal_reward)\n",
    "        # lines1.set_data(result_reward_x, result_reward_y)\n",
    "        result_episodes.append(episode)\n",
    "        result_ave.append(mean)\n",
    "        result_dev.append(var)\n",
    "\n",
    "        for i, agent, line, result in zip(range(AGENTS_COUNT), env.agents, lines, results_agents_y):\n",
    "            # result.append(rewards[i])\n",
    "            result.append(violations[i])\n",
    "            line.set_data(result_episodes, result)\n",
    "\n",
    "        line_ave.set_data(result_episodes, result_ave)\n",
    "        line_dev.set_data(result_episodes, result_dev)\n",
    "\n",
    "        # lines1.set_data(result_agents_x, result_agent1_y)\n",
    "        # lines2.set_data(result_agents_x, result_agent2_y)\n",
    "        # lines3.set_data(result_agents_x, result_agent3_y)\n",
    "\n",
    "        ax1.relim()\n",
    "        ax1.autoscale_view()\n",
    "\n",
    "        ax2.relim()\n",
    "        ax2.autoscale_view()\n",
    "\n",
    "        ax3.relim()\n",
    "        ax3.autoscale_view()\n",
    "\n",
    "        # lines1.set_data(result_satisfaction_x, result_satisfaction_y)\n",
    "\n",
    "        # lines2.set_data(result_optimal_reward_x, result_optimal_reward_y)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "# print(\"\\n---- 最適解 ----\")\n",
    "# for agent, loss, stock, request in zip(env.agents, optimal_loss_values, optimal_agent_stock, REQUESTS):\n",
    "#     diff = stock - request\n",
    "#     print(f\"{agent.name}: 損失{loss:.1f} 要求との差{diff} 在庫{stock}\")\n",
    "# print(f\"報酬: {optimal_reward:.4f}\")\n",
    "# print(f\"発見したエピソード: {optimal_episode}\")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"\\n終了時刻: {0:%Y/%m/%d %H:%M:%S}\".format(end_time), file=f)\n",
    "\n",
    "figure_name = f\"figure_{file_name_time}.png\"\n",
    "figure_path = os.path.join(DIR_PATH, \"figures\", figure_name)\n",
    "plt.savefig(figure_path)\n",
    "\n",
    "# csv_name = f\"data_{file_name_time}.csv\"\n",
    "# csv_path = os.path.join(DIR_PATH, \"data\", csv_name)\n",
    "# with open(csv_path, \"w\", newline='', encoding=\"UTF-8\") as csv_f:\n",
    "#     writer = csv.writer(csv_f)\n",
    "#     writer.writerow([\"Episode\", \"Distance\"])\n",
    "#     writer.writerows(self.result)\n",
    "\n",
    "print(\"\\n\\n終了\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3627c14751eaa7c73cc8d47219c6adb280d3451f32f6ace3fb2ffdf3eb5f2175"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
