{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# フードバンクにおける食品分配問題\n",
    "深層強化学習の適用\n",
    "\n",
    "DDQN (Double Deep Q-Network)\n",
    "\n",
    "ニューラルネットワークにはPyTorchを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 状態定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 状態定義\n",
    "from enum import IntEnum\n",
    "\n",
    "# 在庫の残り\n",
    "class StockRemaining(IntEnum):\n",
    "    FULL = 0\n",
    "    MANY = 1\n",
    "    FEW = 2\n",
    "    NONE = 3\n",
    "\n",
    "# 在庫の変動\n",
    "class StockChange(IntEnum):\n",
    "    NONE = 0\n",
    "    SLIGHTLY = 1\n",
    "    SOMEWHAT = 2\n",
    "    GREATLY = 3\n",
    "\n",
    "# エージェントの充足度\n",
    "class Satisfaction(IntEnum):\n",
    "    HARDLY = 0\n",
    "    SOMEWHAT = 1\n",
    "    COMLETELY = 2\n",
    "    OVERLY = 3\n",
    "\n",
    "# 終端状態の区別用\n",
    "class Progress(IntEnum):\n",
    "    ONGOING = 0\n",
    "    DONE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用パラメータ\n",
    "NUM_EPISODES = 10001\n",
    "MAX_STEPS = 100\n",
    "GREEDY_CYCLE = 100\n",
    "\n",
    "GAMMA = 0.98\n",
    "\n",
    "INITIAL_EPSILON = 1.0\n",
    "MINIMUM_EPSILON = 0.6\n",
    "EPSILON_DELTA = (INITIAL_EPSILON - MINIMUM_EPSILON) / (NUM_EPISODES * 0.95)\n",
    "\n",
    "ALPHA = 0.0001\n",
    "# INITIAL_ALPHA = 0.1\n",
    "# MINIMUM_ALPHA = 0.0001\n",
    "# ALPHA_DELTA = (INITIAL_ALPHA - MINIMUM_ALPHA) / (NUM_EPISODES * 0.95)\n",
    "\n",
    "HIDDEN_SIZE = 32\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000\n",
    "\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "\n",
    "\n",
    "# 環境設定\n",
    "\n",
    "# AGENTS_COUNT = 1\n",
    "# FOODS = [5, 5, 5]\n",
    "# NUM_FOODS = len(FOODS)\n",
    "# REQUESTS = [\n",
    "#     [1, 3, 5],\n",
    "# ]\n",
    "\n",
    "AGENTS_COUNT = 3\n",
    "FOODS = [20, 20, 20]\n",
    "NUM_FOODS = len(FOODS)\n",
    "REQUESTS = [\n",
    "    [10, 10, 10],\n",
    "    [5, 10, 5],\n",
    "    [5, 5, 10],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 経験再生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 経験を保存するメモリクラスを定義\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    Experience Replay(経験再生)を実装するクラス\n",
    "    学習データを一定数メモリに保存し、ランダムにサンプリングすることで\n",
    "    時系列の相関をなくすことができる\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"メモリを初期化\"\"\"\n",
    "        # 状態遷移を保持するバッファ\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        # deque: double-ended queue （両端キュー）\n",
    "        # 両サイドからデータを取り出したり追加できる\n",
    "        # リストに比べて先頭・末尾の操作がO(1)の計算量で済む\n",
    "        # 最大要素数のcapacityを超えると古い要素から削除される\n",
    "\n",
    "    def push(self, state, action, state_next, reward):\n",
    "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
    "        \n",
    "        tensor_state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        tensor_state_next = torch.tensor(state_next, dtype=torch.float).unsqueeze(0)\n",
    "        tensor_action = torch.tensor([action], dtype=torch.long).unsqueeze(0)\n",
    "        tensor_reward = torch.tensor([reward], dtype=torch.long)\n",
    "\n",
    "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
    "        t = Transition(tensor_state, tensor_action, tensor_state_next, tensor_reward)\n",
    "        self.memory.append(t)\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディープ・ニューラルネットワークの構築\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.fc3 = nn.Linear(n_mid, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        output = self.fc3(h2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントが持つ脳\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions, f):\n",
    "        self.f = f\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        n_in, n_mid, n_out = num_states, 32, num_actions\n",
    "        self.main_q_network = Net(n_in, n_mid, n_out)  # Netクラスを使用\n",
    "        self.target_q_network = Net(n_in, n_mid, n_out)  # Netクラスを使用\n",
    "        print(self.main_q_network)  # ネットワークの形を出力\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.main_q_network.parameters(), lr=ALPHA)\n",
    "\n",
    "        # 損失関数\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        # self.model.apply(init_weights)\n",
    "    \n",
    "    def replay(self, greedy):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "\n",
    "        # メモリに十分にたまるまで待つ\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # ミニバッチの作成\n",
    "        self.batch, self.state_batch, self.action_batch, self.reward_batch, self.non_final_next_states = self.make_minibatch()\n",
    "\n",
    "        # 教師信号となるQ(s_t, a_t)値を求める\n",
    "        self.target_q_values = self.get_target_q_values()\n",
    "\n",
    "        # 結合パラメータの更新\n",
    "        self.update_main_q_network()\n",
    "\n",
    "\n",
    "    def decide_action(self, state, options, greedy, epsilon):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        # epsilon = 0.5 * (1 / (episode + 1))\n",
    "        greedy_print = greedy\n",
    "\n",
    "\n",
    "        if greedy is False:\n",
    "            # ε-greedyで行動を決定する\n",
    "            if np.random.rand() >= epsilon:\n",
    "                greedy = True\n",
    "\n",
    "\n",
    "        if greedy:\n",
    "            self.main_q_network.eval()  # ネットワークを推論モードに切り替える\n",
    "\n",
    "            tensor_state = torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "            # print(state)\n",
    "            # print(tensor_state)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                q = self.main_q_network(tensor_state).numpy()\n",
    "                # print(f\"Q値: {q}\")\n",
    "                options_q = q[options]\n",
    "                max_arg = np.argmax(options_q)\n",
    "                action = options[max_arg]\n",
    "                # print(f\"action: {action}\")\n",
    "                if greedy_print:\n",
    "                    print(f\"Q値: {q} 選択した行動: {action}\", file=f)\n",
    "                \n",
    "            # ネットワークの出力の最大値のindexを取り出す\n",
    "\n",
    "        else:\n",
    "            action = random.choice(options)\n",
    "            # print(f\"ランダムな行動: {action}\")\n",
    "\n",
    "        return action\n",
    "\n",
    "    def make_minibatch(self):\n",
    "        '''ミニバッチの作成'''\n",
    "\n",
    "        # メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # 各変数をミニバッチに対応する形に変形\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # 次状態のうち、終端状態でない部分を抜き出す\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s.numpy()[0][-1] != Progress.DONE])\n",
    "\n",
    "        return batch, state_batch, action_batch, reward_batch, non_final_next_states\n",
    "\n",
    "    def get_target_q_values(self):\n",
    "        '''教師信号となるQ(s_t, a_t)値を求める'''\n",
    "\n",
    "        # ネットワークを推論モードに切り替える\n",
    "        self.main_q_network.eval()\n",
    "        self.target_q_network.eval()\n",
    "\n",
    "        # 現在のQ値（メインネットワークの出力）\n",
    "        self.current_q_values = torch.gather(self.main_q_network(self.state_batch), 1, self.action_batch)\n",
    "\n",
    "\n",
    "        # 終端状態でない次状態のマスク（終端でない所がTrueになる）\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s.numpy()[0][-1] != Progress.DONE, self.batch.next_state)))\n",
    "\n",
    "        # print(self.non_final_next_states)\n",
    "\n",
    "        non_final_q_values  = self.target_q_network(self.non_final_next_states).detach()\n",
    "        non_final_possible_q_values =  torch.tensor([ [non_final_q_values[si, a] if s[a] != StockRemaining.NONE else float(\"-inf\") for a in range(NUM_FOODS + 1)  ] for si, s in enumerate(self.non_final_next_states.detach())])\n",
    "\n",
    "\n",
    "        \n",
    "        next_max_q_values = torch.zeros(BATCH_SIZE) # すべて0にしておく\n",
    "        # 次状態のQ値の最大値（ターゲットネットワークの出力）\n",
    "        # 終端状態でないインデックスに、次状態の最大Q値を挿入\n",
    "        next_max_q_values[non_final_mask] = non_final_possible_q_values.max(1)[0].detach()\n",
    "\n",
    "        \n",
    "\n",
    "        # if greedy:\n",
    "            # print(non_final_q_values)\n",
    "            # print(possible_action_index)\n",
    "            # print(self.main_q_network(self.state_batch))\n",
    "            # test_batch  = torch.tensor([[0, 1]] * len(self.non_final_next_states))\n",
    "            # print(test_batch)\n",
    "            # print(self.current_q_values)\n",
    "            \n",
    "            # print(non_final_q_values)\n",
    "            # print(self.non_final_next_states)\n",
    "            # print(non_final_possible_q_values)\n",
    "            # print(test_batch)\n",
    "            # print(torch.tensor(possible_action_index))\n",
    "            # print(torch.gather(non_final_q_values, 1, test_batch))\n",
    "        \n",
    "\n",
    "        # next_Q = []\n",
    "        # for i in range(es.NUM_FOODS):\n",
    "        #     if state_next[i] != StockRemaining.NONE:\n",
    "        #         next_Q.append(self.Q[state_next][i])\n",
    "        # next_Q.append(self.Q[state_next][-1])\n",
    "\n",
    "        # max_Q = max(next_Q)\n",
    "\n",
    "        # print(next_max_q_values)\n",
    "\n",
    "\n",
    "        # 教師データとなるQ値の目標値\n",
    "        # （報酬 + 割引率 * 次状態の最大Q値）\n",
    "        target_q_values = (self.reward_batch + GAMMA * next_max_q_values).unsqueeze(1)\n",
    "\n",
    "        return target_q_values\n",
    "    \n",
    "    def update_main_q_network(self):\n",
    "        '''4. 結合パラメータの更新'''\n",
    "\n",
    "        # ネットワークを訓練モードに切り替える\n",
    "        self.main_q_network.train()\n",
    "\n",
    "        # 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = self.criterion(self.current_q_values,\n",
    "                                self.target_q_values)\n",
    "\n",
    "        # 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "\n",
    "        nn.utils.clip_grad_norm_(self.main_q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()  # 結合パラメータを更新\n",
    "\n",
    "    def update_target_q_network(self):  # DDQNで追加\n",
    "        '''Target Q-NetworkをMainと同じにする'''\n",
    "        self.target_q_network.load_state_dict(self.main_q_network.state_dict())\n",
    "        # print(\"Target Q-Networkを更新\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentクラス\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, request, num_states, num_actions,  f):\n",
    "        self.name = name\n",
    "        self.REQUEST = request\n",
    "        self.brain = Brain(num_states, num_actions, f)\n",
    "        self.f = f\n",
    "\n",
    "    def reset(self, env_stock, greedy):\n",
    "        self.current_requests = self.REQUEST.copy()\n",
    "        self.stock = np.zeros(NUM_FOODS, dtype=np.int64)\n",
    "\n",
    "        self.food_complete = False\n",
    "        self.learning_done = False\n",
    "        self.old_env_stock = env_stock.copy()\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, greedy):\n",
    "        self.brain.replay(greedy)\n",
    "\n",
    "    def update_target_q_function(self):\n",
    "        '''Target Q-NetworkをMain Q-Networkと同じに更新'''\n",
    "        \n",
    "        self.brain.update_target_q_network()        \n",
    "\n",
    "\n",
    "    # 行動（どの食品を取得するか）を決定\n",
    "    def get_action(self, state, env_stock, greedy, epsilon):\n",
    "        # 行動の候補\n",
    "        action_options = []\n",
    "\n",
    "        # 本部に在庫がある食品を候補に入れる\n",
    "        for food in range(len(env_stock)):\n",
    "            if env_stock[food] != 0:\n",
    "                action_options.append(food)\n",
    "\n",
    "\n",
    "        # 「何もしない」という選択肢も候補に加える\n",
    "        action_options.append(NUM_FOODS)\n",
    "\n",
    "\n",
    "        # 行動を決定\n",
    "        action = self.brain.decide_action(state, action_options, greedy, epsilon)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward, greedy):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        if state == state_next:\n",
    "            if greedy:\n",
    "                print(f\"{self.name}: 状態が変化していません\", file=self.f)\n",
    "        else:\n",
    "            # if greedy:\n",
    "            #     print(\"--- メモリに保存 ---\", file=self.f)\n",
    "            #     print(\"現在の状態: \", file=self.f)\n",
    "            #     self.print_state(state)\n",
    "            #     print(f\"とった行動: {action}\", file=self.f)\n",
    "            #     print(\"次の状態: \", file=self.f)\n",
    "            #     self.print_state(state_next)\n",
    "            #     print(f\"獲得した報酬: {reward}\", file=self.f)\n",
    "            #     print(\"-----------------\", file=self.f)\n",
    "            if not greedy:\n",
    "                self.brain.memory.push(state, action, state_next, reward)\n",
    "\n",
    "\n",
    "    def observe_state(self, env_stock, episode_terminal):\n",
    "        remainings = []\n",
    "        changes = []\n",
    "        satisfactions = []\n",
    "        progress = []\n",
    "\n",
    "        granularity = len(StockRemaining) - 2\n",
    "        for amount, original in zip(env_stock, FOODS):\n",
    "            section = round(original / granularity)\n",
    "            if amount == 0:\n",
    "                remainings.append(StockRemaining.NONE)\n",
    "            elif amount < section:\n",
    "                remainings.append(StockRemaining.FEW)\n",
    "            elif amount < original:\n",
    "                remainings.append(StockRemaining.MANY)\n",
    "            else:\n",
    "                remainings.append(StockRemaining.FULL)\n",
    "\n",
    "        difference = self.old_env_stock - env_stock\n",
    "        for diff in difference:\n",
    "            if diff == 0:\n",
    "                changes.append(StockChange.NONE)\n",
    "            elif diff == 1:\n",
    "                changes.append(StockChange.SLIGHTLY)\n",
    "            elif diff == 2:\n",
    "                changes.append(StockChange.SOMEWHAT)\n",
    "            else:\n",
    "                changes.append(StockChange.GREATLY)\n",
    "\n",
    "        satisfaction_rates = self.stock / self.REQUEST\n",
    "        for rate in satisfaction_rates:\n",
    "            if rate < 0.5:\n",
    "                satisfactions.append(Satisfaction.HARDLY)\n",
    "            elif rate < 1:\n",
    "                satisfactions.append(Satisfaction.SOMEWHAT)\n",
    "            elif rate == 1:\n",
    "                satisfactions.append(Satisfaction.COMLETELY)\n",
    "            else:\n",
    "                satisfactions.append(Satisfaction.OVERLY)\n",
    "\n",
    "        if episode_terminal:\n",
    "            progress.append(Progress.DONE)\n",
    "        else:\n",
    "            progress.append(Progress.ONGOING)\n",
    "\n",
    "        state = tuple(remainings + changes + satisfactions + progress)\n",
    "\n",
    "        # state = tuple(remainings + satisfactions + progress)\n",
    "\n",
    "        self.old_env_stock = env_stock.copy()\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    def grab_food(self, food):\n",
    "        # 手元の在庫が1つ増える\n",
    "        self.stock[food] += 1\n",
    "        # 要求リストから1つ減らす\n",
    "        if (self.current_requests[food] != 0):\n",
    "            self.current_requests[food] -= 1\n",
    "\n",
    "        # self.check_food_complete()\n",
    "\n",
    "    def check_food_complete(self, env_stock):\n",
    "        # print(f\"要求: {self.current_requests}\")\n",
    "        if np.all(self.current_requests == 0):\n",
    "            # 要求がすべて満たされたことを記録\n",
    "            return True\n",
    "            # print(f\"{self.name} 要求がすべて満たされました\")\n",
    "        else:\n",
    "            # 要求が満たされてない場合、食品がまだ余っているか在庫をチェック\n",
    "            # 欲しい食品の在庫\n",
    "            required_food_stock =  env_stock[self.current_requests > 0]\n",
    "            # 欲しい食品の在庫がすべてない\n",
    "            if np.all(required_food_stock == 0):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def get_violation(self):\n",
    "        diffs = self.REQUEST - self.stock\n",
    "        diff_rates = diffs / self.REQUEST * 10\n",
    "        abs_diffs = np.absolute(diff_rates)\n",
    "        violation = np.sum(abs_diffs)\n",
    "\n",
    "        # if (abs(violation) > 100):\n",
    "        #     print(f\"violation: {violation}\")\n",
    "\n",
    "        self.violation = violation\n",
    "        return violation\n",
    "\n",
    "\n",
    "    def print_state(self, state):\n",
    "        num = NUM_FOODS\n",
    "        print(f\"{self.name} State: \", end=\"\", file=self.f)\n",
    "\n",
    "        print(\"Remaining[ \", end=\"\", file=self.f)\n",
    "        for i in range(num):\n",
    "            print(f\"{state[i].name} \", end=\"\", file=self.f)\n",
    "        print(\"], Change[ \", end=\"\", file=self.f)\n",
    "        for i in range(num, num * 2):\n",
    "            print(f\"{state[i].name} \", end=\"\", file=self.f)\n",
    "        print(\"], Satisfaction[ \", end=\"\", file=self.f)\n",
    "        for i in range(num * 2, num * 3):\n",
    "            print(f\"{state[i].name} \", end=\"\", file=self.f)\n",
    "\n",
    "        print(f\"] Progress[{state[num * 3].name}]\", file=self.f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "        self.agents = self.init_agents()\n",
    "\n",
    "    def init_agents(self):\n",
    "        state_size = pow(len(StockRemaining), NUM_FOODS) * pow(len(StockChange),\n",
    "                                                                  NUM_FOODS) * pow(len(Satisfaction), NUM_FOODS) * len(Progress)\n",
    "        num_states = NUM_FOODS * 3 + 1\n",
    "        num_actions = NUM_FOODS + 1\n",
    "\n",
    "        print(\"========= 各エージェントのもつ状態行動空間 =========\", file=self.f)\n",
    "        print(f\"状態数: {state_size:,}\", file=self.f)\n",
    "        print(f\"行動数: {num_actions}\", file=self.f)\n",
    "        print(f\"状態 × 行動の組み合わせ: {(state_size * num_actions):,}\", file=self.f)\n",
    "        print(\"\\n\\n\", file=self.f)\n",
    "\n",
    "        agents: List[Agent] = []\n",
    "        for i in range(AGENTS_COUNT):\n",
    "            name = f\"Agent{i + 1}\"\n",
    "            agent = Agent(\n",
    "                name, np.array(REQUESTS[i]), num_states, num_actions, self.f)\n",
    "            agents.append(agent)\n",
    "        return agents\n",
    "\n",
    "    def reset(self, greedy):\n",
    "        self.stock = np.array(FOODS, dtype=np.int64)\n",
    "        states = None\n",
    "\n",
    "        for agent in self.agents:\n",
    "            agent.reset(self.stock, greedy)\n",
    "\n",
    "        return states\n",
    "\n",
    "    def get_actions(self, states):\n",
    "        actions = []\n",
    "        # すべてのエージェントに対して\n",
    "        for agent, state in zip(self.agents, states):\n",
    "            # 行動を決定\n",
    "            action = agent.get_action(state)\n",
    "            actions.append(action)\n",
    "            if action == NUM_FOODS:\n",
    "                # print(f\"{agent.name} 行動: 何もしない\")\n",
    "                pass\n",
    "            else:\n",
    "                # print(f\"{agent.name} 行動: 食品{action}を１つ取る\")\n",
    "                pass\n",
    "        return actions\n",
    "\n",
    "    def check_food_run_out(self):\n",
    "        # 全ての在庫が0になったかチェック\n",
    "        return np.all(self.stock == 0)\n",
    "\n",
    "    def check_agents_food_complete(self):\n",
    "        # 全てのエージェントが終了条件を満たしているかチェック\n",
    "        all_done = True\n",
    "        for agent in self.agents:\n",
    "            if not agent.check_food_complete(self.stock):\n",
    "                all_done = False\n",
    "                break\n",
    "        # 全エージェントの取れる行動がなくなったか\n",
    "        return all_done\n",
    "\n",
    "    def check_agents_learning_done(self):\n",
    "        all_done = True\n",
    "        for agent in self.agents:\n",
    "            if not agent.learning_done:\n",
    "                all_done = False\n",
    "                break\n",
    "        return all_done\n",
    "\n",
    "    # def learn(self, states, actions, rewards, states_next, alpha):\n",
    "    #     for agent, state, action, reward, state_next in zip(self.agents, states, actions, rewards, states_next):\n",
    "    #         agent.learn(state, action, reward, state_next, alpha)\n",
    "\n",
    "    def get_reward(self, target_agent: Agent, terminal, greedy):\n",
    "        if terminal:\n",
    "            # - |制約違反度の平均からの偏差|\n",
    "            # violations = []\n",
    "            # for agent in self.agents:\n",
    "            #     v = agent.get_violation()\n",
    "            #     violations.append(v)\n",
    "            # mean = np.mean(violations)\n",
    "            # abs_deviation = np.absolute(mean - target_agent.get_violation())\n",
    "            # reward = - abs_deviation\n",
    "\n",
    "            # - (制約違反度 + 制約違反度の標準偏差)\n",
    "            violations = []\n",
    "            for agent in self.agents:\n",
    "                v = agent.get_violation()\n",
    "                # print(f\"violation: {v}\")\n",
    "                violations.append(v)\n",
    "            std = np.std(violations)\n",
    "            # print(f\"std: {std}\")\n",
    "            \n",
    "            reward = - (target_agent.get_violation() + std * 10)\n",
    "            # reward = 0\n",
    "            # print(f\"reward: {reward}\")\n",
    "\n",
    "            # - (制約違反度の平均+標準偏差)　統一\n",
    "            # violations = []\n",
    "            # for agent in self.agents:\n",
    "            #     v = agent.get_violation()\n",
    "            #     violations.append(v)\n",
    "            # mean = np.mean(violations)\n",
    "            # std = np.std(violations)\n",
    "            # reward = - (mean + std)\n",
    "\n",
    "            if greedy:\n",
    "                print(\n",
    "                    f\"{target_agent.name}: 報酬{reward:.3f}  要求{target_agent.REQUEST} 在庫{target_agent.stock}\", file=self.f)\n",
    "\n",
    "        else:\n",
    "            reward = -1\n",
    "            # reward = 0\n",
    "       \n",
    "        return reward\n",
    "\n",
    "    def print_env_state(self):\n",
    "        print(\"Env State: [\", end=\"\", file=self.f)\n",
    "        for status in self.env_state:\n",
    "            print(f\"{status.name} \", end=\"\", file=self.f)\n",
    "\n",
    "        print(\"]\", file=self.f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メイン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.set_printoptions(precision=5, floatmode='maxprec_equal')\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "DIR_PATH = \"D:\\\\Lighthouse\\\\Documents\\\\Reinforcement Learning\\\\Food Distribution\\\\results\"\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "file_name_time = \"{0:%Y-%m-%d_%H%M%S}\".format(start_time)\n",
    "\n",
    "log_name = f\"log_{file_name_time}.txt\"\n",
    "log_path = os.path.join(DIR_PATH, \"logs\", log_name)\n",
    "f = open(log_path, mode=\"w\", encoding=\"UTF-8\")\n",
    "\n",
    "print(\"フードバンク食品分配 DDQN\\n\", file=f)\n",
    "\n",
    "print(\"開始時刻: {0:%Y/%m/%d %H:%M:%S}\\n\".format(start_time), file=f)\n",
    "\n",
    "print(\"====================== 環境設定 ======================\", file=f)\n",
    "print(f\"食品の種類: {NUM_FOODS}\", file=f)\n",
    "print(f\"本部の在庫: {FOODS}\", file=f)\n",
    "print(f\"エージェント数: {AGENTS_COUNT}\", file=f)\n",
    "print(f\"エージェントの要求リスト: {REQUESTS}\", file=f)\n",
    "print(\"\\n\\n\", file=f)\n",
    "\n",
    "print(\"==================== 学習パラメーター ====================\", file=f)\n",
    "print(f\"エピソード数: {NUM_EPISODES:,}\", file=f)\n",
    "print(f\"最大ステップ数: {MAX_STEPS:,}\\n\", file=f)\n",
    "print(f\"割引率: {GAMMA}\\n\", file=f)\n",
    "print(f\"学習率: {ALPHA}\", file=f)\n",
    "print(f\"εの初期値: {INITIAL_EPSILON}\", file=f)\n",
    "print(f\"εの最終値: {MINIMUM_EPSILON}\", file=f)\n",
    "print(f\"1エピソードごとのεの減少値: {EPSILON_DELTA:.7f}\\n\", file=f)\n",
    "print(f\"隠れニューロン数: {HIDDEN_SIZE}\", file=f)\n",
    "print(f\"経験再生メモリ容量: {CAPACITY}\", file=f)\n",
    "print(f\"バッチサイズ: {BATCH_SIZE}\", file=f)\n",
    "print(f\"ターゲットネットワークの更新間隔EP: {TARGET_UPDATE}\", file=f)\n",
    "print(\"\\n\\n\", file=f)\n",
    "\n",
    "print(\"====================== 報酬設定 ======================\", file=f)\n",
    "print(\"- (制約違反度 + 制約違反度の標準偏差)\", file=f)\n",
    "# print(\"- |制約違反度の平均からの偏差|\", file=f)\n",
    "# print(\"- (制約違反度の平均 + 標準偏差) 統一\", file=f)\n",
    "print(\"\\n\\n\", file=f)\n",
    "\n",
    "env = Environment(f)\n",
    "\n",
    "result_episodes = []\n",
    "results_agents_y = [[], [], []]\n",
    "\n",
    "result_ave = []\n",
    "result_dev = []\n",
    "\n",
    "epsilon = INITIAL_EPSILON\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(3, 1, 1)\n",
    "ax2 = fig.add_subplot(3, 1, 2)\n",
    "ax3 = fig.add_subplot(3, 1, 3)\n",
    "\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax2.set_xlabel(\"Episode\")\n",
    "ax3.set_xlabel(\"Episode\")\n",
    "\n",
    "ax1.set_ylabel(\"Constraint Violations\")\n",
    "ax2.set_ylabel(\"Mean\")\n",
    "ax3.set_ylabel(\"Variance\")\n",
    "\n",
    "line1, = ax1.plot([], [], label=\"Agent1\")\n",
    "line2, = ax1.plot([], [], label=\"Agent2\")\n",
    "line3, = ax1.plot([], [], label=\"Agent3\")\n",
    "\n",
    "line_ave, = ax2.plot([], [])\n",
    "line_dev, = ax3.plot([], [])\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "lines = [line1, line2, line3]\n",
    "\n",
    "# 各エピソード\n",
    "for episode in range(NUM_EPISODES):\n",
    "\n",
    "    if episode % GREEDY_CYCLE == 0:\n",
    "        greedy = True\n",
    "        print(\n",
    "            f\"-------------- Episode:{episode} (greedy) --------------\")\n",
    "        print(\n",
    "            f\"\\n\\n-------------- Episode:{episode} (greedy) ----------------------------------------------------------------------\", file=f)\n",
    "    else:\n",
    "        greedy = False\n",
    "        # print(f\"-------------- Episode:{episode} --------------\")\n",
    "    states = [[], [], []]\n",
    "    actions = [[], [], []]\n",
    "    rewards = [0, 0, 0]\n",
    "\n",
    "    old_states = env.reset(greedy)\n",
    "    old_actions = []\n",
    "\n",
    "    learning_complete = False\n",
    "    step = 0\n",
    "\n",
    "    if greedy:\n",
    "        # print(f\"学習率: {alpha:.5f}\", file=f)\n",
    "        print(f\"ε: {epsilon:.5f}\", file=f)\n",
    "\n",
    "    # 各ステップ\n",
    "    while True:\n",
    "        if greedy:\n",
    "            print(f\"\\n------- Step:{step} -------\", file=f)\n",
    "\n",
    "        # 各エージェント\n",
    "        for index, agent in enumerate(env.agents):\n",
    "\n",
    "            # 終了条件確認\n",
    "            # food_run_out = env.check_food_run_out()\n",
    "            all_agents_food_compolete = env.check_agents_food_complete()\n",
    "            exceed_max_step = step == MAX_STEPS - 1\n",
    "\n",
    "            episode_terminal = all_agents_food_compolete or exceed_max_step\n",
    "\n",
    "            if greedy:\n",
    "                print(f\"\\n本部の在庫: {env.stock}\", file=f)\n",
    "                print(f\"{agent.name}の在庫:{agent.stock} 要求:{agent.current_requests}\", file=f)\n",
    "                diff = agent.old_env_stock - env.stock\n",
    "                print(f\"{agent.name}視点の本部在庫の変化:{diff}\", file=f)\n",
    "                if episode_terminal:\n",
    "                    print(\"*** 終了条件を満たしています ***\", file=f)\n",
    "\n",
    "            # 状態を観測\n",
    "            state = agent.observe_state(env.stock, episode_terminal)\n",
    "            states[index] = state\n",
    "\n",
    "            if greedy:\n",
    "                agent.print_state(state)\n",
    "\n",
    "            # 行動を決定\n",
    "            if not episode_terminal:\n",
    "                action = agent.get_action(\n",
    "                    state, env.stock, greedy, epsilon)\n",
    "                actions[index] = action\n",
    "\n",
    "                if greedy:\n",
    "                    if action == NUM_FOODS:\n",
    "                        action_string = \"何もしない\"\n",
    "                    else:\n",
    "                        action_string = f\"食品{action}\"\n",
    "\n",
    "                    print(\n",
    "                        f\"{agent.name} 選択した行動: {action_string}\", file=f)\n",
    "\n",
    "                # 行動をとる\n",
    "                if action != NUM_FOODS:\n",
    "                    # エージェントが食品を1つとる\n",
    "                    agent.grab_food(action)\n",
    "                    # 本部の在庫が1つ減る\n",
    "                    env.stock[action] -= 1\n",
    "\n",
    "            reward = env.get_reward(agent, episode_terminal, greedy)\n",
    "\n",
    "            if old_states is not None:\n",
    "                agent.memorize(old_states[index], old_actions[index], state, reward, greedy)\n",
    "                agent.learn(greedy)\n",
    "\n",
    "            if episode_terminal:\n",
    "                if greedy:\n",
    "                    print(f\"{agent.name} 学習終了\\n\", file=f)\n",
    "\n",
    "                rewards[index] = reward\n",
    "                agent.learning_done = True\n",
    "                learning_complete = env.check_agents_learning_done()\n",
    "                if learning_complete:\n",
    "                    break\n",
    "\n",
    "        if exceed_max_step and greedy:\n",
    "            print(\"最大ステップ数を超えました\", file=f)\n",
    "\n",
    "        if learning_complete:\n",
    "            break\n",
    "\n",
    "        old_states = copy.deepcopy(states)\n",
    "        old_actions = copy.deepcopy(actions)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "\n",
    "    if epsilon > MINIMUM_EPSILON:\n",
    "        epsilon = epsilon - EPSILON_DELTA\n",
    "\n",
    "    if (episode % TARGET_UPDATE == 0):\n",
    "        for agent in env.agents:\n",
    "            agent.update_target_q_function()\n",
    "\n",
    "    if greedy:\n",
    "        print(\n",
    "            f\"************ Episode{episode}の結果 ************\", file=f)\n",
    "        print(f\"要したステップ数: {step + 1}\", file=f)\n",
    "        print(f\"要したステップ数: {step + 1}\")\n",
    "        print(f\"食品の残り個数: {np.sum(env.stock)}\", file=f)\n",
    "        violations = []\n",
    "        for agent, reward in zip(env.agents, rewards):\n",
    "            print(\n",
    "                f\"{agent.name}の在庫: {agent.stock}  制約違反度: {agent.violation:.3f}  報酬: {reward:.3f}\", file=f)\n",
    "            print(f\"{agent.name} reward: {reward:.3f}\")\n",
    "            violations.append(agent.violation)\n",
    "\n",
    "        mean = np.average(violations)\n",
    "        var = np.std(violations)\n",
    "\n",
    "        print(f\"制約違反度の平均: {mean:.3f}  分散: {var:.3f}\", file=f)\n",
    "        # print(f\"現在のエピソード: {episode}\")\n",
    "        # result_reward_x.append(episode)\n",
    "        # result_reward_y.append(rewards[0])\n",
    "        # result_optimal_reward_x.append(episode)\n",
    "        # result_optimal_reward_y.append(optimal_reward)\n",
    "        # lines1.set_data(result_reward_x, result_reward_y)\n",
    "        result_episodes.append(episode)\n",
    "        result_ave.append(mean)\n",
    "        result_dev.append(var)\n",
    "\n",
    "        for i, agent, line, result in zip(range(AGENTS_COUNT), env.agents, lines, results_agents_y):\n",
    "            # result.append(rewards[i])\n",
    "            result.append(violations[i])\n",
    "            line.set_data(result_episodes, result)\n",
    "\n",
    "        line_ave.set_data(result_episodes, result_ave)\n",
    "        line_dev.set_data(result_episodes, result_dev)\n",
    "\n",
    "        # lines1.set_data(result_agents_x, result_agent1_y)\n",
    "        # lines2.set_data(result_agents_x, result_agent2_y)\n",
    "        # lines3.set_data(result_agents_x, result_agent3_y)\n",
    "\n",
    "        ax1.relim()\n",
    "        ax1.autoscale_view()\n",
    "\n",
    "        ax2.relim()\n",
    "        ax2.autoscale_view()\n",
    "\n",
    "        ax3.relim()\n",
    "        ax3.autoscale_view()\n",
    "\n",
    "        # lines1.set_data(result_satisfaction_x, result_satisfaction_y)\n",
    "\n",
    "        # lines2.set_data(result_optimal_reward_x, result_optimal_reward_y)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "# print(\"\\n---- 最適解 ----\")\n",
    "# for agent, loss, stock, request in zip(env.agents, optimal_loss_values, optimal_agent_stock, REQUESTS):\n",
    "#     diff = stock - request\n",
    "#     print(f\"{agent.name}: 損失{loss:.1f} 要求との差{diff} 在庫{stock}\")\n",
    "# print(f\"報酬: {optimal_reward:.4f}\")\n",
    "# print(f\"発見したエピソード: {optimal_episode}\")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"\\n終了時刻: {0:%Y/%m/%d %H:%M:%S}\".format(end_time), file=f)\n",
    "\n",
    "figure_name = f\"figure_{file_name_time}.png\"\n",
    "figure_path = os.path.join(DIR_PATH, \"figures\", figure_name)\n",
    "# plt.savefig(figure_path)\n",
    "\n",
    "# csv_name = f\"data_{file_name_time}.csv\"\n",
    "# csv_path = os.path.join(DIR_PATH, \"data\", csv_name)\n",
    "# with open(csv_path, \"w\", newline='', encoding=\"UTF-8\") as csv_f:\n",
    "#     writer = csv.writer(csv_f)\n",
    "#     writer.writerow([\"Episode\", \"Distance\"])\n",
    "#     writer.writerows(self.result)\n",
    "\n",
    "print(\"\\n\\n終了\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3627c14751eaa7c73cc8d47219c6adb280d3451f32f6ace3fb2ffdf3eb5f2175"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
