{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3、5.4  PyTorchでDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "from IPython.display import HTML, display\n",
    "from matplotlib import animation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動画の描画関数の宣言\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0),\n",
    "               dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        img = patch.set_data(frames[i])\n",
    "        return img\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
    "                                   interval=50)\n",
    "\n",
    "    anim.save('movie_cartpole_DQN.mp4')  # 動画のファイル名と保存です\n",
    "    return HTML(anim.to_jshtml())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr(name_a='名前Aです', value_b=100)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# 本コードでは、namedtupleを使用します。\n",
    "# namedtupleを使うことで、値をフィールド名とペアで格納できます。\n",
    "# すると値に対して、フィールド名でアクセスできて便利です。\n",
    "# https://docs.python.jp/3/library/collections.html#collections.namedtuple\n",
    "# 以下は使用例です\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "Tr = namedtuple('tr', ('name_a', 'value_b'))\n",
    "Tr_object = Tr('名前Aです', 100)\n",
    "\n",
    "print(Tr_object)  # 出力：tr(name_a='名前Aです', value_b=100)\n",
    "print(Tr_object.value_b)  # 出力：100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定数の設定\n",
    "ENV = 'CartPole-v0'  # 使用する課題名\n",
    "GAMMA = 0.99  # 時間割引率\n",
    "MAX_STEPS = 200  # 1試行のstep数\n",
    "NUM_EPISODES = 500  # 最大試行回数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 経験を保存するメモリクラスを定義します\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY  # メモリの最大長さ\n",
    "        self.memory = []  # 経験を保存する変数\n",
    "        self.index = 0  # 保存するindexを示す変数\n",
    "\n",
    "    def push(self, state, action, state_next, reward):\n",
    "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
    "\n",
    "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントが持つ脳となるクラスです、DQNを実行します\n",
    "# Q関数をディープラーニングのネットワークをクラスとして定義\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000\n",
    "\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
    "        self.model.add_module('relu1', nn.ReLU())\n",
    "        self.model.add_module('fc2', nn.Linear(32, 32))\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
    "\n",
    "        print(self.model)  # ネットワークの形を出力\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "\n",
    "    def replay(self):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 1. メモリサイズの確認\n",
    "        # -----------------------------------------\n",
    "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2. ミニバッチの作成\n",
    "        # -----------------------------------------\n",
    "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # 2.2 各変数をミニバッチに対応する形に変形\n",
    "        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
    "        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
    "        # これをミニバッチにしたい。つまり\n",
    "        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
    "        # それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "        # catはConcatenates（結合）のことです。\n",
    "        \n",
    "        # print(f\"batch.action: {batch.action}\")\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        # -----------------------------------------\n",
    "        # 3.1 ネットワークを推論モードに切り替える\n",
    "        self.model.eval()\n",
    "\n",
    "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
    "        # [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
    "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
    "        # それに対応するQ値をgatherでひっぱり出す。\n",
    "\n",
    "        # print(f\"state_batch: {state_batch}\")\n",
    "        # print(f\"出力: {self.model(state_batch)}\")\n",
    "        # print(f\"action_batch: {action_batch}\")\n",
    "        print(f\"reward_batch: {reward_batch}\")\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
    "\n",
    "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                                    batch.next_state)))\n",
    "        # まずは全部0にしておく\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        # 次の状態があるindexの最大Q値を求める\n",
    "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
    "        # そしてそのQ値（index=0）を出力します\n",
    "        # detachでその値を取り出します\n",
    "        next_state_values[non_final_mask] = self.model(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "        print(f\"次状態の最大Q値: {next_state_values}\")\n",
    "\n",
    "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "        print(f\"Q値の教師データ: {expected_state_action_values}\")\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 4. 結合パラメータの更新\n",
    "        # -----------------------------------------\n",
    "        # 4.1 ネットワークを訓練モードに切り替える\n",
    "        self.model.train()\n",
    "\n",
    "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                                expected_state_action_values.unsqueeze(1))\n",
    "        # print(f\"current_q_values: {state_action_values}\")\n",
    "        # print(f\"target_q_values: {expected_state_action_values.unsqueeze(1)}\")\n",
    "\n",
    "        # 4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "        self.optimizer.step()  # 結合パラメータを更新\n",
    "\n",
    "    def decide_action(self, state, episode):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1, 1)\n",
    "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
    "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
    "\n",
    "        else:\n",
    "            # 0,1の行動をランダムに返す\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
    "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
    "\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPoleで動くエージェントクラスです、棒付き台車そのものになります\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        '''課題の状態と行動の数を設定する'''\n",
    "        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n",
    "\n",
    "    def update_q_function(self):\n",
    "        '''Q関数を更新する'''\n",
    "        self.brain.replay()\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        self.brain.memory.push(state, action, state_next, reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPoleを実行する環境のクラスです\n",
    "\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)  # 実行する課題を設定\n",
    "        num_states = self.env.observation_space.shape[0]  # 課題の状態数4を取得\n",
    "        num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n",
    "        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "        episode_final = False  # 最後の試行フラグ\n",
    "        frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
    "\n",
    "        for episode in range(NUM_EPISODES):  # 最大試行数分繰り返す\n",
    "            observation = self.env.reset()  # 環境の初期化\n",
    "\n",
    "            state = observation  # 観測をそのまま状態sとして使用\n",
    "            state = torch.from_numpy(state).type(\n",
    "                torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
    "            state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "\n",
    "                if episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
    "                    frames.append(self.env.render(mode='rgb_array'))\n",
    "\n",
    "                action = self.agent.get_action(state, episode)  # 行動を求める\n",
    "\n",
    "                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "                # actionから.item()を指定して、中身を取り出す\n",
    "                observation_next, _, done, _ = self.env.step(\n",
    "                    action.item())  # rewardとinfoは使わないので_にする\n",
    "\n",
    "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    state_next = None  # 次の状態はないので、Noneを格納\n",
    "\n",
    "                    # 直近10episodeの立てたstep数リストに追加\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))\n",
    "\n",
    "                    if step < 195:\n",
    "                        reward = torch.FloatTensor(\n",
    "                            [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "                        complete_episodes = 0  # 連続成功記録をリセット\n",
    "                    else:\n",
    "                        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "                        complete_episodes = complete_episodes + 1  # 連続記録を更新\n",
    "                else:\n",
    "                    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                    state_next = observation_next  # 観測をそのまま状態とする\n",
    "                    state_next = torch.from_numpy(state_next).type(\n",
    "                        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                    state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "                # メモリに経験を追加\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                # Experience ReplayでQ関数を更新する\n",
    "                self.agent.update_q_function()\n",
    "\n",
    "                # 観測の更新\n",
    "                state = state_next\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "                        episode, step + 1, episode_10_list.mean()))\n",
    "                    break\n",
    "\n",
    "            if episode_final is True:\n",
    "                # 動画を保存と描画\n",
    "                # html = display_frames_as_gif(frames) ## *** 描画結果をHTMLオブジェクトで取得する  ***\n",
    "                # html ## *** IPython系でインラインHTML表示 ***\n",
    "                break\n",
    "\n",
    "            # 10連続で200step経ち続けたら成功\n",
    "            if complete_episodes >= 10:\n",
    "                print('10回連続成功')\n",
    "                episode_final = True  # 次の試行を描画を行う最終試行とする\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "0 Episode: Finished after 10 steps：10試行の平均step数 = 1.0\n",
      "1 Episode: Finished after 9 steps：10試行の平均step数 = 1.9\n",
      "2 Episode: Finished after 12 steps：10試行の平均step数 = 3.1\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0873, 0.0640, 0.0646, 0.0576, 0.0499, 0.0685, 0.0000, 0.0727, 0.0484,\n",
      "        0.0464, 0.0467, 0.0554, 0.0798, 0.0780, 0.0750, 0.0713, 0.0943, 0.0846,\n",
      "        0.0994, 0.1057, 0.0000, 0.0814, 0.0487, 0.0505, 0.0492, 0.0459, 0.0912,\n",
      "        0.0000, 0.0521, 0.0719, 0.1092, 0.0921])\n",
      "Q値の教師データ: tensor([ 0.0865,  0.0634,  0.0639,  0.0571,  0.0494,  0.0678, -1.0000,  0.0720,\n",
      "         0.0479,  0.0460,  0.0463,  0.0549,  0.0790,  0.0773,  0.0742,  0.0706,\n",
      "         0.0934,  0.0838,  0.0984,  0.1047, -1.0000,  0.0806,  0.0482,  0.0500,\n",
      "         0.0487,  0.0454,  0.0903, -1.0000,  0.0515,  0.0712,  0.1081,  0.0912])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0459, 0.0454, 0.0922, 0.0891, 0.0676, 0.0853, 0.0901, 0.0701, 0.0714,\n",
      "        0.0000, 0.1067, 0.0487, 0.0483, 0.0462, 0.0478, 0.0493, 0.0000, 0.0492,\n",
      "        0.0513, 0.0781, 0.0827, 0.1031, 0.0632, 0.0499, 0.0637, 0.0735, 0.0796,\n",
      "        0.0569, 0.0000, 0.0547, 0.0705, 0.0763])\n",
      "Q値の教師データ: tensor([ 0.0455,  0.0449,  0.0913,  0.0882,  0.0669,  0.0844,  0.0892,  0.0694,\n",
      "         0.0707, -1.0000,  0.1056,  0.0482,  0.0478,  0.0458,  0.0473,  0.0488,\n",
      "        -1.0000,  0.0487,  0.0508,  0.0773,  0.0819,  0.1021,  0.0626,  0.0494,\n",
      "         0.0630,  0.0727,  0.0788,  0.0563, -1.0000,  0.0541,  0.0698,  0.0755])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0485, 0.0486, 0.0870, 0.0473, 0.0454, 0.0479, 0.0458, 0.0832, 0.0628,\n",
      "        0.0700, 0.0745, 0.0946, 0.0000, 0.0667, 0.0900, 0.0881, 0.0764, 0.0494,\n",
      "        0.0624, 0.0778, 0.0561, 0.0506, 0.0449, 0.0481, 0.0000, 0.0624, 0.1042,\n",
      "        0.0808, 0.0690, 0.0000, 0.1005, 0.0539])\n",
      "Q値の教師データ: tensor([ 0.0480,  0.0481,  0.0861,  0.0468,  0.0449,  0.0474,  0.0453,  0.0824,\n",
      "         0.0621,  0.0693,  0.0738,  0.0937, -1.0000,  0.0660,  0.0891,  0.0872,\n",
      "         0.0756,  0.0489,  0.0617,  0.0770,  0.0555,  0.0501,  0.0444,  0.0476,\n",
      "        -1.0000,  0.0618,  0.1031,  0.0800,  0.0683, -1.0000,  0.0995,  0.0534])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0615, 0.0922, 0.0658, 0.0677, 0.0000, 0.0478, 0.0678, 0.0705, 0.0978,\n",
      "        0.0448, 0.0616, 0.0443, 0.0488, 0.0498, 0.0618, 0.0000, 0.0467, 0.0000,\n",
      "        0.0879, 0.1017, 0.0727, 0.0861, 0.0479, 0.0681, 0.0453, 0.0789, 0.0687,\n",
      "        0.0760, 0.0476, 0.0811, 0.0849, 0.0532])\n",
      "Q値の教師データ: tensor([ 0.0609,  0.0913,  0.0651,  0.0670, -1.0000,  0.0474,  0.0671,  0.0698,\n",
      "         0.0969,  0.0444,  0.0609,  0.0439,  0.0483,  0.0494,  0.0612, -1.0000,\n",
      "         0.0463, -1.0000,  0.0870,  0.1007,  0.0720,  0.0852,  0.0474,  0.0674,\n",
      "         0.0448,  0.0781,  0.0680,  0.0752,  0.0471,  0.0803,  0.0840,  0.0526])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0827, 0.0858, 0.0663, 0.0690, 0.0666, 0.0000, 0.0448, 0.0438, 0.0709,\n",
      "        0.0470, 0.0606, 0.0670, 0.0742, 0.0649, 0.0491, 0.0000, 0.0952, 0.0524,\n",
      "        0.0483, 0.0462, 0.0992, 0.0730, 0.0898, 0.0841, 0.0545, 0.0674, 0.0609,\n",
      "        0.0709, 0.0791, 0.0000, 0.0607, 0.0443])\n",
      "Q値の教師データ: tensor([ 0.0819,  0.0849,  0.0656,  0.0683,  0.0659, -1.0000,  0.0443,  0.0434,\n",
      "         0.0702,  0.0465,  0.0600,  0.0663,  0.0734,  0.0642,  0.0486, -1.0000,\n",
      "         0.0943,  0.0519,  0.0478,  0.0457,  0.0982,  0.0723,  0.0889,  0.0833,\n",
      "         0.0540,  0.0667,  0.0603,  0.0702,  0.0783, -1.0000,  0.0601,  0.0439])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0465, 0.0926, 0.0713, 0.0465, 0.0724, 0.0874, 0.0433, 0.0600, 0.0538,\n",
      "        0.0693, 0.0658, 0.0517, 0.0640, 0.0456, 0.0598, 0.0000, 0.0438, 0.0000,\n",
      "        0.0661, 0.0675, 0.0836, 0.0654, 0.0967, 0.0466, 0.0821, 0.0465, 0.0750,\n",
      "        0.0443, 0.0692, 0.0477, 0.0806, 0.0599])\n",
      "Q値の教師データ: tensor([ 0.0460,  0.0916,  0.0706,  0.0460,  0.0716,  0.0865,  0.0429,  0.0594,\n",
      "         0.0532,  0.0686,  0.0652,  0.0512,  0.0633,  0.0452,  0.0592, -1.0000,\n",
      "         0.0433, -1.0000,  0.0654,  0.0668,  0.0828,  0.0648,  0.0958,  0.0461,\n",
      "         0.0813,  0.0460,  0.0743,  0.0438,  0.0685,  0.0472,  0.0798,  0.0593])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0635, 0.0660, 0.0677, 0.0731, 0.0815, 0.0451, 0.0459, 0.0510, 0.0591,\n",
      "        0.0785, 0.0893, 0.0589, 0.0631, 0.0943, 0.0000, 0.0459, 0.0471, 0.0000,\n",
      "        0.0460, 0.0851, 0.0428, 0.0801, 0.0432, 0.0706, 0.0648, 0.0697, 0.0591,\n",
      "        0.0674, 0.0438, 0.0772, 0.0647, 0.0477])\n",
      "Q値の教師データ: tensor([ 0.0629,  0.0654,  0.0671,  0.0724,  0.0807,  0.0447,  0.0455,  0.0505,\n",
      "         0.0585,  0.0777,  0.0884,  0.0583,  0.0624,  0.0934, -1.0000,  0.0455,\n",
      "         0.0467, -1.0000,  0.0455,  0.0842,  0.0424,  0.0793,  0.0428,  0.0699,\n",
      "         0.0641,  0.0690,  0.0585,  0.0668,  0.0434,  0.0764,  0.0641,  0.0472])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([0.0875, 0.0000, 0.0452, 0.0423, 0.0631, 0.0581, 0.0662, 0.0827, 0.0466,\n",
      "        0.0991, 0.0446, 0.0434, 0.0919, 0.0622, 0.0470, 0.0583, 0.0765, 0.0523,\n",
      "        0.0680, 0.0455, 0.0782, 0.0636, 0.0872, 0.0622, 0.0795, 0.0658, 0.0754,\n",
      "        0.0713, 0.0453, 0.0454, 0.0635, 0.0000])\n",
      "Q値の教師データ: tensor([ 0.0866, -1.0000,  0.0447,  0.0419,  0.0625,  0.0575,  0.0655,  0.0819,\n",
      "         0.0461,  0.0981,  0.0441,  0.0430,  0.0910,  0.0616,  0.0465,  0.0577,\n",
      "         0.0757,  0.0517,  0.0674,  0.0451,  0.0774,  0.0630,  0.0863,  0.0616,\n",
      "         0.0787,  0.0651,  0.0746,  0.0706,  0.0448,  0.0449,  0.0629, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0., -1.])\n",
      "次状態の最大Q値: tensor([0.0623, 0.0624, 0.0446, 0.0496, 0.0647, 0.0441, 0.0573, 0.0613, 0.0664,\n",
      "        0.0805, 0.0575, 0.0574, 0.0000, 0.0430, 0.0850, 0.0632, 0.0763, 0.0672,\n",
      "        0.0736, 0.0896, 0.0418, 0.0745, 0.0449, 0.0851, 0.1056, 0.0515, 0.0461,\n",
      "        0.0775, 0.0965, 0.0000, 0.0451, 0.0000])\n",
      "Q値の教師データ: tensor([ 0.0616,  0.0618,  0.0442,  0.0491,  0.0640,  0.0436,  0.0567,  0.0607,\n",
      "         0.0658,  0.0797,  0.0570,  0.0568, -1.0000,  0.0425,  0.0841,  0.0625,\n",
      "         0.0755,  0.0665,  0.0728,  0.0887,  0.0414,  0.0737,  0.0444,  0.0842,\n",
      "         0.1046,  0.0510,  0.0456,  0.0767,  0.0956, -1.0000,  0.0446, -1.0000])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0000, 0.0568, 0.0939, 0.0829, 0.0608, 0.0725, 0.0624, 0.0440, 0.0443,\n",
      "        0.0456, 0.0691, 0.0488, 0.0743, 0.0781, 0.0414, 0.0565, 0.0824, 0.0617,\n",
      "        0.0632, 0.0872, 0.0677, 0.0000, 0.0417, 0.0717, 0.0754, 0.0436, 0.0000,\n",
      "        0.0610, 0.0648, 0.0655, 0.0604, 0.0508])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0562,  0.0930,  0.0821,  0.0602,  0.0717,  0.0618,  0.0436,\n",
      "         0.0439,  0.0451,  0.0684,  0.0484,  0.0736,  0.0774,  0.0409,  0.0559,\n",
      "         0.0816,  0.0611,  0.0625,  0.0864,  0.0670, -1.0000,  0.0413,  0.0710,\n",
      "         0.0747,  0.0431, -1.0000,  0.0604,  0.0642,  0.0648,  0.0598,  0.0503])\n",
      "3 Episode: Finished after 10 steps：10試行の平均step数 = 4.1\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0412, 0.0603, 0.0799, 0.0733, 0.0430, 0.0481, 0.0848, 0.0557, 0.0600,\n",
      "        0.0409, 0.0000, 0.0450, 0.0699, 0.0449, 0.0632, 0.0500, 0.0912, 0.0996,\n",
      "        0.0704, 0.0758, 0.0808, 0.0432, 0.0607, 0.0433, 0.0724, 0.0671, 0.0000,\n",
      "        0.0557, 0.0438, 0.0596, 0.0597, 0.0409])\n",
      "Q値の教師データ: tensor([ 0.0408,  0.0597,  0.0791,  0.0726,  0.0426,  0.0476,  0.0840,  0.0551,\n",
      "         0.0594,  0.0404, -1.0000,  0.0445,  0.0692,  0.0444,  0.0625,  0.0495,\n",
      "         0.0903,  0.0986,  0.0697,  0.0750,  0.0800,  0.0428,  0.0600,  0.0429,\n",
      "         0.0716,  0.0664, -1.0000,  0.0551,  0.0434,  0.0590,  0.0591,  0.0405])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0620, 0.0886, 0.0433, 0.0474, 0.0443, 0.0584, 0.0639, 0.0442, 0.0603,\n",
      "        0.0548, 0.0683, 0.0584, 0.0786, 0.0416, 0.0585, 0.0444, 0.0713, 0.0588,\n",
      "        0.0615, 0.0403, 0.0680, 0.0000, 0.0427, 0.0000, 0.0590, 0.0548, 0.0000,\n",
      "        0.0551, 0.0425, 0.0406, 0.0493, 0.0704])\n",
      "Q値の教師データ: tensor([ 0.0614,  0.0877,  0.0428,  0.0469,  0.0438,  0.0578,  0.0633,  0.0437,\n",
      "         0.0597,  0.0542,  0.0677,  0.0578,  0.0778,  0.0412,  0.0579,  0.0440,\n",
      "         0.0706,  0.0583,  0.0609,  0.0399,  0.0673, -1.0000,  0.0423, -1.0000,\n",
      "         0.0584,  0.0543, -1.0000,  0.0546,  0.0421,  0.0402,  0.0488,  0.0697])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([0.0401, 0.0543, 0.0436, 0.0435, 0.0588, 0.0572, 0.0621, 0.0711, 0.0420,\n",
      "        0.0574, 0.0411, 0.0398, 0.0663, 0.0571, 0.0555, 0.0572, 0.0000, 0.0662,\n",
      "        0.0575, 0.0439, 0.0000, 0.0685, 0.0602, 0.0420, 0.0747, 0.0436, 0.0419,\n",
      "        0.0539, 0.0000, 0.0631, 0.0692, 0.0000])\n",
      "Q値の教師データ: tensor([ 0.0397,  0.0537,  0.0432,  0.0430,  0.0582,  0.0566,  0.0615,  0.0704,\n",
      "         0.0415,  0.0568,  0.0407,  0.0394,  0.0656,  0.0566,  0.0549,  0.0567,\n",
      "        -1.0000,  0.0655,  0.0569,  0.0434, -1.0000,  0.0678,  0.0596,  0.0416,\n",
      "         0.0740,  0.0432,  0.0415,  0.0534, -1.0000,  0.0625,  0.0685, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0584, 0.0532, 0.0565, 0.0569, 0.0393, 0.0421, 0.0687, 0.0000, 0.0393,\n",
      "        0.0000, 0.0564, 0.0582, 0.0396, 0.0573, 0.0671, 0.0555, 0.0000, 0.0742,\n",
      "        0.0412, 0.0558, 0.0721, 0.0610, 0.0433, 0.0439, 0.0414, 0.0541, 0.0664,\n",
      "        0.0560, 0.0460, 0.0832, 0.0776, 0.0642])\n",
      "Q値の教師データ: tensor([ 0.0578,  0.0526,  0.0559,  0.0564,  0.0389,  0.0417,  0.0680, -1.0000,\n",
      "         0.0389, -1.0000,  0.0558,  0.0576,  0.0392,  0.0567,  0.0664,  0.0549,\n",
      "        -1.0000,  0.0735,  0.0408,  0.0553,  0.0714,  0.0604,  0.0428,  0.0434,\n",
      "         0.0410,  0.0536,  0.0658,  0.0554,  0.0455,  0.0823,  0.0768,  0.0635])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0., -1., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0427, 0.0804, 0.0401, 0.0537, 0.0416, 0.0620, 0.0452, 0.0590, 0.0644,\n",
      "        0.0720, 0.0405, 0.0554, 0.0694, 0.0420, 0.0556, 0.0623, 0.0526, 0.0558,\n",
      "        0.0000, 0.0442, 0.0552, 0.0000, 0.0000, 0.0422, 0.0407, 0.0000, 0.0649,\n",
      "        0.0872, 0.0523, 0.0527, 0.0566, 0.0524])\n",
      "Q値の教師データ: tensor([ 0.0423,  0.0796,  0.0397,  0.0532,  0.0411,  0.0614,  0.0448,  0.0584,\n",
      "         0.0638,  0.0713,  0.0401,  0.0548,  0.0687,  0.0416,  0.0551,  0.0617,\n",
      "         0.0520,  0.0552, -1.0000,  0.0437,  0.0546, -1.0000, -1.0000,  0.0418,\n",
      "         0.0403, -1.0000,  0.0643,  0.0863,  0.0518,  0.0522,  0.0561,  0.0519])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0515, 0.0539, 0.0840, 0.0726, 0.0542, 0.0528, 0.0510, 0.0548, 0.0624,\n",
      "        0.0543, 0.0399, 0.0604, 0.0519, 0.0563, 0.0445, 0.0513, 0.0413, 0.0410,\n",
      "        0.0396, 0.0697, 0.0776, 0.0512, 0.0000, 0.0421, 0.0000, 0.0530, 0.0382,\n",
      "        0.0569, 0.0000, 0.0462, 0.0628, 0.0514])\n",
      "Q値の教師データ: tensor([ 0.0510,  0.0534,  0.0832,  0.0719,  0.0537,  0.0523,  0.0505,  0.0542,\n",
      "         0.0617,  0.0538,  0.0395,  0.0598,  0.0514,  0.0557,  0.0440,  0.0508,\n",
      "         0.0409,  0.0406,  0.0392,  0.0690,  0.0768,  0.0507, -1.0000,  0.0417,\n",
      "        -1.0000,  0.0525,  0.0378,  0.0563, -1.0000,  0.0457,  0.0621,  0.0509])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0502, 0.0533, 0.0808, 0.0527, 0.0505, 0.0518, 0.0613, 0.0000, 0.0543,\n",
      "        0.0538, 0.0526, 0.0506, 0.0548, 0.0404, 0.0675, 0.0408, 0.0584, 0.0500,\n",
      "        0.0391, 0.0516, 0.0501, 0.0378, 0.0376, 0.0748, 0.0513, 0.0508, 0.0454,\n",
      "        0.0000, 0.0606, 0.0397, 0.0530, 0.0415])\n",
      "Q値の教師データ: tensor([ 0.0497,  0.0527,  0.0800,  0.0522,  0.0500,  0.0513,  0.0607, -1.0000,\n",
      "         0.0538,  0.0533,  0.0521,  0.0501,  0.0542,  0.0400,  0.0668,  0.0403,\n",
      "         0.0579,  0.0495,  0.0387,  0.0511,  0.0496,  0.0375,  0.0372,  0.0741,\n",
      "         0.0508,  0.0503,  0.0450, -1.0000,  0.0600,  0.0393,  0.0524,  0.0411])\n",
      "reward_batch: tensor([ 0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0498, 0.0450, 0.0000, 0.0000, 0.0485, 0.0777, 0.0552, 0.0527, 0.0589,\n",
      "        0.0483, 0.0499, 0.0583, 0.0522, 0.0593, 0.0398, 0.0524, 0.0514, 0.0000,\n",
      "        0.0401, 0.0497, 0.0721, 0.0512, 0.0392, 0.0446, 0.0503, 0.0491, 0.0512,\n",
      "        0.0614, 0.0398, 0.0370, 0.0514, 0.0387])\n",
      "Q値の教師データ: tensor([ 0.0493,  0.0446, -1.0000, -1.0000,  0.0480,  0.0769,  0.0546,  0.0522,\n",
      "         0.0583,  0.0478,  0.0494,  0.0577,  0.0517,  0.0587,  0.0394,  0.0519,\n",
      "         0.0509, -1.0000,  0.0397,  0.0492,  0.0713,  0.0507,  0.0388,  0.0442,\n",
      "         0.0498,  0.0486,  0.0507,  0.0608,  0.0394,  0.0367,  0.0508,  0.0383])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([0.0497, 0.0482, 0.0378, 0.0693, 0.0529, 0.0501, 0.0497, 0.0386, 0.0484,\n",
      "        0.0746, 0.0494, 0.0393, 0.0403, 0.0652, 0.0512, 0.0422, 0.0563, 0.0000,\n",
      "        0.0489, 0.0588, 0.0381, 0.0489, 0.0573, 0.0000, 0.0483, 0.0505, 0.0466,\n",
      "        0.0454, 0.0392, 0.0471, 0.0439, 0.0000])\n",
      "Q値の教師データ: tensor([ 0.0492,  0.0477,  0.0374,  0.0686,  0.0524,  0.0496,  0.0492,  0.0382,\n",
      "         0.0480,  0.0738,  0.0489,  0.0390,  0.0399,  0.0645,  0.0506,  0.0418,\n",
      "         0.0558, -1.0000,  0.0484,  0.0582,  0.0377,  0.0484,  0.0567, -1.0000,\n",
      "         0.0478,  0.0500,  0.0461,  0.0449,  0.0388,  0.0466,  0.0434, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0542, 0.0431, 0.0482, 0.0608, 0.0480, 0.0475, 0.0000, 0.0543, 0.0487,\n",
      "        0.0468, 0.0397, 0.0666, 0.0478, 0.0480, 0.0381, 0.0384, 0.0376, 0.0714,\n",
      "        0.0448, 0.0489, 0.0541, 0.0480, 0.0386, 0.0626, 0.0371, 0.0000, 0.0486,\n",
      "        0.0387, 0.0482, 0.0506, 0.0552, 0.0486])\n",
      "Q値の教師データ: tensor([ 0.0537,  0.0427,  0.0477,  0.0602,  0.0475,  0.0471, -1.0000,  0.0537,\n",
      "         0.0482,  0.0464,  0.0393,  0.0659,  0.0473,  0.0475,  0.0377,  0.0380,\n",
      "         0.0372,  0.0707,  0.0443,  0.0484,  0.0535,  0.0475,  0.0383,  0.0619,\n",
      "         0.0367, -1.0000,  0.0482,  0.0383,  0.0477,  0.0501,  0.0546,  0.0481])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0443, 0.0532, 0.0423, 0.0464, 0.0000, 0.0382, 0.0491, 0.0355, 0.0600,\n",
      "        0.0371, 0.0639, 0.0465, 0.0454, 0.0431, 0.0684, 0.0463, 0.0536, 0.0391,\n",
      "        0.0000, 0.0472, 0.0471, 0.0408, 0.0000, 0.0377, 0.0365, 0.0356, 0.0478,\n",
      "        0.0462, 0.0380, 0.0521, 0.0460, 0.0517])\n",
      "Q値の教師データ: tensor([ 0.0439,  0.0527,  0.0419,  0.0459, -1.0000,  0.0378,  0.0486,  0.0351,\n",
      "         0.0594,  0.0367,  0.0633,  0.0460,  0.0449,  0.0426,  0.0677,  0.0459,\n",
      "         0.0531,  0.0387, -1.0000,  0.0467,  0.0467,  0.0404, -1.0000,  0.0373,\n",
      "         0.0361,  0.0352,  0.0474,  0.0458,  0.0376,  0.0516,  0.0456,  0.0512])\n",
      "4 Episode: Finished after 11 steps：10試行の平均step数 = 5.2\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0463, 0.0449, 0.0370, 0.0465, 0.0348, 0.0460, 0.0362, 0.0592, 0.0574,\n",
      "        0.0503, 0.0000, 0.0439, 0.0000, 0.0385, 0.0509, 0.0447, 0.0441, 0.0416,\n",
      "        0.0376, 0.0000, 0.0000, 0.0350, 0.0452, 0.0512, 0.0565, 0.0366, 0.0653,\n",
      "        0.0349, 0.0000, 0.0373, 0.0400, 0.0370])\n",
      "Q値の教師データ: tensor([ 0.0458,  0.0444,  0.0367,  0.0460,  0.0345,  0.0455,  0.0358,  0.0586,\n",
      "         0.0568,  0.0498, -1.0000,  0.0435, -1.0000,  0.0381,  0.0504,  0.0442,\n",
      "         0.0436,  0.0412,  0.0373, -1.0000, -1.0000,  0.0347,  0.0448,  0.0507,\n",
      "         0.0559,  0.0362,  0.0646,  0.0346, -1.0000,  0.0369,  0.0396,  0.0366])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([0.0430, 0.0352, 0.0360, 0.0547, 0.0470, 0.0468, 0.0452, 0.0584, 0.0408,\n",
      "        0.0436, 0.0000, 0.0469, 0.0453, 0.0595, 0.0542, 0.0425, 0.0452, 0.0483,\n",
      "        0.0415, 0.0491, 0.0479, 0.0424, 0.0000, 0.0342, 0.0448, 0.0371, 0.0344,\n",
      "        0.0470, 0.0650, 0.0395, 0.0000, 0.0483])\n",
      "Q値の教師データ: tensor([ 0.0425,  0.0348,  0.0357,  0.0541,  0.0465,  0.0463,  0.0447,  0.0578,\n",
      "         0.0404,  0.0431, -1.0000,  0.0464,  0.0449,  0.0589,  0.0537,  0.0421,\n",
      "         0.0448,  0.0479,  0.0411,  0.0486,  0.0474,  0.0419, -1.0000,  0.0339,\n",
      "         0.0444,  0.0367,  0.0341,  0.0465,  0.0643,  0.0391, -1.0000,  0.0478])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0421, 0.0355, 0.0345, 0.0425, 0.0337, 0.0408, 0.0338, 0.0446, 0.0457,\n",
      "        0.0355, 0.0338, 0.0439, 0.0599, 0.0448, 0.0377, 0.0520, 0.0348, 0.0472,\n",
      "        0.0418, 0.0410, 0.0000, 0.0444, 0.0436, 0.0422, 0.0444, 0.0404, 0.0654,\n",
      "        0.0365, 0.0457, 0.0436, 0.0373, 0.0385])\n",
      "Q値の教師データ: tensor([ 0.0417,  0.0352,  0.0341,  0.0420,  0.0333,  0.0404,  0.0335,  0.0441,\n",
      "         0.0452,  0.0351,  0.0335,  0.0435,  0.0593,  0.0443,  0.0374,  0.0515,\n",
      "         0.0345,  0.0467,  0.0414,  0.0406, -1.0000,  0.0440,  0.0431,  0.0418,\n",
      "         0.0440,  0.0400,  0.0648,  0.0362,  0.0453,  0.0432,  0.0369,  0.0381])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0499, 0.0000, 0.0408, 0.0792, 0.0394, 0.0428, 0.0361, 0.0000, 0.0421,\n",
      "        0.0389, 0.0383, 0.0000, 0.0402, 0.0443, 0.0659, 0.0427, 0.0000, 0.0437,\n",
      "        0.0476, 0.0333, 0.0342, 0.0367, 0.0603, 0.0437, 0.0355, 0.0427, 0.0530,\n",
      "        0.0407, 0.0393, 0.0449, 0.0339, 0.0360])\n",
      "Q値の教師データ: tensor([ 0.0494, -1.0000,  0.0404,  0.0784,  0.0390,  0.0424,  0.0357, -1.0000,\n",
      "         0.0417,  0.0385,  0.0379, -1.0000,  0.0398,  0.0439,  0.0653,  0.0422,\n",
      "        -1.0000,  0.0433,  0.0471,  0.0329,  0.0339,  0.0364,  0.0597,  0.0433,\n",
      "         0.0351,  0.0423,  0.0525,  0.0403,  0.0389,  0.0444,  0.0335,  0.0357])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0000, 0.0414, 0.0608, 0.0398, 0.0393, 0.0503, 0.0417, 0.0414, 0.0431,\n",
      "        0.0429, 0.0385, 0.0399, 0.0369, 0.0362, 0.0327, 0.0380, 0.0361, 0.0424,\n",
      "        0.0332, 0.0431, 0.0481, 0.0349, 0.0355, 0.0800, 0.0000, 0.0418, 0.0375,\n",
      "        0.0427, 0.0000, 0.0371, 0.0371, 0.0370])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0410,  0.0602,  0.0394,  0.0389,  0.0498,  0.0412,  0.0409,\n",
      "         0.0427,  0.0425,  0.0381,  0.0395,  0.0365,  0.0358,  0.0324,  0.0376,\n",
      "         0.0358,  0.0419,  0.0329,  0.0427,  0.0476,  0.0346,  0.0352,  0.0792,\n",
      "        -1.0000,  0.0414,  0.0371,  0.0423, -1.0000,  0.0367,  0.0368,  0.0366])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0412, 0.0352, 0.0341, 0.0360, 0.0327, 0.0402, 0.0395, 0.0344, 0.0382,\n",
      "        0.0329, 0.0363, 0.0000, 0.0363, 0.0613, 0.0349, 0.0498, 0.0340, 0.0402,\n",
      "        0.0374, 0.0409, 0.0357, 0.0411, 0.0405, 0.0350, 0.0000, 0.0321, 0.0357,\n",
      "        0.0418, 0.0454, 0.0487, 0.0443, 0.0420])\n",
      "Q値の教師データ: tensor([ 0.0408,  0.0349,  0.0337,  0.0356,  0.0324,  0.0398,  0.0391,  0.0341,\n",
      "         0.0378,  0.0326,  0.0360, -1.0000,  0.0360,  0.0607,  0.0346,  0.0493,\n",
      "         0.0336,  0.0398,  0.0370,  0.0404,  0.0353,  0.0407,  0.0401,  0.0346,\n",
      "        -1.0000,  0.0318,  0.0353,  0.0414,  0.0449,  0.0482,  0.0438,  0.0416])\n",
      "reward_batch: tensor([-1., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0000, 0.0000, 0.0336, 0.0382, 0.0418, 0.0389, 0.0000, 0.0347, 0.0329,\n",
      "        0.0352, 0.0468, 0.0391, 0.0394, 0.0323, 0.0375, 0.0318, 0.0400, 0.0931,\n",
      "        0.0450, 0.0316, 0.0404, 0.0493, 0.0350, 0.0370, 0.0335, 0.0819, 0.0000,\n",
      "        0.0723, 0.0339, 0.0678, 0.1002, 0.0360])\n",
      "Q値の教師データ: tensor([-1.0000, -1.0000,  0.0332,  0.0378,  0.0413,  0.0385, -1.0000,  0.0344,\n",
      "         0.0325,  0.0348,  0.0463,  0.0387,  0.0390,  0.0320,  0.0371,  0.0315,\n",
      "         0.0396,  0.0922,  0.0445,  0.0313,  0.0400,  0.0488,  0.0347,  0.0366,\n",
      "         0.0332,  0.0810, -1.0000,  0.0716,  0.0336,  0.0671,  0.0992,  0.0356])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.1128, 0.0339, 0.0308, 0.0309, 0.0000, 0.0437, 0.0348, 0.0328, 0.0355,\n",
      "        0.0364, 0.0000, 0.0296, 0.0310, 0.0326, 0.0317, 0.0360, 0.0401, 0.0731,\n",
      "        0.0685, 0.0391, 0.0331, 0.0360, 0.0371, 0.0331, 0.0321, 0.0395, 0.0365,\n",
      "        0.0354, 0.1244, 0.0305, 0.0407, 0.0295])\n",
      "Q値の教師データ: tensor([ 0.1117,  0.0335,  0.0305,  0.0306, -1.0000,  0.0433,  0.0345,  0.0325,\n",
      "         0.0351,  0.0360, -1.0000,  0.0293,  0.0307,  0.0323,  0.0314,  0.0357,\n",
      "         0.0397,  0.0724,  0.0678,  0.0387,  0.0328,  0.0356,  0.0368,  0.0328,\n",
      "         0.0318,  0.0391,  0.0361,  0.0350,  0.1232,  0.0302,  0.0403,  0.0292])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0300, 0.0342, 0.0000, 0.0386, 0.0305, 0.1364, 0.0740, 0.0346, 0.0339,\n",
      "        0.0384, 0.0629, 0.0352, 0.0335, 0.0504, 0.0000, 0.0371, 0.0303, 0.0310,\n",
      "        0.0395, 0.0397, 0.0319, 0.0316, 0.0000, 0.0306, 0.0347, 0.0355, 0.0318,\n",
      "        0.0316, 0.0363, 0.0367, 0.0339, 0.0303])\n",
      "Q値の教師データ: tensor([ 0.0297,  0.0339, -1.0000,  0.0382,  0.0302,  0.1351,  0.0732,  0.0342,\n",
      "         0.0336,  0.0380,  0.0623,  0.0348,  0.0332,  0.0499, -1.0000,  0.0367,\n",
      "         0.0300,  0.0307,  0.0391,  0.0393,  0.0316,  0.0313, -1.0000,  0.0303,\n",
      "         0.0343,  0.0351,  0.0315,  0.0312,  0.0360,  0.0364,  0.0336,  0.0300])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0286, 0.0304, 0.0268, 0.0302, 0.0309, 0.0326, 0.0000, 0.0300, 0.0346,\n",
      "        0.0254, 0.0263, 0.0334, 0.1153, 0.0313, 0.0261, 0.0334, 0.0371, 0.0000,\n",
      "        0.0965, 0.0359, 0.0850, 0.0361, 0.0000, 0.0339, 0.0377, 0.0305, 0.0302,\n",
      "        0.0388, 0.0307, 0.0698, 0.0319, 0.0377])\n",
      "Q値の教師データ: tensor([ 0.0283,  0.0301,  0.0266,  0.0299,  0.0306,  0.0323, -1.0000,  0.0297,\n",
      "         0.0343,  0.0251,  0.0260,  0.0331,  0.1141,  0.0310,  0.0258,  0.0331,\n",
      "         0.0368, -1.0000,  0.0955,  0.0356,  0.0842,  0.0357, -1.0000,  0.0336,\n",
      "         0.0373,  0.0302,  0.0299,  0.0385,  0.0303,  0.0692,  0.0315,  0.0373])\n",
      "5 Episode: Finished after 10 steps：10試行の平均step数 = 6.2\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0699, 0.0000, 0.0347, 0.0345, 0.0000, 0.0313, 0.1032, 0.0305, 0.0269,\n",
      "        0.0373, 0.1147, 0.0380, 0.0343, 0.0266, 0.0303, 0.0314, 0.0308, 0.0340,\n",
      "        0.0291, 0.0239, 0.0298, 0.0348, 0.0962, 0.0364, 0.1371, 0.1263, 0.0249,\n",
      "        0.0000, 0.0302, 0.0368, 0.0348, 0.0292])\n",
      "Q値の教師データ: tensor([ 0.0692, -1.0000,  0.0343,  0.0341, -1.0000,  0.0310,  0.1022,  0.0302,\n",
      "         0.0266,  0.0369,  0.1135,  0.0376,  0.0340,  0.0263,  0.0300,  0.0311,\n",
      "         0.0304,  0.0336,  0.0289,  0.0237,  0.0295,  0.0344,  0.0953,  0.0360,\n",
      "         0.1357,  0.1250,  0.0246, -1.0000,  0.0299,  0.0365,  0.0344,  0.0289])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0229, 0.0234, 0.0249, 0.0309, 0.0360, 0.0000, 0.0000, 0.0370, 0.0287,\n",
      "        0.0218, 0.0514, 0.0212, 0.0000, 0.0270, 0.0317, 0.0315, 0.0366, 0.0285,\n",
      "        0.0291, 0.0333, 0.0000, 0.0000, 0.0289, 0.1364, 0.0350, 0.1257, 0.0295,\n",
      "        0.0278, 0.0355, 0.0513, 0.0291, 0.1029])\n",
      "Q値の教師データ: tensor([ 0.0227,  0.0231,  0.0246,  0.0306,  0.0356, -1.0000, -1.0000,  0.0366,\n",
      "         0.0284,  0.0215,  0.0509,  0.0210, -1.0000,  0.0267,  0.0314,  0.0312,\n",
      "         0.0362,  0.0282,  0.0288,  0.0330, -1.0000, -1.0000,  0.0286,  0.1350,\n",
      "         0.0346,  0.1244,  0.0292,  0.0275,  0.0351,  0.0508,  0.0288,  0.1019])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0000, 0.0325, 0.0356, 0.0263, 0.0281, 0.0296, 0.0360, 0.0283, 0.0313,\n",
      "        0.0273, 0.0276, 0.0846, 0.0000, 0.0362, 0.0286, 0.0306, 0.0252, 0.0376,\n",
      "        0.0257, 0.0286, 0.0201, 0.0295, 0.0000, 0.0284, 0.0255, 0.0345, 0.0636,\n",
      "        0.0000, 0.0514, 0.0210, 0.1352, 0.1247])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0322,  0.0352,  0.0260,  0.0278,  0.0293,  0.0357,  0.0280,\n",
      "         0.0310,  0.0271,  0.0273,  0.0838, -1.0000,  0.0358,  0.0283,  0.0303,\n",
      "         0.0250,  0.0372,  0.0254,  0.0283,  0.0199,  0.0292, -1.0000,  0.0281,\n",
      "         0.0252,  0.0342,  0.0629, -1.0000,  0.0509,  0.0208,  0.1339,  0.1234])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0276, 0.0950, 0.0253, 0.0318, 0.0306, 0.0634, 0.0000, 0.0216, 0.0841,\n",
      "        0.0344, 0.0235, 0.0282, 0.0268, 0.0175, 0.0186, 0.0249, 0.0377, 0.0361,\n",
      "        0.0300, 0.0000, 0.0000, 0.0239, 0.0000, 0.0237, 0.0247, 0.0336, 0.0276,\n",
      "        0.0295, 0.0000, 0.1234, 0.0173, 0.0310])\n",
      "Q値の教師データ: tensor([ 0.0273,  0.0940,  0.0251,  0.0315,  0.0303,  0.0628, -1.0000,  0.0213,\n",
      "         0.0833,  0.0341,  0.0233,  0.0280,  0.0265,  0.0174,  0.0184,  0.0246,\n",
      "         0.0373,  0.0357,  0.0297, -1.0000, -1.0000,  0.0237, -1.0000,  0.0234,\n",
      "         0.0245,  0.0333,  0.0273,  0.0292, -1.0000,  0.1222,  0.0171,  0.0307])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0669, 0.0268, 0.0235, 0.0364, 0.0195, 0.0213, 0.0223, 0.0334, 0.0345,\n",
      "        0.0301, 0.0944, 0.1223, 0.0269, 0.0233, 0.1113, 0.0267, 0.0515, 0.0209,\n",
      "        0.0295, 0.0271, 0.0286, 0.0373, 0.0273, 0.0339, 0.1010, 0.0361, 0.0000,\n",
      "        0.0228, 0.0838, 0.0352, 0.0161, 0.0344])\n",
      "Q値の教師データ: tensor([ 0.0662,  0.0265,  0.0232,  0.0360,  0.0194,  0.0211,  0.0220,  0.0331,\n",
      "         0.0342,  0.0298,  0.0935,  0.1210,  0.0266,  0.0230,  0.1102,  0.0264,\n",
      "         0.0510,  0.0207,  0.0292,  0.0268,  0.0283,  0.0369,  0.0270,  0.0336,\n",
      "         0.0999,  0.0358, -1.0000,  0.0225,  0.0830,  0.0348,  0.0159,  0.0340])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0264, 0.0147, 0.0000, 0.0323, 0.0192, 0.0251, 0.0308, 0.0296, 0.0189,\n",
      "        0.0378, 0.0517, 0.0520, 0.0317, 0.0223, 0.0179, 0.0633, 0.0201, 0.0192,\n",
      "        0.0330, 0.0220, 0.0219, 0.0213, 0.0000, 0.0256, 0.0209, 0.0286, 0.0940,\n",
      "        0.0124, 0.0266, 0.0668, 0.0249, 0.0337])\n",
      "Q値の教師データ: tensor([ 0.0261,  0.0146, -1.0000,  0.0320,  0.0191,  0.0248,  0.0305,  0.0293,\n",
      "         0.0187,  0.0374,  0.0512,  0.0515,  0.0314,  0.0221,  0.0177,  0.0627,\n",
      "         0.0199,  0.0190,  0.0327,  0.0218,  0.0216,  0.0211, -1.0000,  0.0253,\n",
      "         0.0207,  0.0284,  0.0931,  0.0123,  0.0263,  0.0662,  0.0246,  0.0333])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0322, 0.0262, 0.1001, 0.0243, 0.0668, 0.0000, 0.0518, 0.0204, 0.0278,\n",
      "        0.0106, 0.1096, 0.0172, 0.0379, 0.0000, 0.0185, 0.0185, 0.0245, 0.0287,\n",
      "        0.0364, 0.0191, 0.0634, 0.0375, 0.0161, 0.0834, 0.0148, 0.0313, 0.0254,\n",
      "        0.0328, 0.0695, 0.0115, 0.0194, 0.1203])\n",
      "Q値の教師データ: tensor([ 0.0319,  0.0259,  0.0991,  0.0241,  0.0662, -1.0000,  0.0513,  0.0202,\n",
      "         0.0276,  0.0105,  0.1085,  0.0171,  0.0376, -1.0000,  0.0183,  0.0183,\n",
      "         0.0243,  0.0284,  0.0360,  0.0190,  0.0627,  0.0371,  0.0160,  0.0825,\n",
      "         0.0147,  0.0310,  0.0251,  0.0325,  0.0688,  0.0114,  0.0192,  0.1191])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1., -1.])\n",
      "次状態の最大Q値: tensor([0.0522, 0.0519, 0.0348, 0.0146, 0.0000, 0.0289, 0.0253, 0.0287, 0.0737,\n",
      "        0.0319, 0.0247, 0.0519, 0.0875, 0.0198, 0.0146, 0.0183, 0.0831, 0.0178,\n",
      "        0.0224, 0.0633, 0.0297, 0.0134, 0.0203, 0.0314, 0.0300, 0.0995, 0.0257,\n",
      "        0.0778, 0.0278, 0.0096, 0.0000, 0.0000])\n",
      "Q値の教師データ: tensor([ 0.0517,  0.0514,  0.0345,  0.0144, -1.0000,  0.0286,  0.0250,  0.0284,\n",
      "         0.0730,  0.0316,  0.0245,  0.0513,  0.0866,  0.0196,  0.0145,  0.0181,\n",
      "         0.0823,  0.0177,  0.0222,  0.0627,  0.0294,  0.0133,  0.0201,  0.0311,\n",
      "         0.0297,  0.0985,  0.0255,  0.0770,  0.0275,  0.0095, -1.0000, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0377, 0.0829, 0.0368, 0.0258, 0.0175, 0.0271, 0.0990, 0.0130, 0.0100,\n",
      "        0.1182, 0.0292, 0.0000, 0.0000, 0.0254, 0.0158, 0.0523, 0.0138, 0.0095,\n",
      "        0.0241, 0.0350, 0.0000, 0.0313, 0.0520, 0.0155, 0.0944, 0.0667, 0.0366,\n",
      "        0.0242, 0.0155, 0.0155, 0.0240, 0.0294])\n",
      "Q値の教師データ: tensor([ 0.0373,  0.0821,  0.0365,  0.0256,  0.0173,  0.0269,  0.0980,  0.0128,\n",
      "         0.0099,  0.1170,  0.0289, -1.0000, -1.0000,  0.0251,  0.0156,  0.0518,\n",
      "         0.0137,  0.0094,  0.0239,  0.0346, -1.0000,  0.0309,  0.0515,  0.0154,\n",
      "         0.0934,  0.0660,  0.0362,  0.0240,  0.0154,  0.0154,  0.0238,  0.0291])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0773, 0.0938, 0.0265, 0.0289, 0.0285, 0.0114, 0.0000, 0.0000, 0.0209,\n",
      "        0.0177, 0.0382, 0.0134, 0.0095, 0.0140, 0.0000, 0.0304, 0.0227, 0.0114,\n",
      "        0.0922, 0.0521, 0.0283, 0.0259, 0.0235, 0.0524, 0.0236, 0.0000, 0.0999,\n",
      "        0.0133, 0.0297, 0.0000, 0.0247, 0.0733])\n",
      "Q値の教師データ: tensor([ 0.0766,  0.0929,  0.0262,  0.0287,  0.0282,  0.0113, -1.0000, -1.0000,\n",
      "         0.0207,  0.0175,  0.0378,  0.0133,  0.0094,  0.0139, -1.0000,  0.0301,\n",
      "         0.0225,  0.0113,  0.0913,  0.0515,  0.0280,  0.0256,  0.0232,  0.0519,\n",
      "         0.0234, -1.0000,  0.0989,  0.0132,  0.0295, -1.0000,  0.0244,  0.0726])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0218, 0.0631, 0.0730, 0.0259, 0.0991, 0.0086, 0.0275, 0.0288, 0.0000,\n",
      "        0.0127, 0.0054, 0.0709, 0.0000, 0.0351, 0.0258, 0.0198, 0.0163, 0.0230,\n",
      "        0.0163, 0.0122, 0.0521, 0.0220, 0.0864, 0.0098, 0.0112, 0.0297, 0.0124,\n",
      "        0.1084, 0.0524, 0.0115, 0.0931, 0.0097])\n",
      "Q値の教師データ: tensor([ 0.0216,  0.0625,  0.0722,  0.0257,  0.0982,  0.0085,  0.0272,  0.0285,\n",
      "        -1.0000,  0.0126,  0.0054,  0.0702, -1.0000,  0.0347,  0.0256,  0.0196,\n",
      "         0.0161,  0.0228,  0.0161,  0.0120,  0.0516,  0.0218,  0.0855,  0.0097,\n",
      "         0.0111,  0.0294,  0.0122,  0.1073,  0.0519,  0.0114,  0.0922,  0.0096])\n",
      "reward_batch: tensor([-1., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0000, 0.0000, 0.0147, 0.0314, 0.0123, 0.1144, 0.1042, 0.0521, 0.0000,\n",
      "        0.0069, 0.0371, 0.0112, 0.0110, 0.0000, 0.0970, 0.0379, 0.0288, 0.0107,\n",
      "        0.0207, 0.1072, 0.0081, 0.0175, 0.0260, 0.0727, 0.0214, 0.0663, 0.0227,\n",
      "        0.0358, 0.0137, 0.0121, 0.0289, 0.0819])\n",
      "Q値の教師データ: tensor([-1.0000, -1.0000,  0.0146,  0.0310,  0.0121,  0.1132,  0.1032,  0.0516,\n",
      "        -1.0000,  0.0068,  0.0367,  0.0111,  0.0109, -1.0000,  0.0961,  0.0375,\n",
      "         0.0285,  0.0106,  0.0205,  0.1062,  0.0080,  0.0173,  0.0257,  0.0720,\n",
      "         0.0211,  0.0657,  0.0225,  0.0354,  0.0136,  0.0120,  0.0286,  0.0811])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([0.0150, 0.0227, 0.0215, 0.0106, 0.0059, 0.0260, 0.0906, 0.0000, 0.0211,\n",
      "        0.0280, 0.0525, 0.0000, 0.0154, 0.0115, 0.0523, 0.0078, 0.0083, 0.0522,\n",
      "        0.0195, 0.1235, 0.0352, 0.0135, 0.0817, 0.0104, 0.0073, 0.0965, 0.0368,\n",
      "        0.0214, 0.0206, 0.0288, 0.0175, 0.0000])\n",
      "Q値の教師データ: tensor([ 0.0149,  0.0225,  0.0213,  0.0105,  0.0059,  0.0258,  0.0896, -1.0000,\n",
      "         0.0209,  0.0277,  0.0520, -1.0000,  0.0153,  0.0114,  0.0518,  0.0077,\n",
      "         0.0082,  0.0517,  0.0193,  0.1222,  0.0349,  0.0134,  0.0809,  0.0103,\n",
      "         0.0072,  0.0956,  0.0365,  0.0212,  0.0204,  0.0285,  0.0173, -1.0000])\n",
      "6 Episode: Finished after 13 steps：10試行の平均step数 = 7.5\n",
      "reward_batch: tensor([-1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([0.0000, 0.0143, 0.0000, 0.0685, 0.0091, 0.0025, 0.0722, 0.0381, 0.0148,\n",
      "        0.0704, 0.0000, 0.0369, 0.0111, 0.0245, 0.0068, 0.0226, 0.1021, 0.0123,\n",
      "        0.0359, 0.0662, 0.0072, 0.0092, 0.0184, 0.0150, 0.0256, 0.0107, 0.0853,\n",
      "        0.0297, 0.0265, 0.0203, 0.0072, 0.0000])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0142, -1.0000,  0.0679,  0.0090,  0.0025,  0.0715,  0.0377,\n",
      "         0.0146,  0.0697, -1.0000,  0.0365,  0.0110,  0.0243,  0.0067,  0.0224,\n",
      "         0.1011,  0.0121,  0.0356,  0.0655,  0.0071,  0.0091,  0.0182,  0.0148,\n",
      "         0.0254,  0.0106,  0.0844,  0.0294,  0.0262,  0.0201,  0.0071, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0718, 0.1007, 0.0190, 0.0660, 0.0702, 0.0808, 0.0353, 0.0136, 0.0679,\n",
      "        0.0202, 0.0050, 0.0847, 0.0000, 0.0523, 0.0192, 0.0381, 0.0065, 0.1106,\n",
      "        0.0950, 0.0000, 0.0965, 0.0092, 0.0264, 0.0105, 0.0263, 0.0173, 0.0111,\n",
      "        0.0000, 0.0893, 0.0108, 0.0064, 0.0208])\n",
      "Q値の教師データ: tensor([ 0.0711,  0.0997,  0.0188,  0.0654,  0.0695,  0.0800,  0.0349,  0.0134,\n",
      "         0.0672,  0.0200,  0.0050,  0.0838, -1.0000,  0.0517,  0.0190,  0.0377,\n",
      "         0.0065,  0.1095,  0.0941, -1.0000,  0.0955,  0.0092,  0.0261,  0.0104,\n",
      "         0.0261,  0.0171,  0.0110, -1.0000,  0.0884,  0.0107,  0.0063,  0.0206])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0232, 0.0128, 0.0199, 0.0523, 0.0256, 0.0681, 0.0381, 0.0140, 0.0037,\n",
      "        0.1092, 0.0353, 0.0000, 0.0181, 0.0384, 0.0842, 0.0226, 0.0525, 0.0254,\n",
      "        0.0051, 0.0063, 0.0000, 0.0202, 0.0094, 0.0957, 0.0754, 0.0677, 0.0000,\n",
      "        0.0712, 0.1024, 0.0133, 0.0625, 0.0056])\n",
      "Q値の教師データ: tensor([ 0.0229,  0.0127,  0.0197,  0.0517,  0.0253,  0.0674,  0.0377,  0.0139,\n",
      "         0.0037,  0.1081,  0.0349, -1.0000,  0.0180,  0.0381,  0.0834,  0.0223,\n",
      "         0.0520,  0.0252,  0.0051,  0.0063, -1.0000,  0.0200,  0.0094,  0.0948,\n",
      "         0.0747,  0.0670, -1.0000,  0.0705,  0.1014,  0.0132,  0.0619,  0.0055])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lighthouse\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:101: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([0.0046, 0.0051, 0.0353, 0.0040, 0.0253, 0.0025, 0.0000, 0.0381, 0.0049,\n",
      "        0.0198, 0.0285, 0.0711, 0.0881, 0.0315, 0.0000, 0.0000, 0.0173, 0.0034,\n",
      "        0.0126, 0.0674, 0.1106, 0.0000, 0.0750, 0.0260, 0.0522, 0.0038, 0.1010,\n",
      "        0.0079, 0.1179, 0.0000, 0.0059, 0.0624])\n",
      "Q値の教師データ: tensor([ 0.0045,  0.0051,  0.0350,  0.0040,  0.0251,  0.0024, -1.0000,  0.0377,\n",
      "         0.0049,  0.0196,  0.0282,  0.0704,  0.0872,  0.0312, -1.0000, -1.0000,\n",
      "         0.0172,  0.0034,  0.0125,  0.0668,  0.1094, -1.0000,  0.0743,  0.0257,\n",
      "         0.0517,  0.0038,  0.1000,  0.0078,  0.1167, -1.0000,  0.0058,  0.0618])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([0.0656, 0.0039, 0.0284, 0.0045, 0.0973, 0.0156, 0.0260, 0.0231, 0.0223,\n",
      "        0.0926, 0.0000, 0.0940, 0.0112, 0.0294, 0.0830, 0.0672, 0.0000, 0.0789,\n",
      "        0.0139, 0.0018, 0.0746, 0.0023, 0.0372, 0.0704, 0.0118, 0.0206, 0.0000,\n",
      "        0.0885, 0.0521, 0.0874, 0.0522, 0.0000])\n",
      "Q値の教師データ: tensor([ 0.0650,  0.0039,  0.0282,  0.0044,  0.0963,  0.0155,  0.0257,  0.0229,\n",
      "         0.0220,  0.0917, -1.0000,  0.0931,  0.0111,  0.0291,  0.0822,  0.0665,\n",
      "        -1.0000,  0.0781,  0.0138,  0.0018,  0.0738,  0.0023,  0.0368,  0.0697,\n",
      "         0.0117,  0.0204, -1.0000,  0.0877,  0.0516,  0.0866,  0.0517, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0353,  0.0221,  0.0012,  0.0222,  0.0000,  0.0284,  0.0315,  0.1046,\n",
      "         0.0000,  0.0022,  0.0867,  0.0031,  0.1074,  0.0056,  0.0231,  0.0520,\n",
      "         0.0091,  0.0691,  0.0036, -0.0002,  0.0205,  0.0106,  0.0032,  0.0213,\n",
      "         0.0033,  0.0000,  0.0702,  0.0231,  0.0524,  0.0000,  0.0259,  0.0368])\n",
      "Q値の教師データ: tensor([ 3.4925e-02,  2.1915e-02,  1.1501e-03,  2.2025e-02, -1.0000e+00,\n",
      "         2.8094e-02,  3.1144e-02,  1.0358e-01, -1.0000e+00,  2.1623e-03,\n",
      "         8.5832e-02,  3.0833e-03,  1.0631e-01,  5.5293e-03,  2.2846e-02,\n",
      "         5.1529e-02,  9.0194e-03,  6.8451e-02,  3.5704e-03, -2.2095e-04,\n",
      "         2.0335e-02,  1.0453e-02,  3.2127e-03,  2.1069e-02,  3.2872e-03,\n",
      "        -1.0000e+00,  6.9503e-02,  2.2915e-02,  5.1860e-02, -1.0000e+00,\n",
      "         2.5658e-02,  3.6450e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([0.0250, 0.0093, 0.0698, 0.0653, 0.0380, 0.0095, 0.0616, 0.0359, 0.0969,\n",
      "        0.0101, 0.0523, 0.0952, 0.0259, 0.0000, 0.0048, 0.0916, 0.0184, 0.1059,\n",
      "        0.0070, 0.0000, 0.0520, 0.0737, 0.0016, 0.0785, 0.0000, 0.0000, 0.0029,\n",
      "        0.1032, 0.0818, 0.0031, 0.0315, 0.0000])\n",
      "Q値の教師データ: tensor([ 0.0247,  0.0092,  0.0691,  0.0647,  0.0377,  0.0094,  0.0610,  0.0355,\n",
      "         0.0960,  0.0100,  0.0518,  0.0943,  0.0256, -1.0000,  0.0048,  0.0906,\n",
      "         0.0182,  0.1048,  0.0069, -1.0000,  0.0515,  0.0730,  0.0016,  0.0777,\n",
      "        -1.0000, -1.0000,  0.0029,  0.1021,  0.0810,  0.0030,  0.0311, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0205,  0.0009,  0.0000,  0.0813,  0.1021,  0.0055,  0.0284, -0.0003,\n",
      "         0.0353,  0.0122,  0.0000,  0.0000,  0.0663, -0.0002,  0.0000,  0.0939,\n",
      "         0.0018,  0.0293, -0.0014,  0.0182,  0.0031,  0.0780,  0.0000,  0.0384,\n",
      "         0.0686,  0.0694,  0.0023,  0.0219,  0.0000, -0.0007,  0.0258,  0.0521])\n",
      "Q値の教師データ: tensor([ 2.0264e-02,  9.1870e-04, -1.0000e+00,  8.0517e-02,  1.0109e-01,\n",
      "         5.4152e-03,  2.8068e-02, -2.7804e-04,  3.4907e-02,  1.2059e-02,\n",
      "        -1.0000e+00, -1.0000e+00,  6.5662e-02, -2.0602e-04, -1.0000e+00,\n",
      "         9.2991e-02,  1.8103e-03,  2.8961e-02, -1.3841e-03,  1.8049e-02,\n",
      "         3.0284e-03,  7.7226e-02, -1.0000e+00,  3.7970e-02,  6.7924e-02,\n",
      "         6.8706e-02,  2.3200e-03,  2.1679e-02, -1.0000e+00, -6.4942e-04,\n",
      "         2.5548e-02,  5.1561e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([ 0.0164,  0.0205,  0.0367,  0.0728,  0.0000,  0.0292,  0.0110,  0.0185,\n",
      "         0.1101,  0.0023,  0.1104, -0.0009,  0.0929,  0.0041, -0.0021,  0.0217,\n",
      "         0.0892,  0.0945,  0.0057,  0.0000,  0.0000,  0.0649,  0.0686,  0.0013,\n",
      "         0.0903,  0.0774,  0.0010,  0.0013,  0.0192, -0.0010,  0.0054,  0.0000])\n",
      "Q値の教師データ: tensor([ 1.6194e-02,  2.0301e-02,  3.6357e-02,  7.2035e-02, -1.0000e+00,\n",
      "         2.8905e-02,  1.0856e-02,  1.8335e-02,  1.0900e-01,  2.2392e-03,\n",
      "         1.0930e-01, -8.8048e-04,  9.1957e-02,  4.0669e-03, -2.0347e-03,\n",
      "         2.1519e-02,  8.8286e-02,  9.3587e-02,  5.6340e-03, -1.0000e+00,\n",
      "        -1.0000e+00,  6.4281e-02,  6.7944e-02,  1.2758e-03,  8.9361e-02,\n",
      "         7.6619e-02,  1.0224e-03,  1.3207e-03,  1.8964e-02, -9.5451e-04,\n",
      "         5.3046e-03, -1.0000e+00])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0379,  0.0800,  0.1086,  0.0179,  0.0982,  0.0186,  0.0842,  0.0282,\n",
      "         0.0005,  0.0370,  0.0660,  0.0079, -0.0002,  0.0153,  0.0519,  0.0000,\n",
      "         0.0683,  0.0678, -0.0009,  0.0253,  0.0520,  0.0040,  0.0000,  0.0313,\n",
      "         0.0215,  0.0000,  0.0366,  0.0036,  0.0006,  0.0889,  0.0056,  0.0033])\n",
      "Q値の教師データ: tensor([ 3.7485e-02,  7.9178e-02,  1.0750e-01,  1.7700e-02,  9.7245e-02,\n",
      "         1.8460e-02,  8.3318e-02,  2.7928e-02,  5.4314e-04,  3.6630e-02,\n",
      "         6.5313e-02,  7.7742e-03, -1.5062e-04,  1.5136e-02,  5.1396e-02,\n",
      "        -1.0000e+00,  6.7616e-02,  6.7162e-02, -8.7625e-04,  2.5017e-02,\n",
      "         5.1511e-02,  3.9778e-03, -1.0000e+00,  3.0988e-02,  2.1331e-02,\n",
      "        -1.0000e+00,  3.6264e-02,  3.5760e-03,  6.4340e-04,  8.8007e-02,\n",
      "         5.5034e-03,  3.2259e-03])\n",
      "7 Episode: Finished after 10 steps：10試行の平均step数 = 8.5\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 2.7816e-03,  7.9242e-02, -9.6952e-05,  1.8775e-02, -2.5015e-03,\n",
      "        -2.3562e-03,  9.0492e-02,  5.6630e-03,  0.0000e+00, -2.1508e-03,\n",
      "        -3.4143e-03,  8.2772e-02, -7.1828e-04,  6.2436e-02,  5.1871e-02,\n",
      "         1.4405e-03,  4.8660e-03,  1.8738e-02,  9.8921e-02, -4.6162e-04,\n",
      "         4.3451e-03,  8.9498e-03,  1.8784e-02,  2.4250e-02,  5.1804e-02,\n",
      "         2.9020e-02,  2.5479e-02,  6.5126e-02,  0.0000e+00,  1.7689e-02,\n",
      "         6.0346e-02,  8.3154e-02])\n",
      "Q値の教師データ: tensor([ 2.7538e-03,  7.8450e-02, -9.5982e-05,  1.8588e-02, -2.4765e-03,\n",
      "        -2.3327e-03,  8.9587e-02,  5.6064e-03, -1.0000e+00, -2.1293e-03,\n",
      "        -3.3802e-03,  8.1944e-02, -7.1110e-04,  6.1812e-02,  5.1352e-02,\n",
      "         1.4261e-03,  4.8173e-03,  1.8551e-02,  9.7932e-02, -4.5700e-04,\n",
      "         4.3016e-03,  8.8603e-03,  1.8596e-02,  2.4007e-02,  5.1286e-02,\n",
      "         2.8730e-02,  2.5224e-02,  6.4475e-02, -1.0000e+00,  1.7512e-02,\n",
      "         5.9743e-02,  8.2323e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0016, -0.0013,  0.0006,  0.0668,  0.0169,  0.0000,  0.0000,  0.0622,\n",
      "        -0.0032,  0.0517,  0.0092,  0.0000,  0.0517,  0.0311,  0.1047, -0.0028,\n",
      "         0.0709,  0.0018,  0.0047,  0.0902,  0.0893,  0.0028,  0.0000,  0.0671,\n",
      "         0.0254, -0.0013,  0.0355,  0.1050,  0.0056, -0.0031,  0.0822,  0.0380])\n",
      "Q値の教師データ: tensor([ 1.5988e-03, -1.2832e-03,  6.0418e-04,  6.6157e-02,  1.6779e-02,\n",
      "        -1.0000e+00, -1.0000e+00,  6.1535e-02, -3.2170e-03,  5.1208e-02,\n",
      "         9.1131e-03, -1.0000e+00,  5.1186e-02,  3.0832e-02,  1.0362e-01,\n",
      "        -2.7293e-03,  7.0217e-02,  1.7761e-03,  4.6896e-03,  8.9309e-02,\n",
      "         8.8431e-02,  2.8040e-03, -1.0000e+00,  6.6439e-02,  2.5118e-02,\n",
      "        -1.3293e-03,  3.5177e-02,  1.0391e-01,  5.5054e-03, -3.0689e-03,\n",
      "         8.1354e-02,  3.7639e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0877, -0.0040, -0.0034, -0.0018,  0.0021,  0.1030,  0.0082,  0.0173,\n",
      "         0.0813,  0.0516,  0.0693,  0.0887,  0.0239,  0.0602,  0.0020,  0.0068,\n",
      "         0.0858,  0.0354, -0.0015, -0.0021,  0.0041, -0.0021,  0.0000,  0.0138,\n",
      "         0.0009, -0.0038, -0.0037,  0.0516,  0.0289,  0.0618,  0.0210,  0.0855])\n",
      "Q値の教師データ: tensor([ 8.6833e-02, -3.9541e-03, -3.3395e-03, -1.7997e-03,  2.0632e-03,\n",
      "         1.0201e-01,  8.0827e-03,  1.7144e-02,  8.0443e-02,  5.1037e-02,\n",
      "         6.8643e-02,  8.7809e-02,  2.3618e-02,  5.9617e-02,  1.9911e-03,\n",
      "         6.7382e-03,  8.4914e-02,  3.5084e-02, -1.5033e-03, -2.1148e-03,\n",
      "         4.0145e-03, -2.0948e-03, -1.0000e+00,  1.3637e-02,  8.9018e-04,\n",
      "        -3.7227e-03, -3.6715e-03,  5.1046e-02,  2.8566e-02,  6.1222e-02,\n",
      "         2.0776e-02,  8.4619e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0848,  0.0134,  0.0514, -0.0023, -0.0023,  0.0659,  0.0697, -0.0029,\n",
      "         0.0237,  0.0515,  0.0635,  0.0034,  0.0593,  0.0803, -0.0016,  0.0367,\n",
      "         0.0013,  0.0030, -0.0023,  0.0600,  0.0654,  0.0874, -0.0053,  0.0348,\n",
      "         0.1013,  0.0279, -0.0010,  0.0354,  0.0164, -0.0029,  0.0000,  0.0038])\n",
      "Q値の教師データ: tensor([ 8.3954e-02,  1.3232e-02,  5.0920e-02, -2.2839e-03, -2.2971e-03,\n",
      "         6.5248e-02,  6.8985e-02, -2.8267e-03,  2.3454e-02,  5.0954e-02,\n",
      "         6.2892e-02,  3.3793e-03,  5.8684e-02,  7.9499e-02, -1.6298e-03,\n",
      "         3.6312e-02,  1.2447e-03,  2.9222e-03, -2.3207e-03,  5.9383e-02,\n",
      "         6.4738e-02,  8.6480e-02, -5.2800e-03,  3.4475e-02,  1.0029e-01,\n",
      "         2.7630e-02, -9.8269e-04,  3.5031e-02,  1.6279e-02, -2.8786e-03,\n",
      "        -1.0000e+00,  3.7843e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-3.3803e-03,  1.7046e-02,  2.1711e-03, -6.1337e-04,  2.5128e-02,\n",
      "         3.3660e-03,  2.7902e-02,  7.3271e-02,  0.0000e+00,  2.0715e-02,\n",
      "        -2.9762e-03,  3.1002e-02,  6.5087e-02,  6.9199e-02,  1.5733e-02,\n",
      "         3.6295e-02,  2.4365e-03,  6.5128e-02,  8.6168e-02,  8.3927e-02,\n",
      "         6.8273e-02, -2.9523e-03,  1.2544e-02,  9.0224e-02, -2.7677e-03,\n",
      "         1.3165e-02,  8.5648e-02, -4.6995e-03,  5.9458e-05,  5.1429e-02,\n",
      "         5.9816e-02, -2.7459e-03])\n",
      "Q値の教師データ: tensor([-3.3465e-03,  1.6876e-02,  2.1494e-03, -6.0723e-04,  2.4876e-02,\n",
      "         3.3323e-03,  2.7623e-02,  7.2538e-02, -1.0000e+00,  2.0508e-02,\n",
      "        -2.9465e-03,  3.0692e-02,  6.4436e-02,  6.8507e-02,  1.5576e-02,\n",
      "         3.5932e-02,  2.4121e-03,  6.4477e-02,  8.5306e-02,  8.3088e-02,\n",
      "         6.7591e-02, -2.9228e-03,  1.2419e-02,  8.9322e-02, -2.7401e-03,\n",
      "         1.3033e-02,  8.4791e-02, -4.6525e-03,  5.8864e-05,  5.0914e-02,\n",
      "         5.9218e-02, -2.7184e-03])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0033,  0.0000,  0.0375,  0.0140, -0.0036,  0.0348,  0.0288,  0.0678,\n",
      "         0.0514,  0.0206,  0.0354,  0.0826, -0.0051,  0.0786,  0.0024, -0.0049,\n",
      "         0.0151,  0.0588,  0.0830,  0.0722,  0.0633,  0.0014, -0.0023,  0.0000,\n",
      "         0.0000,  0.0850,  0.0788, -0.0059,  0.0000,  0.0310, -0.0036,  0.0251])\n",
      "Q値の教師データ: tensor([-0.0033, -1.0000,  0.0371,  0.0139, -0.0035,  0.0345,  0.0285,  0.0671,\n",
      "         0.0509,  0.0204,  0.0350,  0.0818, -0.0051,  0.0778,  0.0023, -0.0049,\n",
      "         0.0150,  0.0582,  0.0821,  0.0715,  0.0626,  0.0014, -0.0023, -1.0000,\n",
      "        -1.0000,  0.0842,  0.0780, -0.0058, -1.0000,  0.0307, -0.0035,  0.0248])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0036,  0.0000, -0.0050,  0.0279,  0.0673,  0.0645,  0.0781,  0.0512,\n",
      "        -0.0019, -0.0069,  0.0233,  0.0145,  0.0363,  0.0820, -0.0054,  0.0039,\n",
      "        -0.0057,  0.0122,  0.0610,  0.0145, -0.0042, -0.0035,  0.0641,  0.0965,\n",
      "         0.0000,  0.0652, -0.0056,  0.0016,  0.0000,  0.0780,  0.0783,  0.0752])\n",
      "Q値の教師データ: tensor([-0.0036, -1.0000, -0.0049,  0.0276,  0.0666,  0.0638,  0.0773,  0.0507,\n",
      "        -0.0019, -0.0069,  0.0231,  0.0143,  0.0359,  0.0812, -0.0054,  0.0038,\n",
      "        -0.0057,  0.0121,  0.0604,  0.0143, -0.0042, -0.0034,  0.0635,  0.0956,\n",
      "        -1.0000,  0.0646, -0.0055,  0.0016, -1.0000,  0.0772,  0.0776,  0.0745])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0056,  0.0123,  0.0348, -0.0044,  0.0677,  0.0739,  0.0811,  0.0809,\n",
      "         0.0946,  0.0250, -0.0071,  0.0626,  0.0378,  0.0279, -0.0041,  0.0115,\n",
      "         0.0000,  0.0636, -0.0009,  0.0232,  0.0000,  0.0009,  0.0511,  0.0203,\n",
      "         0.0628, -0.0048,  0.0827, -0.0060,  0.0641, -0.0062,  0.0513,  0.0000])\n",
      "Q値の教師データ: tensor([-5.5908e-03,  1.2189e-02,  3.4434e-02, -4.4041e-03,  6.7004e-02,\n",
      "         7.3140e-02,  8.0304e-02,  8.0068e-02,  9.3656e-02,  2.4734e-02,\n",
      "        -7.0339e-03,  6.1998e-02,  3.7419e-02,  2.7577e-02, -4.0425e-03,\n",
      "         1.1395e-02, -1.0000e+00,  6.3002e-02, -9.0041e-04,  2.2942e-02,\n",
      "        -1.0000e+00,  8.5664e-04,  5.0631e-02,  2.0146e-02,  6.2166e-02,\n",
      "        -4.7688e-03,  8.1846e-02, -5.9410e-03,  6.3481e-02, -6.1748e-03,\n",
      "         5.0760e-02, -1.0000e+00])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0509,  0.0762,  0.0361,  0.0001,  0.0866,  0.0813, -0.0032, -0.0050,\n",
      "        -0.0054,  0.0000,  0.0132,  0.0000,  0.0365, -0.0011,  0.0737,  0.0730,\n",
      "        -0.0036,  0.0626,  0.0000,  0.0848, -0.0044,  0.0021,  0.0347,  0.0506,\n",
      "         0.0931,  0.0813,  0.0176, -0.0048,  0.0796, -0.0009,  0.0853,  0.0624])\n",
      "Q値の教師データ: tensor([ 5.0432e-02,  7.5481e-02,  3.5771e-02,  1.1817e-04,  8.5745e-02,\n",
      "         8.0461e-02, -3.2153e-03, -4.9818e-03, -5.3916e-03, -1.0000e+00,\n",
      "         1.3065e-02, -1.0000e+00,  3.6121e-02, -1.0615e-03,  7.2983e-02,\n",
      "         7.2245e-02, -3.5447e-03,  6.1979e-02, -1.0000e+00,  8.3938e-02,\n",
      "        -4.3650e-03,  2.0382e-03,  3.4331e-02,  5.0121e-02,  9.2191e-02,\n",
      "         8.0511e-02,  1.7468e-02, -4.7990e-03,  7.8833e-02, -8.4409e-04,\n",
      "         8.4464e-02,  6.1825e-02])\n",
      "8 Episode: Finished after 9 steps：10試行の平均step数 = 9.4\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0630,  0.0372,  0.0748,  0.0797,  0.0345,  0.0620, -0.0056,  0.0846,\n",
      "         0.0117,  0.0000, -0.0060, -0.0006,  0.0000,  0.0573,  0.0615, -0.0050,\n",
      "        -0.0028, -0.0073,  0.0000,  0.0360,  0.0276,  0.0782,  0.0285,  0.0017,\n",
      "         0.0719,  0.0585,  0.0247,  0.0504,  0.0797,  0.0509, -0.0016,  0.0727])\n",
      "Q値の教師データ: tensor([ 6.2364e-02,  3.6819e-02,  7.4075e-02,  7.8912e-02,  3.4187e-02,\n",
      "         6.1424e-02, -5.5587e-03,  8.3793e-02,  1.1565e-02, -1.0000e+00,\n",
      "        -5.9328e-03, -6.0852e-04, -1.0000e+00,  5.6707e-02,  6.0894e-02,\n",
      "        -4.9569e-03, -2.7418e-03, -7.1891e-03, -1.0000e+00,  3.5625e-02,\n",
      "         2.7337e-02,  7.7425e-02,  2.8216e-02,  1.6809e-03,  7.1213e-02,\n",
      "         5.7950e-02,  2.4451e-02,  4.9864e-02,  7.8902e-02,  5.0379e-02,\n",
      "        -1.5995e-03,  7.1956e-02])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-5.5778e-03,  0.0000e+00, -9.1375e-03, -5.8793e-05,  1.3288e-03,\n",
      "         8.2407e-02,  6.5670e-02,  8.8906e-02, -7.3758e-03,  0.0000e+00,\n",
      "         5.0378e-02,  3.6156e-02,  7.7932e-02,  2.2467e-02,  1.9671e-02,\n",
      "         2.8336e-02,  6.1104e-02,  3.0553e-02,  7.6954e-02, -5.7548e-03,\n",
      "        -7.6941e-03,  6.7963e-02,  8.2591e-02,  7.6748e-02,  3.4902e-02,\n",
      "         8.0838e-02,  6.1558e-02, -6.6886e-03,  7.6657e-02,  1.1350e-02,\n",
      "         3.4363e-02,  2.4518e-02])\n",
      "Q値の教師データ: tensor([-5.5221e-03, -1.0000e+00, -9.0461e-03, -5.8205e-05,  1.3155e-03,\n",
      "         8.1583e-02,  6.5013e-02,  8.8017e-02, -7.3020e-03, -1.0000e+00,\n",
      "         4.9874e-02,  3.5795e-02,  7.7153e-02,  2.2242e-02,  1.9474e-02,\n",
      "         2.8053e-02,  6.0493e-02,  3.0247e-02,  7.6184e-02, -5.6972e-03,\n",
      "        -7.6172e-03,  6.7283e-02,  8.1766e-02,  7.5981e-02,  3.4553e-02,\n",
      "         8.0029e-02,  6.0943e-02, -6.6217e-03,  7.5891e-02,  1.1236e-02,\n",
      "         3.4020e-02,  2.4273e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0646,  0.0647,  0.0576,  0.0872,  0.0222,  0.0342, -0.0095,  0.0721,\n",
      "         0.0674, -0.0070,  0.0869,  0.0000,  0.0724,  0.0763,  0.0010,  0.0650,\n",
      "         0.0704,  0.0611,  0.0789, -0.0023,  0.0243,  0.0766,  0.0357,  0.0000,\n",
      "         0.0755, -0.0073,  0.0498, -0.0067,  0.0372,  0.0602,  0.0170, -0.0080])\n",
      "Q値の教師データ: tensor([ 6.3907e-02,  6.4031e-02,  5.7067e-02,  8.6333e-02,  2.2003e-02,\n",
      "         3.3863e-02, -9.4219e-03,  7.1353e-02,  6.6763e-02, -6.9780e-03,\n",
      "         8.6071e-02, -1.0000e+00,  7.1710e-02,  7.5554e-02,  9.5042e-04,\n",
      "         6.4336e-02,  6.9736e-02,  6.0496e-02,  7.8077e-02, -2.3181e-03,\n",
      "         2.4104e-02,  7.5845e-02,  3.5299e-02, -1.0000e+00,  7.4702e-02,\n",
      "        -7.2126e-03,  4.9287e-02, -6.6811e-03,  3.6840e-02,  5.9600e-02,\n",
      "         1.6785e-02, -7.9140e-03])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0079,  0.0610,  0.0849,  0.0341,  0.0607, -0.0090, -0.0087, -0.0079,\n",
      "         0.0355, -0.0027, -0.0064,  0.0192,  0.0155,  0.0642, -0.0073, -0.0085,\n",
      "         0.0227,  0.0637,  0.0006, -0.0083, -0.0070,  0.0557,  0.0498,  0.0272,\n",
      "         0.0788,  0.0769,  0.0100, -0.0013,  0.0643,  0.0737, -0.0025, -0.0058])\n",
      "Q値の教師データ: tensor([-0.0079,  0.0604,  0.0841,  0.0337,  0.0601, -0.0089, -0.0086, -0.0078,\n",
      "         0.0352, -0.0027, -0.0063,  0.0190,  0.0154,  0.0636, -0.0072, -0.0084,\n",
      "         0.0225,  0.0631,  0.0006, -0.0082, -0.0069,  0.0551,  0.0493,  0.0269,\n",
      "         0.0780,  0.0761,  0.0099, -0.0013,  0.0637,  0.0729, -0.0025, -0.0057])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0271,  0.0096,  0.0696,  0.0699,  0.0279,  0.0000, -0.0080,  0.0345,\n",
      "        -0.0106,  0.0583,  0.0357,  0.0634, -0.0089,  0.0613,  0.0015,  0.0000,\n",
      "         0.0771,  0.0756,  0.0241,  0.0648, -0.0033,  0.0370,  0.0732,  0.0366,\n",
      "         0.0000,  0.0500, -0.0076, -0.0086, -0.0028,  0.0000, -0.0063,  0.0003])\n",
      "Q値の教師データ: tensor([ 2.6783e-02,  9.4803e-03,  6.8921e-02,  6.9198e-02,  2.7650e-02,\n",
      "        -1.0000e+00, -7.9352e-03,  3.4142e-02, -1.0465e-02,  5.7716e-02,\n",
      "         3.5358e-02,  6.2759e-02, -8.8240e-03,  6.0675e-02,  1.5328e-03,\n",
      "        -1.0000e+00,  7.6325e-02,  7.4819e-02,  2.3825e-02,  6.4177e-02,\n",
      "        -3.2378e-03,  3.6586e-02,  7.2439e-02,  3.6212e-02, -1.0000e+00,\n",
      "         4.9490e-02, -7.4831e-03, -8.5476e-03, -2.7988e-03, -1.0000e+00,\n",
      "        -6.2586e-03,  2.7855e-04])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 7.1584e-02,  5.4720e-02,  3.6440e-02, -5.7671e-03,  5.6453e-02,\n",
      "        -8.2186e-03, -8.9859e-03, -1.5072e-03,  5.7605e-02,  5.8404e-02,\n",
      "         2.2316e-02,  6.8314e-02,  7.5085e-02,  6.4612e-02, -1.1125e-02,\n",
      "         7.5247e-02,  1.5125e-02,  6.2224e-02,  4.9030e-02,  6.2508e-02,\n",
      "         3.3821e-02,  0.0000e+00, -7.4960e-03, -9.3648e-03, -8.1211e-03,\n",
      "         0.0000e+00,  6.6814e-02,  8.1160e-02, -5.7433e-05, -4.9291e-03,\n",
      "         6.3441e-02,  7.0841e-02])\n",
      "Q値の教師データ: tensor([ 7.0868e-02,  5.4173e-02,  3.6076e-02, -5.7094e-03,  5.5889e-02,\n",
      "        -8.1365e-03, -8.8960e-03, -1.4921e-03,  5.7029e-02,  5.7820e-02,\n",
      "         2.2092e-02,  6.7631e-02,  7.4334e-02,  6.3966e-02, -1.1014e-02,\n",
      "         7.4495e-02,  1.4974e-02,  6.1602e-02,  4.8539e-02,  6.1882e-02,\n",
      "         3.3483e-02, -1.0000e+00, -7.4210e-03, -9.2712e-03, -8.0399e-03,\n",
      "        -1.0000e+00,  6.6146e-02,  8.0349e-02, -5.6858e-05, -4.8798e-03,\n",
      "         6.2806e-02,  7.0133e-02])\n",
      "reward_batch: tensor([-1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000,  0.0704,  0.0569,  0.0000,  0.0615, -0.0004,  0.0636,  0.0717,\n",
      "        -0.0055,  0.0629, -0.0116, -0.0116,  0.0488, -0.0026,  0.0162, -0.0063,\n",
      "        -0.0036,  0.0601,  0.0673,  0.0683,  0.0631,  0.0100,  0.0299,  0.0000,\n",
      "         0.0075,  0.0561,  0.0000,  0.0592,  0.0695, -0.0075, -0.0081, -0.0089])\n",
      "Q値の教師データ: tensor([-1.0000e+00,  6.9727e-02,  5.6359e-02, -1.0000e+00,  6.0920e-02,\n",
      "        -3.6705e-04,  6.3013e-02,  7.0935e-02, -5.4667e-03,  6.2250e-02,\n",
      "        -1.1463e-02, -1.1460e-02,  4.8337e-02, -2.5318e-03,  1.6021e-02,\n",
      "        -6.2393e-03, -3.5900e-03,  5.9541e-02,  6.6586e-02,  6.7590e-02,\n",
      "         6.2458e-02,  9.8960e-03,  2.9629e-02, -1.0000e+00,  7.4039e-03,\n",
      "         5.5554e-02, -1.0000e+00,  5.8643e-02,  6.8815e-02, -7.4505e-03,\n",
      "        -8.0170e-03, -8.8553e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0663, -0.0007,  0.0486,  0.0620, -0.0091,  0.0000,  0.0601,  0.0186,\n",
      "         0.0690,  0.0000,  0.0489,  0.0660, -0.0092,  0.0000,  0.0353,  0.0591,\n",
      "        -0.0029,  0.0718,  0.0586,  0.0620, -0.0080,  0.0671,  0.0661,  0.0237,\n",
      "         0.0684,  0.0556,  0.0704,  0.0579, -0.0094,  0.0276,  0.0778, -0.0079])\n",
      "Q値の教師データ: tensor([ 6.5613e-02, -6.9788e-04,  4.8115e-02,  6.1396e-02, -9.0443e-03,\n",
      "        -1.0000e+00,  5.9493e-02,  1.8448e-02,  6.8276e-02, -1.0000e+00,\n",
      "         4.8395e-02,  6.5329e-02, -9.1413e-03, -1.0000e+00,  3.4995e-02,\n",
      "         5.8554e-02, -2.8402e-03,  7.1128e-02,  5.8051e-02,  6.1412e-02,\n",
      "        -7.8815e-03,  6.6390e-02,  6.5451e-02,  2.3448e-02,  6.7724e-02,\n",
      "         5.5076e-02,  6.9744e-02,  5.7341e-02, -9.3218e-03,  2.7322e-02,\n",
      "         7.7049e-02, -7.8516e-03])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000,  0.0651, -0.0041,  0.0092,  0.0158,  0.0275,  0.0701,  0.0235,\n",
      "         0.0536, -0.0032,  0.0683, -0.0035, -0.0068,  0.0349, -0.0102,  0.0335,\n",
      "         0.0492, -0.0098, -0.0085,  0.0000,  0.0587,  0.0651,  0.0616,  0.0000,\n",
      "         0.0185,  0.0593,  0.0641,  0.0675,  0.0093, -0.0118,  0.0093,  0.0217])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0645, -0.0041,  0.0091,  0.0156,  0.0272,  0.0694,  0.0233,\n",
      "         0.0530, -0.0032,  0.0676, -0.0035, -0.0067,  0.0346, -0.0101,  0.0332,\n",
      "         0.0487, -0.0097, -0.0084, -1.0000,  0.0581,  0.0645,  0.0610, -1.0000,\n",
      "         0.0183,  0.0587,  0.0635,  0.0668,  0.0092, -0.0117,  0.0092,  0.0215])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 6.1215e-02,  6.9309e-06,  6.3955e-02,  0.0000e+00,  3.3373e-02,\n",
      "        -1.2365e-02,  5.5706e-02, -3.5085e-03,  9.0164e-03,  6.0830e-02,\n",
      "         6.1903e-02,  3.5109e-02,  0.0000e+00,  1.8385e-02,  4.8125e-02,\n",
      "         4.8400e-02,  6.5444e-02,  0.0000e+00,  0.0000e+00,  5.7387e-02,\n",
      "         6.3859e-02, -9.9366e-03,  2.3413e-02,  0.0000e+00,  2.7361e-02,\n",
      "         6.6593e-02,  6.5401e-02, -1.0397e-02,  6.0633e-02, -5.0776e-03,\n",
      "         2.6497e-02,  2.1284e-02])\n",
      "Q値の教師データ: tensor([ 6.0603e-02,  6.8616e-06,  6.3315e-02, -1.0000e+00,  3.3039e-02,\n",
      "        -1.2242e-02,  5.5149e-02, -3.4734e-03,  8.9263e-03,  6.0221e-02,\n",
      "         6.1284e-02,  3.4758e-02, -1.0000e+00,  1.8202e-02,  4.7644e-02,\n",
      "         4.7916e-02,  6.4790e-02, -1.0000e+00, -1.0000e+00,  5.6813e-02,\n",
      "         6.3221e-02, -9.8372e-03,  2.3178e-02, -1.0000e+00,  2.7087e-02,\n",
      "         6.5927e-02,  6.4747e-02, -1.0293e-02,  6.0026e-02, -5.0268e-03,\n",
      "         2.6232e-02,  2.1071e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0350,  0.0069,  0.0593,  0.0646,  0.0644,  0.0584,  0.0272,  0.0650,\n",
      "         0.0627,  0.0624,  0.0631,  0.0603,  0.0264,  0.0642, -0.0050, -0.0048,\n",
      "         0.0721, -0.0111,  0.0646, -0.0122,  0.0550,  0.0479, -0.0130,  0.0570,\n",
      "         0.0535,  0.0212, -0.0070,  0.0000,  0.0054,  0.0562, -0.0097,  0.0347])\n",
      "Q値の教師データ: tensor([ 0.0346,  0.0069,  0.0587,  0.0640,  0.0638,  0.0578,  0.0270,  0.0643,\n",
      "         0.0620,  0.0618,  0.0624,  0.0596,  0.0261,  0.0635, -0.0049, -0.0047,\n",
      "         0.0714, -0.0110,  0.0640, -0.0121,  0.0545,  0.0474, -0.0129,  0.0564,\n",
      "         0.0529,  0.0210, -0.0069, -1.0000,  0.0053,  0.0557, -0.0096,  0.0344])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0616,  0.0565, -0.0115,  0.0084,  0.0211, -0.0057,  0.0000,  0.0650,\n",
      "         0.0569,  0.0635,  0.0480,  0.0639,  0.0182, -0.0076, -0.0053,  0.0572,\n",
      "         0.0000,  0.0550,  0.0556,  0.0337,  0.0152,  0.0596, -0.0088,  0.0064,\n",
      "         0.0349,  0.0614,  0.0041, -0.0127, -0.0103,  0.0708,  0.0477,  0.0332])\n",
      "Q値の教師データ: tensor([ 0.0610,  0.0560, -0.0114,  0.0083,  0.0208, -0.0057, -1.0000,  0.0644,\n",
      "         0.0563,  0.0629,  0.0475,  0.0633,  0.0180, -0.0076, -0.0052,  0.0566,\n",
      "        -1.0000,  0.0545,  0.0550,  0.0333,  0.0151,  0.0590, -0.0087,  0.0064,\n",
      "         0.0345,  0.0608,  0.0041, -0.0126, -0.0102,  0.0701,  0.0472,  0.0328])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0139,  0.0000,  0.0581,  0.0608,  0.0210,  0.0557,  0.0000, -0.0098,\n",
      "         0.0573, -0.0133,  0.0606,  0.0038, -0.0125,  0.0361,  0.0230, -0.0024,\n",
      "         0.0331,  0.0552,  0.0545, -0.0122,  0.0604,  0.0475,  0.0045,  0.0549,\n",
      "         0.0522, -0.0116,  0.0592,  0.0181, -0.0122,  0.0537, -0.0056,  0.0059])\n",
      "Q値の教師データ: tensor([ 0.0137, -1.0000,  0.0576,  0.0602,  0.0208,  0.0551, -1.0000, -0.0097,\n",
      "         0.0567, -0.0132,  0.0600,  0.0038, -0.0124,  0.0357,  0.0228, -0.0024,\n",
      "         0.0327,  0.0546,  0.0540, -0.0121,  0.0597,  0.0470,  0.0044,  0.0544,\n",
      "         0.0516, -0.0115,  0.0586,  0.0179, -0.0121,  0.0532, -0.0055,  0.0059])\n",
      "9 Episode: Finished after 13 steps：10試行の平均step数 = 10.7\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0620,  0.0149,  0.0078,  0.0538,  0.0335,  0.0180,  0.0607,  0.0606,\n",
      "         0.0517,  0.0562,  0.0610,  0.0598,  0.0520,  0.0344, -0.0147,  0.0570,\n",
      "         0.0599,  0.0360,  0.0565,  0.0000,  0.0137,  0.0355,  0.0000,  0.0000,\n",
      "        -0.0071,  0.0621, -0.0089,  0.0483,  0.0604,  0.0330, -0.0121,  0.0607])\n",
      "Q値の教師データ: tensor([ 0.0614,  0.0148,  0.0077,  0.0533,  0.0332,  0.0178,  0.0601,  0.0599,\n",
      "         0.0512,  0.0557,  0.0604,  0.0592,  0.0515,  0.0341, -0.0146,  0.0564,\n",
      "         0.0593,  0.0356,  0.0560, -1.0000,  0.0136,  0.0351, -1.0000, -1.0000,\n",
      "        -0.0070,  0.0615, -0.0088,  0.0479,  0.0598,  0.0326, -0.0119,  0.0601])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0032,  0.0135,  0.0000,  0.0591, -0.0133,  0.0000,  0.0554,  0.0513,\n",
      "         0.0000,  0.0508,  0.0179, -0.0106,  0.0532, -0.0015, -0.0067, -0.0137,\n",
      "        -0.0157,  0.0588,  0.0586,  0.0228,  0.0000, -0.0133,  0.0291,  0.0482,\n",
      "         0.0558, -0.0031,  0.0595, -0.0154,  0.0548, -0.0134,  0.0594,  0.0598])\n",
      "Q値の教師データ: tensor([ 0.0032,  0.0134, -1.0000,  0.0585, -0.0131, -1.0000,  0.0549,  0.0508,\n",
      "        -1.0000,  0.0503,  0.0177, -0.0105,  0.0526, -0.0015, -0.0066, -0.0136,\n",
      "        -0.0156,  0.0582,  0.0580,  0.0226, -1.0000, -0.0132,  0.0288,  0.0477,\n",
      "         0.0552, -0.0030,  0.0589, -0.0153,  0.0542, -0.0132,  0.0588,  0.0592])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0061, -0.0084, -0.0130,  0.0206,  0.0145, -0.0133,  0.0500,  0.0585,\n",
      "         0.0000,  0.0000,  0.0640,  0.0573, -0.0070,  0.0571,  0.0072,  0.0575,\n",
      "        -0.0162,  0.0577,  0.0000,  0.0535,  0.0596, -0.0125,  0.0000,  0.0530,\n",
      "         0.0554,  0.0333,  0.0550, -0.0144,  0.0548,  0.0553,  0.0578,  0.0546])\n",
      "Q値の教師データ: tensor([ 0.0060, -0.0083, -0.0129,  0.0204,  0.0144, -0.0131,  0.0495,  0.0580,\n",
      "        -1.0000, -1.0000,  0.0634,  0.0567, -0.0069,  0.0566,  0.0071,  0.0569,\n",
      "        -0.0160,  0.0571, -1.0000,  0.0529,  0.0590, -0.0124, -1.0000,  0.0525,\n",
      "         0.0549,  0.0329,  0.0544, -0.0143,  0.0542,  0.0547,  0.0572,  0.0541])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000,  0.0176,  0.0069,  0.0026,  0.0143,  0.0560,  0.0000,  0.0591,\n",
      "         0.0563,  0.0033,  0.0555,  0.0514, -0.0067,  0.0205,  0.0568,  0.0343,\n",
      "         0.0563,  0.0566,  0.0531,  0.0536,  0.0541,  0.0491,  0.0567, -0.0073,\n",
      "         0.0559, -0.0107,  0.0568, -0.0143,  0.0566, -0.0145,  0.0000,  0.0621])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0174,  0.0068,  0.0025,  0.0142,  0.0554, -1.0000,  0.0585,\n",
      "         0.0558,  0.0032,  0.0550,  0.0508, -0.0067,  0.0203,  0.0562,  0.0340,\n",
      "         0.0558,  0.0561,  0.0526,  0.0530,  0.0535,  0.0486,  0.0561, -0.0073,\n",
      "         0.0553, -0.0106,  0.0562, -0.0141,  0.0561, -0.0143, -1.0000,  0.0614])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0557,  0.0174, -0.0151,  0.0499, -0.0143,  0.0560, -0.0121,  0.0553,\n",
      "         0.0264, -0.0171,  0.0464,  0.0528, -0.0172,  0.0529,  0.0000,  0.0141,\n",
      "         0.0000,  0.0542,  0.0533, -0.0131,  0.0564, -0.0140, -0.0025,  0.0512,\n",
      "         0.0602,  0.0555,  0.0558,  0.0569,  0.0000, -0.0061,  0.0522,  0.0329])\n",
      "Q値の教師データ: tensor([ 0.0552,  0.0173, -0.0149,  0.0494, -0.0141,  0.0554, -0.0120,  0.0547,\n",
      "         0.0261, -0.0169,  0.0459,  0.0522, -0.0171,  0.0524, -1.0000,  0.0140,\n",
      "        -1.0000,  0.0537,  0.0528, -0.0130,  0.0558, -0.0139, -0.0025,  0.0507,\n",
      "         0.0596,  0.0550,  0.0552,  0.0564, -1.0000, -0.0060,  0.0517,  0.0326])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000,  0.0522, -0.0076,  0.0514,  0.0139, -0.0136,  0.0460,  0.0533,\n",
      "         0.0322,  0.0524, -0.0145,  0.0019,  0.0547, -0.0174,  0.0521,  0.0254,\n",
      "        -0.0064,  0.0000,  0.0026,  0.0513,  0.0554,  0.0534,  0.0221, -0.0177,\n",
      "         0.0527,  0.0558,  0.0480,  0.0545, -0.0161,  0.0000,  0.0527,  0.0522])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0517, -0.0075,  0.0509,  0.0137, -0.0134,  0.0455,  0.0528,\n",
      "         0.0319,  0.0518, -0.0143,  0.0019,  0.0541, -0.0173,  0.0516,  0.0251,\n",
      "        -0.0063, -1.0000,  0.0026,  0.0508,  0.0549,  0.0529,  0.0219, -0.0175,\n",
      "         0.0522,  0.0553,  0.0475,  0.0539, -0.0160, -1.0000,  0.0521,  0.0517])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0528,  0.0171, -0.0032,  0.0338, -0.0179,  0.0508,  0.0521,  0.0552,\n",
      "         0.0508,  0.0515,  0.0522,  0.0000,  0.0531,  0.0480,  0.0513,  0.0517,\n",
      "        -0.0164, -0.0171, -0.0139, -0.0161,  0.0000,  0.0564, -0.0141,  0.0506,\n",
      "         0.0346,  0.0531,  0.0513,  0.0576, -0.0156,  0.0203, -0.0078,  0.0524])\n",
      "Q値の教師データ: tensor([ 0.0522,  0.0169, -0.0032,  0.0335, -0.0178,  0.0503,  0.0516,  0.0546,\n",
      "         0.0503,  0.0510,  0.0517, -1.0000,  0.0525,  0.0475,  0.0507,  0.0511,\n",
      "        -0.0163, -0.0169, -0.0137, -0.0159, -1.0000,  0.0559, -0.0139,  0.0501,\n",
      "         0.0342,  0.0526,  0.0508,  0.0570, -0.0155,  0.0201, -0.0077,  0.0518])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0551,  0.0344,  0.0523,  0.0259,  0.0218,  0.0020, -0.0071,  0.0123,\n",
      "         0.0526, -0.0144,  0.0000, -0.0184,  0.0504,  0.0012,  0.0452,  0.0509,\n",
      "         0.0281, -0.0161,  0.0334,  0.0508, -0.0154,  0.0542,  0.0518,  0.0506,\n",
      "        -0.0170, -0.0145,  0.0511,  0.0505,  0.0135, -0.0155,  0.0527,  0.0319])\n",
      "Q値の教師データ: tensor([ 0.0545,  0.0341,  0.0518,  0.0257,  0.0216,  0.0019, -0.0070,  0.0122,\n",
      "         0.0521, -0.0143, -1.0000, -0.0183,  0.0499,  0.0012,  0.0448,  0.0504,\n",
      "         0.0279, -0.0159,  0.0330,  0.0503, -0.0153,  0.0537,  0.0513,  0.0501,\n",
      "        -0.0168, -0.0144,  0.0506,  0.0500,  0.0133, -0.0154,  0.0522,  0.0316])\n",
      "10 Episode: Finished after 8 steps：10試行の平均step数 = 10.5\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0343,  0.0481,  0.0473,  0.0534,  0.0000,  0.0323,  0.0530,  0.0000,\n",
      "         0.0517,  0.0524,  0.0017, -0.0178, -0.0139, -0.0174, -0.0127,  0.0512,\n",
      "         0.0492,  0.0528,  0.0000,  0.0489,  0.0000, -0.0165,  0.0506,  0.0453,\n",
      "         0.0509,  0.0503,  0.0542,  0.0533, -0.0086,  0.0505,  0.0499,  0.0535])\n",
      "Q値の教師データ: tensor([ 0.0340,  0.0476,  0.0468,  0.0529, -1.0000,  0.0320,  0.0525, -1.0000,\n",
      "         0.0511,  0.0518,  0.0016, -0.0176, -0.0137, -0.0172, -0.0126,  0.0507,\n",
      "         0.0487,  0.0523, -1.0000,  0.0484, -1.0000, -0.0163,  0.0501,  0.0449,\n",
      "         0.0504,  0.0498,  0.0536,  0.0527, -0.0085,  0.0499,  0.0494,  0.0529])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0486,  0.0000,  0.0449,  0.0315, -0.0158,  0.0330,  0.0215,  0.0000,\n",
      "         0.0507,  0.0463, -0.0183, -0.0134,  0.0514,  0.0535, -0.0094, -0.0180,\n",
      "         0.0198,  0.0506,  0.0498, -0.0194, -0.0089,  0.0500,  0.0000,  0.0006,\n",
      "         0.0517,  0.0013,  0.0498,  0.0000, -0.0145,  0.0519,  0.0481, -0.0077])\n",
      "Q値の教師データ: tensor([ 4.8147e-02, -1.0000e+00,  4.4479e-02,  3.1234e-02, -1.5622e-02,\n",
      "         3.2683e-02,  2.1244e-02, -1.0000e+00,  5.0234e-02,  4.5866e-02,\n",
      "        -1.8128e-02, -1.3245e-02,  5.0846e-02,  5.2945e-02, -9.2620e-03,\n",
      "        -1.7823e-02,  1.9630e-02,  5.0084e-02,  4.9269e-02, -1.9181e-02,\n",
      "        -8.8475e-03,  4.9499e-02, -1.0000e+00,  6.1074e-04,  5.1167e-02,\n",
      "         1.3150e-03,  4.9272e-02, -1.0000e+00, -1.4358e-02,  5.1374e-02,\n",
      "         4.7621e-02, -7.6268e-03])\n",
      "reward_batch: tensor([-1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000,  0.0409,  0.0000,  0.0482,  0.0465,  0.0193,  0.0423,  0.0514,\n",
      "         0.0557, -0.0202,  0.0492,  0.0495, -0.0159,  0.0474, -0.0193,  0.0313,\n",
      "         0.0507,  0.0460,  0.0343, -0.0167, -0.0045,  0.0423,  0.0475,  0.0328,\n",
      "         0.0446, -0.0175,  0.0338,  0.0493,  0.0485,  0.0486,  0.0000,  0.0483])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0405, -1.0000,  0.0477,  0.0460,  0.0191,  0.0419,  0.0508,\n",
      "         0.0551, -0.0200,  0.0487,  0.0490, -0.0158,  0.0469, -0.0191,  0.0310,\n",
      "         0.0502,  0.0455,  0.0340, -0.0165, -0.0045,  0.0419,  0.0471,  0.0324,\n",
      "         0.0441, -0.0173,  0.0335,  0.0488,  0.0480,  0.0482, -1.0000,  0.0478])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000,  0.0494,  0.0043,  0.0113, -0.0192,  0.0502,  0.0493,  0.0463,\n",
      "         0.0463,  0.0470,  0.0483, -0.0190,  0.0490, -0.0175,  0.0472,  0.0438,\n",
      "         0.0438,  0.0474, -0.0164,  0.0463,  0.0463,  0.0000,  0.0472,  0.0316,\n",
      "         0.0162,  0.0472,  0.0475,  0.0000, -0.0167, -0.0177,  0.0515, -0.0097])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0489,  0.0043,  0.0111, -0.0191,  0.0497,  0.0488,  0.0458,\n",
      "         0.0459,  0.0465,  0.0478, -0.0188,  0.0485, -0.0173,  0.0467,  0.0433,\n",
      "         0.0434,  0.0470, -0.0162,  0.0458,  0.0459, -1.0000,  0.0467,  0.0312,\n",
      "         0.0160,  0.0467,  0.0470, -1.0000, -0.0165, -0.0175,  0.0510, -0.0096])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0453,  0.0489,  0.0447, -0.0155,  0.0494,  0.0452, -0.0156,  0.0477,\n",
      "         0.0423, -0.0053,  0.0476,  0.0325,  0.0000,  0.0239,  0.0441,  0.0491,\n",
      "         0.0121,  0.0392,  0.0493, -0.0100,  0.0466,  0.0496,  0.0322,  0.0002,\n",
      "         0.0437,  0.0313, -0.0218,  0.0450,  0.0207,  0.0472,  0.0000, -0.0070])\n",
      "Q値の教師データ: tensor([ 4.4891e-02,  4.8439e-02,  4.4235e-02, -1.5377e-02,  4.8922e-02,\n",
      "         4.4737e-02, -1.5456e-02,  4.7193e-02,  4.1885e-02, -5.2386e-03,\n",
      "         4.7090e-02,  3.2155e-02, -1.0000e+00,  2.3701e-02,  4.3699e-02,\n",
      "         4.8597e-02,  1.1988e-02,  3.8837e-02,  4.8822e-02, -9.9446e-03,\n",
      "         4.6109e-02,  4.9091e-02,  3.1843e-02,  2.3914e-04,  4.3260e-02,\n",
      "         3.0941e-02, -2.1592e-02,  4.4579e-02,  2.0465e-02,  4.6777e-02,\n",
      "        -1.0000e+00, -6.9639e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0404,  0.0442,  0.0462, -0.0161, -0.0188,  0.0451,  0.0448,  0.0480,\n",
      "         0.0378, -0.0184,  0.0478,  0.0459, -0.0187,  0.0458,  0.0245,  0.0453,\n",
      "         0.0455,  0.0460,  0.0489, -0.0056,  0.0000,  0.0439,  0.0442,  0.0237,\n",
      "         0.0434,  0.0416,  0.0304,  0.0357,  0.0414, -0.0185,  0.0410, -0.0008])\n",
      "Q値の教師データ: tensor([ 4.0040e-02,  4.3792e-02,  4.5723e-02, -1.5895e-02, -1.8634e-02,\n",
      "         4.4657e-02,  4.4315e-02,  4.7555e-02,  3.7427e-02, -1.8199e-02,\n",
      "         4.7324e-02,  4.5443e-02, -1.8491e-02,  4.5364e-02,  2.4238e-02,\n",
      "         4.4818e-02,  4.5061e-02,  4.5558e-02,  4.8406e-02, -5.5858e-03,\n",
      "        -1.0000e+00,  4.3425e-02,  4.3741e-02,  2.3445e-02,  4.2975e-02,\n",
      "         4.1192e-02,  3.0105e-02,  3.5325e-02,  4.0945e-02, -1.8283e-02,\n",
      "         4.0563e-02, -8.2074e-04])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([ 0.0396,  0.0442, -0.0184, -0.0106,  0.0413,  0.0445,  0.0381,  0.0428,\n",
      "        -0.0229,  0.0435,  0.0013,  0.0388,  0.0243, -0.0169, -0.0193,  0.0440,\n",
      "         0.0235,  0.0186, -0.0211,  0.0404, -0.0178,  0.0447,  0.0332, -0.0004,\n",
      "         0.0448,  0.0475,  0.0481,  0.0457,  0.0033, -0.0214,  0.0183,  0.0369])\n",
      "Q値の教師データ: tensor([ 0.0392,  0.0438, -0.0182, -0.0105,  0.0409,  0.0440,  0.0377,  0.0424,\n",
      "        -0.0227,  0.0431,  0.0013,  0.0384,  0.0240, -0.0167, -0.0192,  0.0435,\n",
      "         0.0232,  0.0184, -0.0208,  0.0400, -0.0176,  0.0443,  0.0329, -0.0004,\n",
      "         0.0444,  0.0470,  0.0476,  0.0452,  0.0033, -0.0212,  0.0181,  0.0365])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0440,  0.0456,  0.0438,  0.0392,  0.0416,  0.0241,  0.0000,  0.0418,\n",
      "         0.0465,  0.0448, -0.0114, -0.0193,  0.0404,  0.0405,  0.0527, -0.0209,\n",
      "         0.0393,  0.0300,  0.0384, -0.0014, -0.0175,  0.0461,  0.0435, -0.0168,\n",
      "         0.0330,  0.0435,  0.0315,  0.0000,  0.0030,  0.0434, -0.0215, -0.0195])\n",
      "Q値の教師データ: tensor([ 0.0436,  0.0452,  0.0434,  0.0388,  0.0412,  0.0238, -1.0000,  0.0414,\n",
      "         0.0460,  0.0443, -0.0113, -0.0191,  0.0400,  0.0401,  0.0521, -0.0207,\n",
      "         0.0389,  0.0297,  0.0381, -0.0014, -0.0173,  0.0456,  0.0431, -0.0166,\n",
      "         0.0327,  0.0431,  0.0312, -1.0000,  0.0030,  0.0429, -0.0213, -0.0193])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0373,  0.0231, -0.0201,  0.0000,  0.0470,  0.0425,  0.0397,  0.0431,\n",
      "        -0.0186,  0.0423,  0.0328,  0.0324,  0.0380,  0.0410,  0.0033,  0.0000,\n",
      "         0.0415,  0.0404,  0.0410, -0.0192,  0.0405,  0.0373,  0.0384,  0.0435,\n",
      "         0.0000,  0.0464, -0.0199, -0.0239,  0.0239,  0.0341,  0.0000,  0.0304])\n",
      "Q値の教師データ: tensor([ 0.0370,  0.0228, -0.0199, -1.0000,  0.0465,  0.0421,  0.0393,  0.0426,\n",
      "        -0.0184,  0.0419,  0.0325,  0.0320,  0.0376,  0.0405,  0.0033, -1.0000,\n",
      "         0.0411,  0.0400,  0.0406, -0.0190,  0.0401,  0.0369,  0.0381,  0.0430,\n",
      "        -1.0000,  0.0460, -0.0197, -0.0236,  0.0236,  0.0338, -1.0000,  0.0301])\n",
      "11 Episode: Finished after 9 steps：10試行の平均step数 = 10.5\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0115,  0.0416,  0.0404,  0.0348,  0.0360,  0.0348,  0.0412,  0.0000,\n",
      "         0.0326, -0.0189, -0.0207, -0.0225,  0.0409, -0.0205,  0.0382,  0.0000,\n",
      "         0.0301,  0.0420, -0.0175,  0.0458,  0.0440, -0.0205,  0.0258,  0.0334,\n",
      "        -0.0186, -0.0234,  0.0392,  0.0500,  0.0429, -0.0227,  0.0398,  0.0481])\n",
      "Q値の教師データ: tensor([-0.0113,  0.0411,  0.0400,  0.0345,  0.0357,  0.0344,  0.0408, -1.0000,\n",
      "         0.0323, -0.0187, -0.0205, -0.0223,  0.0405, -0.0203,  0.0379, -1.0000,\n",
      "         0.0298,  0.0416, -0.0174,  0.0453,  0.0435, -0.0203,  0.0256,  0.0330,\n",
      "        -0.0184, -0.0232,  0.0388,  0.0495,  0.0425, -0.0225,  0.0394,  0.0476])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0., -1.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0338,  0.0362, -0.0207,  0.0000,  0.0406,  0.0000,  0.0312,  0.0458,\n",
      "         0.0367,  0.0256, -0.0196,  0.0195,  0.0147, -0.0221, -0.0232,  0.0000,\n",
      "         0.0469,  0.0390,  0.0000,  0.0448,  0.0000,  0.0308,  0.0380,  0.0000,\n",
      "         0.0382,  0.0380,  0.0022,  0.0000,  0.0355, -0.0212, -0.0023,  0.0107])\n",
      "Q値の教師データ: tensor([ 0.0335,  0.0358, -0.0205, -1.0000,  0.0402, -1.0000,  0.0308,  0.0454,\n",
      "         0.0364,  0.0254, -0.0194,  0.0193,  0.0146, -0.0219, -0.0229, -1.0000,\n",
      "         0.0464,  0.0386, -1.0000,  0.0443, -1.0000,  0.0305,  0.0377, -1.0000,\n",
      "         0.0379,  0.0376,  0.0022, -1.0000,  0.0351, -0.0210, -0.0023,  0.0106])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0233,  0.0380,  0.0377,  0.0320,  0.0394,  0.0376,  0.0000,  0.0454,\n",
      "         0.0302,  0.0346, -0.0208, -0.0213,  0.0000,  0.0333,  0.0253,  0.0385,\n",
      "        -0.0197,  0.0355,  0.0331,  0.0000,  0.0105,  0.0461, -0.0109,  0.0488,\n",
      "         0.0370, -0.0204,  0.0025,  0.0340, -0.0202,  0.0451,  0.0000, -0.0237])\n",
      "Q値の教師データ: tensor([-0.0231,  0.0376,  0.0373,  0.0317,  0.0390,  0.0372, -1.0000,  0.0449,\n",
      "         0.0299,  0.0342, -0.0206, -0.0211, -1.0000,  0.0330,  0.0251,  0.0381,\n",
      "        -0.0195,  0.0351,  0.0327, -1.0000,  0.0104,  0.0457, -0.0108,  0.0483,\n",
      "         0.0366, -0.0202,  0.0024,  0.0336, -0.0200,  0.0446, -1.0000, -0.0234])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0102,  0.0384,  0.0315,  0.0363,  0.0362,  0.0330,  0.0462,  0.0385,\n",
      "         0.0361,  0.0346,  0.0318,  0.0381, -0.0030,  0.0422,  0.0359,  0.0371,\n",
      "         0.0408,  0.0360,  0.0432, -0.0126,  0.0021, -0.0023,  0.0141,  0.0374,\n",
      "         0.0000,  0.0365,  0.0422,  0.0250,  0.0336,  0.0376,  0.0293,  0.0306])\n",
      "Q値の教師データ: tensor([ 0.0101,  0.0380,  0.0312,  0.0360,  0.0358,  0.0327,  0.0457,  0.0381,\n",
      "         0.0357,  0.0342,  0.0315,  0.0378, -0.0029,  0.0418,  0.0355,  0.0367,\n",
      "         0.0403,  0.0356,  0.0427, -0.0125,  0.0021, -0.0023,  0.0140,  0.0371,\n",
      "        -1.0000,  0.0361,  0.0417,  0.0247,  0.0332,  0.0372,  0.0290,  0.0303])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0248, -0.0252,  0.0290,  0.0379, -0.0244, -0.0228,  0.0000,  0.0292,\n",
      "         0.0346,  0.0089,  0.0392,  0.0309, -0.0034, -0.0026, -0.0219,  0.0436,\n",
      "         0.0000,  0.0348,  0.0000,  0.0386,  0.0367,  0.0383,  0.0000,  0.0427,\n",
      "        -0.0222,  0.0365, -0.0218,  0.0000,  0.0373,  0.0490, -0.0244,  0.0350])\n",
      "Q値の教師データ: tensor([ 0.0246, -0.0250,  0.0287,  0.0375, -0.0241, -0.0225, -1.0000,  0.0289,\n",
      "         0.0343,  0.0088,  0.0388,  0.0306, -0.0033, -0.0026, -0.0217,  0.0431,\n",
      "        -1.0000,  0.0345, -1.0000,  0.0383,  0.0364,  0.0379, -1.0000,  0.0423,\n",
      "        -0.0220,  0.0362, -0.0216, -1.0000,  0.0370,  0.0485, -0.0241,  0.0346])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0224,  0.0406, -0.0241, -0.0227,  0.0428,  0.0425,  0.0436,  0.0294,\n",
      "         0.0313,  0.0000, -0.0132,  0.0165, -0.0037,  0.0398,  0.0256,  0.0425,\n",
      "         0.0222,  0.0325,  0.0354,  0.0291, -0.0227,  0.0299,  0.0341,  0.0244,\n",
      "        -0.0133,  0.0136,  0.0286,  0.0334,  0.0406,  0.0320,  0.0284,  0.0000])\n",
      "Q値の教師データ: tensor([-0.0222,  0.0402, -0.0239, -0.0225,  0.0424,  0.0421,  0.0432,  0.0291,\n",
      "         0.0309, -1.0000, -0.0131,  0.0163, -0.0037,  0.0394,  0.0254,  0.0421,\n",
      "         0.0220,  0.0322,  0.0350,  0.0288, -0.0224,  0.0296,  0.0337,  0.0241,\n",
      "        -0.0132,  0.0134,  0.0283,  0.0330,  0.0402,  0.0317,  0.0281, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0034,  0.0441,  0.0310,  0.0219,  0.0358, -0.0123, -0.0257,  0.0330,\n",
      "        -0.0140,  0.0369,  0.0315,  0.0343,  0.0348, -0.0201,  0.0292,  0.0084,\n",
      "        -0.0216,  0.0343, -0.0229,  0.0000,  0.0000,  0.0000,  0.0392,  0.0317,\n",
      "         0.0419,  0.0421,  0.0310,  0.0293,  0.0273,  0.0377,  0.0309,  0.0000])\n",
      "Q値の教師データ: tensor([-0.0033,  0.0437,  0.0307,  0.0217,  0.0354, -0.0122, -0.0255,  0.0326,\n",
      "        -0.0139,  0.0366,  0.0312,  0.0340,  0.0345, -0.0199,  0.0289,  0.0083,\n",
      "        -0.0214,  0.0340, -0.0227, -1.0000, -1.0000, -1.0000,  0.0388,  0.0314,\n",
      "         0.0414,  0.0417,  0.0307,  0.0290,  0.0271,  0.0374,  0.0305, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 3.9074e-02, -2.3589e-02,  1.7766e-02, -2.3513e-02,  3.7557e-02,\n",
      "         3.0837e-02,  2.7805e-02,  9.0422e-03,  3.3080e-02, -2.8112e-02,\n",
      "         4.1957e-02, -2.6204e-02,  2.3298e-02, -2.5336e-02,  3.3389e-02,\n",
      "         4.1337e-02,  3.1972e-02,  4.5382e-02,  4.6909e-02, -2.3678e-02,\n",
      "        -2.3043e-02,  2.8895e-02,  3.0092e-02, -1.6487e-03,  2.5895e-02,\n",
      "         8.3422e-05,  0.0000e+00,  3.0402e-02, -2.6249e-02,  3.1867e-02,\n",
      "         3.6735e-02,  2.9285e-02])\n",
      "Q値の教師データ: tensor([ 3.8683e-02, -2.3354e-02,  1.7589e-02, -2.3278e-02,  3.7181e-02,\n",
      "         3.0529e-02,  2.7527e-02,  8.9518e-03,  3.2749e-02, -2.7831e-02,\n",
      "         4.1537e-02, -2.5942e-02,  2.3065e-02, -2.5083e-02,  3.3055e-02,\n",
      "         4.0924e-02,  3.1652e-02,  4.4928e-02,  4.6440e-02, -2.3441e-02,\n",
      "        -2.2813e-02,  2.8606e-02,  2.9791e-02, -1.6322e-03,  2.5636e-02,\n",
      "         8.2588e-05, -1.0000e+00,  3.0098e-02, -2.5987e-02,  3.1548e-02,\n",
      "         3.6368e-02,  2.8993e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0280, -0.0230,  0.0285,  0.0290,  0.0333, -0.0267,  0.0078,  0.0308,\n",
      "         0.0213,  0.0318,  0.0320,  0.0000, -0.0265,  0.0249,  0.0000,  0.0218,\n",
      "         0.0295,  0.0400, -0.0237,  0.0264,  0.0206,  0.0397,  0.0447,  0.0277,\n",
      "         0.0157,  0.0405,  0.0319,  0.0198, -0.0249,  0.0340,  0.0377,  0.0406])\n",
      "Q値の教師データ: tensor([ 0.0278, -0.0228,  0.0283,  0.0288,  0.0330, -0.0264,  0.0077,  0.0305,\n",
      "         0.0211,  0.0315,  0.0317, -1.0000, -0.0262,  0.0247, -1.0000,  0.0216,\n",
      "         0.0292,  0.0396, -0.0235,  0.0261,  0.0203,  0.0393,  0.0443,  0.0275,\n",
      "         0.0155,  0.0401,  0.0316,  0.0196, -0.0247,  0.0336,  0.0373,  0.0402])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0295,  0.0204, -0.0023, -0.0051,  0.0370,  0.0246, -0.0243,  0.0277,\n",
      "        -0.0291, -0.0235,  0.0236,  0.0353,  0.0250,  0.0000,  0.0441,  0.0275,\n",
      "         0.0299, -0.0212,  0.0268,  0.0000,  0.0201,  0.0291,  0.0421, -0.0269,\n",
      "         0.0324, -0.0151,  0.0347,  0.0000,  0.0002, -0.0100, -0.0229,  0.0172])\n",
      "Q値の教師データ: tensor([ 2.9253e-02,  2.0147e-02, -2.2539e-03, -5.0687e-03,  3.6601e-02,\n",
      "         2.4393e-02, -2.4023e-02,  2.7465e-02, -2.8857e-02, -2.3228e-02,\n",
      "         2.3326e-02,  3.4982e-02,  2.4781e-02, -1.0000e+00,  4.3632e-02,\n",
      "         2.7194e-02,  2.9642e-02, -2.0994e-02,  2.6492e-02, -1.0000e+00,\n",
      "         1.9904e-02,  2.8821e-02,  4.1663e-02, -2.6673e-02,  3.2095e-02,\n",
      "        -1.4943e-02,  3.4337e-02, -1.0000e+00,  1.6809e-04, -9.8714e-03,\n",
      "        -2.2680e-02,  1.7053e-02])\n",
      "12 Episode: Finished after 10 steps：10試行の平均step数 = 10.3\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([ 0.0207,  0.0302,  0.0276, -0.0274,  0.0170,  0.0401,  0.0000,  0.0388,\n",
      "         0.0237,  0.0252,  0.0326,  0.0265,  0.0272,  0.0213, -0.0009,  0.0287,\n",
      "         0.0434,  0.0304,  0.0189,  0.0298,  0.0221, -0.0276,  0.0296,  0.0258,\n",
      "         0.0339,  0.0272, -0.0247,  0.0000,  0.0388,  0.0254, -0.0151,  0.0000])\n",
      "Q値の教師データ: tensor([ 2.0500e-02,  2.9859e-02,  2.7323e-02, -2.7153e-02,  1.6785e-02,\n",
      "         3.9728e-02, -1.0000e+00,  3.8418e-02,  2.3500e-02,  2.4929e-02,\n",
      "         3.2286e-02,  2.6250e-02,  2.6902e-02,  2.1126e-02, -9.1577e-04,\n",
      "         2.8454e-02,  4.2988e-02,  3.0102e-02,  1.8664e-02,  2.9520e-02,\n",
      "         2.1858e-02, -2.7309e-02,  2.9257e-02,  2.5571e-02,  3.3516e-02,\n",
      "         2.6909e-02, -2.4463e-02, -1.0000e+00,  3.8413e-02,  2.5103e-02,\n",
      "        -1.4936e-02, -1.0000e+00])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0295,  0.0385,  0.0262,  0.0208,  0.0321,  0.0000,  0.0314, -0.0264,\n",
      "         0.0208, -0.0058,  0.0428,  0.0269,  0.0395,  0.0000,  0.0000,  0.0245,\n",
      "        -0.0253,  0.0000,  0.0149,  0.0000, -0.0285,  0.0205, -0.0302,  0.0080,\n",
      "        -0.0254,  0.0355,  0.0355,  0.0442,  0.0383,  0.0309,  0.0201,  0.0262])\n",
      "Q値の教師データ: tensor([ 0.0292,  0.0381,  0.0260,  0.0206,  0.0318, -1.0000,  0.0311, -0.0261,\n",
      "         0.0206, -0.0057,  0.0424,  0.0266,  0.0391, -1.0000, -1.0000,  0.0243,\n",
      "        -0.0251, -1.0000,  0.0147, -1.0000, -0.0282,  0.0203, -0.0299,  0.0079,\n",
      "        -0.0251,  0.0352,  0.0352,  0.0438,  0.0379,  0.0306,  0.0199,  0.0260])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0143,  0.0250,  0.0354,  0.0000,  0.0372, -0.0285,  0.0421,  0.0210,\n",
      "         0.0291, -0.0287,  0.0282, -0.0032,  0.0266,  0.0275,  0.0248,  0.0206,\n",
      "         0.0199,  0.0308, -0.0062,  0.0271,  0.0000,  0.0229,  0.0159,  0.0394,\n",
      "         0.0257,  0.0000, -0.0008,  0.0223,  0.0257,  0.0295,  0.0349, -0.0258])\n",
      "Q値の教師データ: tensor([-1.4176e-02,  2.4714e-02,  3.5014e-02, -1.0000e+00,  3.6875e-02,\n",
      "        -2.8188e-02,  4.1702e-02,  2.0754e-02,  2.8775e-02, -2.8423e-02,\n",
      "         2.7931e-02, -3.1924e-03,  2.6315e-02,  2.7230e-02,  2.4509e-02,\n",
      "         2.0371e-02,  1.9657e-02,  3.0445e-02, -6.0928e-03,  2.6854e-02,\n",
      "        -1.0000e+00,  2.2707e-02,  1.5786e-02,  3.9015e-02,  2.5433e-02,\n",
      "        -1.0000e+00, -7.8256e-04,  2.2069e-02,  2.5468e-02,  2.9204e-02,\n",
      "         3.4508e-02, -2.5529e-02])\n",
      "reward_batch: tensor([-1., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000,  0.0000, -0.0162,  0.0246,  0.0264, -0.0020,  0.0429,  0.0000,\n",
      "         0.0198, -0.0262, -0.0296,  0.0393,  0.0178,  0.0242, -0.0289,  0.0143,\n",
      "        -0.0247,  0.0185,  0.0196, -0.0262,  0.0000,  0.0272,  0.0275,  0.0235,\n",
      "         0.0192,  0.0298,  0.0340,  0.0213,  0.0000,  0.0176,  0.0246, -0.0268])\n",
      "Q値の教師データ: tensor([-1.0000, -1.0000, -0.0160,  0.0244,  0.0262, -0.0019,  0.0425, -1.0000,\n",
      "         0.0196, -0.0260, -0.0293,  0.0389,  0.0177,  0.0240, -0.0287,  0.0141,\n",
      "        -0.0244,  0.0184,  0.0194, -0.0260, -1.0000,  0.0269,  0.0272,  0.0232,\n",
      "         0.0190,  0.0295,  0.0337,  0.0211, -1.0000,  0.0175,  0.0244, -0.0265])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0118,  0.0188,  0.0369,  0.0363,  0.0337,  0.0000,  0.0163,  0.0288,\n",
      "         0.0220, -0.0164,  0.0378, -0.0295,  0.0120,  0.0000,  0.0165,  0.0110,\n",
      "         0.0187, -0.0138,  0.0171,  0.0071,  0.0189, -0.0015, -0.0289,  0.0359,\n",
      "        -0.0252,  0.0352,  0.0251,  0.0158,  0.0000, -0.0298,  0.0222,  0.0000])\n",
      "Q値の教師データ: tensor([-0.0117,  0.0186,  0.0365,  0.0359,  0.0333, -1.0000,  0.0161,  0.0285,\n",
      "         0.0217, -0.0163,  0.0374, -0.0292,  0.0119, -1.0000,  0.0163,  0.0109,\n",
      "         0.0186, -0.0136,  0.0169,  0.0070,  0.0187, -0.0015, -0.0286,  0.0356,\n",
      "        -0.0249,  0.0349,  0.0249,  0.0156, -1.0000, -0.0295,  0.0219, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0148, -0.0122,  0.0113,  0.0222,  0.0261,  0.0351,  0.0169,  0.0235,\n",
      "         0.0374,  0.0000,  0.0239, -0.0257, -0.0290,  0.0244,  0.0287, -0.0272,\n",
      "        -0.0271, -0.0272,  0.0242,  0.0350,  0.0273,  0.0354,  0.0373,  0.0261,\n",
      "         0.0157, -0.0019,  0.0106,  0.0283,  0.0251,  0.0166, -0.0168,  0.0206])\n",
      "Q値の教師データ: tensor([ 0.0147, -0.0121,  0.0112,  0.0220,  0.0258,  0.0347,  0.0167,  0.0232,\n",
      "         0.0370, -1.0000,  0.0236, -0.0254, -0.0287,  0.0241,  0.0284, -0.0270,\n",
      "        -0.0268, -0.0269,  0.0240,  0.0347,  0.0270,  0.0351,  0.0369,  0.0259,\n",
      "         0.0155, -0.0018,  0.0104,  0.0280,  0.0248,  0.0164, -0.0167,  0.0204])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1., -1.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0287,  0.0166,  0.0178,  0.0381,  0.0288, -0.0304,  0.0219,  0.0135,\n",
      "         0.0191,  0.0135,  0.0160,  0.0314,  0.0352,  0.0000,  0.0318, -0.0239,\n",
      "         0.0000, -0.0291,  0.0229, -0.0022, -0.0309,  0.0228,  0.0252,  0.0209,\n",
      "         0.0253,  0.0219,  0.0264, -0.0031,  0.0092,  0.0000,  0.0000,  0.0226])\n",
      "Q値の教師データ: tensor([-0.0284,  0.0164,  0.0177,  0.0377,  0.0285, -0.0301,  0.0217,  0.0134,\n",
      "         0.0189,  0.0134,  0.0158,  0.0311,  0.0348, -1.0000,  0.0315, -0.0237,\n",
      "        -1.0000, -0.0288,  0.0227, -0.0022, -0.0306,  0.0225,  0.0249,  0.0207,\n",
      "         0.0250,  0.0217,  0.0262, -0.0031,  0.0091, -1.0000, -1.0000,  0.0224])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0., -1.])\n",
      "次状態の最大Q値: tensor([ 0.0134,  0.0333,  0.0085,  0.0000, -0.0026,  0.0052,  0.0000, -0.0176,\n",
      "         0.0306,  0.0121,  0.0191,  0.0078,  0.0121,  0.0248, -0.0266,  0.0279,\n",
      "         0.0273,  0.0342,  0.0311,  0.0310,  0.0085,  0.0000, -0.0309,  0.0146,\n",
      "         0.0198, -0.0297, -0.0081,  0.0196,  0.0269,  0.0000, -0.0314,  0.0000])\n",
      "Q値の教師データ: tensor([ 0.0133,  0.0330,  0.0084, -1.0000, -0.0025,  0.0051, -1.0000, -0.0174,\n",
      "         0.0302,  0.0120,  0.0189,  0.0078,  0.0119,  0.0246, -0.0263,  0.0277,\n",
      "         0.0270,  0.0338,  0.0308,  0.0307,  0.0084, -1.0000, -0.0306,  0.0145,\n",
      "         0.0196, -0.0294, -0.0080,  0.0194,  0.0266, -1.0000, -0.0311, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0110,  0.0238,  0.0244, -0.0341, -0.0248,  0.0188, -0.0287,  0.0328,\n",
      "         0.0191,  0.0361, -0.0301,  0.0322,  0.0356,  0.0181,  0.0183,  0.0145,\n",
      "        -0.0310,  0.0335,  0.0376,  0.0237, -0.0029,  0.0189,  0.0108, -0.0320,\n",
      "         0.0097,  0.0133, -0.0271,  0.0000,  0.0132,  0.0269,  0.0297, -0.0180])\n",
      "Q値の教師データ: tensor([ 0.0109,  0.0235,  0.0241, -0.0338, -0.0245,  0.0186, -0.0284,  0.0325,\n",
      "         0.0189,  0.0358, -0.0298,  0.0319,  0.0353,  0.0179,  0.0181,  0.0144,\n",
      "        -0.0307,  0.0331,  0.0373,  0.0234, -0.0029,  0.0187,  0.0107, -0.0317,\n",
      "         0.0096,  0.0131, -0.0268, -1.0000,  0.0131,  0.0267,  0.0294, -0.0178])\n",
      "13 Episode: Finished after 9 steps：10試行の平均step数 = 10.2\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0250, -0.0157,  0.0000,  0.0094, -0.0276, -0.0184,  0.0321,  0.0219,\n",
      "         0.0234,  0.0316, -0.0308,  0.0198, -0.0291,  0.0054,  0.0098,  0.0059,\n",
      "         0.0000,  0.0096,  0.0000,  0.0205, -0.0188,  0.0095,  0.0154,  0.0327,\n",
      "         0.0182,  0.0124,  0.0094, -0.0088,  0.0218,  0.0123,  0.0236, -0.0300])\n",
      "Q値の教師データ: tensor([ 0.0248, -0.0156, -1.0000,  0.0093, -0.0273, -0.0182,  0.0318,  0.0217,\n",
      "         0.0232,  0.0313, -0.0305,  0.0196, -0.0288,  0.0054,  0.0097,  0.0058,\n",
      "        -1.0000,  0.0095, -1.0000,  0.0203, -0.0186,  0.0094,  0.0153,  0.0323,\n",
      "         0.0180,  0.0123,  0.0093, -0.0087,  0.0216,  0.0122,  0.0233, -0.0297])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0138,  0.0224,  0.0044,  0.0187,  0.0263,  0.0319,  0.0165,  0.0382,\n",
      "        -0.0085,  0.0209,  0.0086,  0.0106, -0.0296, -0.0255,  0.0286,  0.0334,\n",
      "         0.0354,  0.0169,  0.0000,  0.0181, -0.0173, -0.0352, -0.0296, -0.0141,\n",
      "         0.0119,  0.0140, -0.0328,  0.0224,  0.0083,  0.0000,  0.0215, -0.0191])\n",
      "Q値の教師データ: tensor([ 0.0137,  0.0222,  0.0044,  0.0186,  0.0260,  0.0315,  0.0164,  0.0378,\n",
      "        -0.0084,  0.0207,  0.0085,  0.0105, -0.0294, -0.0253,  0.0284,  0.0331,\n",
      "         0.0350,  0.0168, -1.0000,  0.0179, -0.0171, -0.0349, -0.0293, -0.0139,\n",
      "         0.0118,  0.0139, -0.0324,  0.0222,  0.0082, -1.0000,  0.0213, -0.0189])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0048, -0.0329, -0.0144,  0.0000,  0.0000,  0.0181,  0.0033, -0.0336,\n",
      "         0.0132, -0.0310,  0.0175,  0.0169,  0.0097,  0.0000, -0.0319,  0.0095,\n",
      "        -0.0049, -0.0191, -0.0165,  0.0088, -0.0328,  0.0068, -0.0259,  0.0281,\n",
      "         0.0071,  0.0000,  0.0088,  0.0196,  0.0334,  0.0278,  0.0080,  0.0310])\n",
      "Q値の教師データ: tensor([ 0.0048, -0.0326, -0.0143, -1.0000, -1.0000,  0.0179,  0.0033, -0.0333,\n",
      "         0.0131, -0.0307,  0.0173,  0.0167,  0.0096, -1.0000, -0.0315,  0.0094,\n",
      "        -0.0049, -0.0189, -0.0163,  0.0087, -0.0325,  0.0068, -0.0256,  0.0278,\n",
      "         0.0070, -1.0000,  0.0087,  0.0194,  0.0331,  0.0275,  0.0079,  0.0307])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0240,  0.0067,  0.0062,  0.0320,  0.0287,  0.0168, -0.0092, -0.0348,\n",
      "         0.0188,  0.0081,  0.0203,  0.0321, -0.0180,  0.0000, -0.0339, -0.0339,\n",
      "         0.0061,  0.0081, -0.0333,  0.0155,  0.0000, -0.0305,  0.0000,  0.0287,\n",
      "        -0.0098,  0.0149, -0.0148,  0.0113,  0.0137, -0.0066, -0.0331,  0.0114])\n",
      "Q値の教師データ: tensor([ 0.0238,  0.0067,  0.0061,  0.0317,  0.0284,  0.0166, -0.0091, -0.0345,\n",
      "         0.0187,  0.0080,  0.0201,  0.0317, -0.0178, -1.0000, -0.0336, -0.0335,\n",
      "         0.0060,  0.0080, -0.0330,  0.0153, -1.0000, -0.0302, -1.0000,  0.0284,\n",
      "        -0.0097,  0.0147, -0.0146,  0.0112,  0.0136, -0.0066, -0.0328,  0.0113])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0329,  0.0227, -0.0368,  0.0252,  0.0061,  0.0155,  0.0040,  0.0000,\n",
      "         0.0000,  0.0062,  0.0000,  0.0054,  0.0178,  0.0067,  0.0177,  0.0069,\n",
      "         0.0050,  0.0142,  0.0127,  0.0048,  0.0220,  0.0177,  0.0049, -0.0102,\n",
      "        -0.0311,  0.0260, -0.0046,  0.0240, -0.0310,  0.0228, -0.0172,  0.0178])\n",
      "Q値の教師データ: tensor([-0.0326,  0.0225, -0.0365,  0.0249,  0.0060,  0.0153,  0.0040, -1.0000,\n",
      "        -1.0000,  0.0062, -1.0000,  0.0053,  0.0176,  0.0067,  0.0175,  0.0068,\n",
      "         0.0049,  0.0140,  0.0126,  0.0048,  0.0218,  0.0176,  0.0048, -0.0101,\n",
      "        -0.0308,  0.0257, -0.0045,  0.0237, -0.0307,  0.0226, -0.0171,  0.0176])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0233,  0.0245,  0.0284, -0.0315,  0.0000, -0.0340,  0.0034,  0.0000,\n",
      "         0.0106,  0.0026, -0.0202,  0.0000,  0.0153, -0.0353,  0.0038,  0.0332,\n",
      "         0.0063,  0.0347,  0.0190, -0.0060,  0.0046,  0.0041, -0.0206,  0.0000,\n",
      "         0.0000,  0.0000, -0.0341,  0.0186,  0.0248, -0.0298,  0.0284,  0.0257])\n",
      "Q値の教師データ: tensor([ 0.0230,  0.0242,  0.0281, -0.0311, -1.0000, -0.0336,  0.0034, -1.0000,\n",
      "         0.0105,  0.0026, -0.0200, -1.0000,  0.0151, -0.0349,  0.0038,  0.0329,\n",
      "         0.0062,  0.0344,  0.0188, -0.0059,  0.0046,  0.0040, -0.0204, -1.0000,\n",
      "        -1.0000, -1.0000, -0.0338,  0.0185,  0.0246, -0.0295,  0.0281,  0.0255])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([ 0.0019,  0.0155,  0.0267,  0.0052,  0.0299,  0.0000,  0.0324, -0.0110,\n",
      "        -0.0026, -0.0181,  0.0264,  0.0114,  0.0173,  0.0177,  0.0000,  0.0287,\n",
      "        -0.0207,  0.0042,  0.0141,  0.0212,  0.0000,  0.0039,  0.0022,  0.0094,\n",
      "         0.0014,  0.0261, -0.0275, -0.0355,  0.0210,  0.0026,  0.0011,  0.0000])\n",
      "Q値の教師データ: tensor([ 0.0018,  0.0154,  0.0264,  0.0052,  0.0296, -1.0000,  0.0321, -0.0109,\n",
      "        -0.0025, -0.0179,  0.0261,  0.0113,  0.0171,  0.0175, -1.0000,  0.0284,\n",
      "        -0.0205,  0.0042,  0.0139,  0.0210, -1.0000,  0.0039,  0.0021,  0.0093,\n",
      "         0.0014,  0.0259, -0.0272, -0.0351,  0.0208,  0.0025,  0.0011, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0338,  0.0184,  0.0060, -0.0360,  0.0100,  0.0151,  0.0183, -0.0359,\n",
      "         0.0240,  0.0059, -0.0305,  0.0200,  0.0257,  0.0016, -0.0386,  0.0007,\n",
      "         0.0000, -0.0308,  0.0000,  0.0000,  0.0090,  0.0208,  0.0192,  0.0101,\n",
      "         0.0339,  0.0178,  0.0151, -0.0068,  0.0274, -0.0214,  0.0097,  0.0114])\n",
      "Q値の教師データ: tensor([-3.3480e-02,  1.8179e-02,  5.9311e-03, -3.5602e-02,  9.9216e-03,\n",
      "         1.4953e-02,  1.8162e-02, -3.5510e-02,  2.3756e-02,  5.8047e-03,\n",
      "        -3.0177e-02,  1.9806e-02,  2.5441e-02,  1.6321e-03, -3.8182e-02,\n",
      "         7.1952e-04, -1.0000e+00, -3.0538e-02, -1.0000e+00, -1.0000e+00,\n",
      "         8.8819e-03,  2.0556e-02,  1.8991e-02,  1.0015e-02,  3.3586e-02,\n",
      "         1.7655e-02,  1.4938e-02, -6.7358e-03,  2.7079e-02, -2.1189e-02,\n",
      "         9.6288e-03,  1.1237e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0344, -0.0314, -0.0309,  0.0143,  0.0072,  0.0009, -0.0350,  0.0159,\n",
      "         0.0308, -0.0017,  0.0000,  0.0283, -0.0007,  0.0179,  0.0097,  0.0083,\n",
      "         0.0233,  0.0251, -0.0358, -0.0366, -0.0365,  0.0182, -0.0012,  0.0169,\n",
      "        -0.0012,  0.0245,  0.0000,  0.0023,  0.0184,  0.0141,  0.0025, -0.0112])\n",
      "Q値の教師データ: tensor([-3.4067e-02, -3.1042e-02, -3.0611e-02,  1.4128e-02,  7.1039e-03,\n",
      "         8.7969e-04, -3.4624e-02,  1.5713e-02,  3.0515e-02, -1.7196e-03,\n",
      "        -1.0000e+00,  2.8046e-02, -6.9006e-04,  1.7685e-02,  9.6409e-03,\n",
      "         8.2453e-03,  2.3024e-02,  2.4808e-02, -3.5441e-02, -3.6236e-02,\n",
      "        -3.6139e-02,  1.7975e-02, -1.2265e-03,  1.6685e-02, -1.2198e-03,\n",
      "         2.4229e-02, -1.0000e+00,  2.2472e-03,  1.8227e-02,  1.3957e-02,\n",
      "         2.5191e-03, -1.1094e-02])\n",
      "14 Episode: Finished after 9 steps：10試行の平均step数 = 10.0\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0122,  0.0012,  0.0236, -0.0288, -0.0006,  0.0178,  0.0077, -0.0335,\n",
      "         0.0200,  0.0000,  0.0313, -0.0384,  0.0180,  0.0319,  0.0185, -0.0028,\n",
      "         0.0210,  0.0240, -0.0020, -0.0335,  0.0232,  0.0008,  0.0217, -0.0063,\n",
      "        -0.0006,  0.0169,  0.0000,  0.0026, -0.0363,  0.0094,  0.0277,  0.0301])\n",
      "Q値の教師データ: tensor([-1.2116e-02,  1.2058e-03,  2.3332e-02, -2.8512e-02, -6.3628e-04,\n",
      "         1.7653e-02,  7.6704e-03, -3.3149e-02,  1.9756e-02, -1.0000e+00,\n",
      "         3.0979e-02, -3.7992e-02,  1.7799e-02,  3.1541e-02,  1.8289e-02,\n",
      "        -2.7959e-03,  2.0812e-02,  2.3755e-02, -1.9405e-03, -3.3202e-02,\n",
      "         2.3008e-02,  7.9647e-04,  2.1437e-02, -6.2139e-03, -5.5805e-04,\n",
      "         1.6752e-02, -1.0000e+00,  2.5682e-03, -3.5957e-02,  9.2775e-03,\n",
      "         2.7465e-02,  2.9751e-02])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000e+00,  1.4525e-02,  2.1625e-02,  2.2693e-02, -1.8015e-03,\n",
      "        -5.9381e-04, -8.7786e-04,  1.6092e-02,  1.7760e-02, -3.8146e-02,\n",
      "        -2.2301e-02,  2.0389e-02, -9.8376e-05,  0.0000e+00,  1.7977e-02,\n",
      "        -2.3643e-03, -3.6799e-02,  2.2444e-02,  1.7873e-02,  0.0000e+00,\n",
      "        -3.3979e-02,  1.5946e-02,  1.5342e-02,  6.3669e-03,  2.5038e-02,\n",
      "         1.8117e-02,  0.0000e+00,  1.7009e-02, -8.1254e-03,  1.0607e-02,\n",
      "        -4.2687e-03, -2.9203e-02])\n",
      "Q値の教師データ: tensor([-1.0000e+00,  1.4380e-02,  2.1409e-02,  2.2466e-02, -1.7835e-03,\n",
      "        -5.8787e-04, -8.6908e-04,  1.5931e-02,  1.7582e-02, -3.7765e-02,\n",
      "        -2.2078e-02,  2.0186e-02, -9.7392e-05, -1.0000e+00,  1.7797e-02,\n",
      "        -2.3407e-03, -3.6431e-02,  2.2220e-02,  1.7694e-02, -1.0000e+00,\n",
      "        -3.3639e-02,  1.5787e-02,  1.5188e-02,  6.3032e-03,  2.4787e-02,\n",
      "         1.7936e-02, -1.0000e+00,  1.6839e-02, -8.0441e-03,  1.0501e-02,\n",
      "        -4.2260e-03, -2.8911e-02])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0124,  0.0000, -0.0052, -0.0073,  0.0000,  0.0000,  0.0286, -0.0387,\n",
      "         0.0258, -0.0097, -0.0361, -0.0048, -0.0023, -0.0057,  0.0073,  0.0000,\n",
      "        -0.0379, -0.0037,  0.0264, -0.0095,  0.0179,  0.0209, -0.0179,  0.0027,\n",
      "         0.0064,  0.0184,  0.0085,  0.0087,  0.0225, -0.0322, -0.0382,  0.0320])\n",
      "Q値の教師データ: tensor([-0.0122, -1.0000, -0.0051, -0.0073, -1.0000, -1.0000,  0.0283, -0.0383,\n",
      "         0.0255, -0.0096, -0.0357, -0.0047, -0.0023, -0.0056,  0.0072, -1.0000,\n",
      "        -0.0375, -0.0037,  0.0262, -0.0094,  0.0177,  0.0207, -0.0178,  0.0027,\n",
      "         0.0063,  0.0182,  0.0084,  0.0086,  0.0223, -0.0318, -0.0378,  0.0317])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0053, -0.0350, -0.0378, -0.0035,  0.0207,  0.0000, -0.0054,  0.0186,\n",
      "         0.0196,  0.0099, -0.0384,  0.0209, -0.0128,  0.0000, -0.0133,  0.0231,\n",
      "         0.0236,  0.0189,  0.0188,  0.0079,  0.0032, -0.0030,  0.0000,  0.0300,\n",
      "        -0.0061, -0.0231,  0.0055, -0.0033, -0.0350, -0.0402,  0.0175, -0.0072])\n",
      "Q値の教師データ: tensor([-0.0052, -0.0347, -0.0374, -0.0035,  0.0205, -1.0000, -0.0053,  0.0184,\n",
      "         0.0194,  0.0098, -0.0380,  0.0207, -0.0126, -1.0000, -0.0132,  0.0228,\n",
      "         0.0233,  0.0187,  0.0186,  0.0079,  0.0032, -0.0029, -1.0000,  0.0297,\n",
      "        -0.0060, -0.0229,  0.0054, -0.0032, -0.0347, -0.0398,  0.0174, -0.0071])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([-8.0557e-03,  0.0000e+00,  1.5986e-02, -2.3759e-02, -7.2222e-03,\n",
      "         0.0000e+00, -3.8543e-02,  2.0087e-02,  3.1147e-02, -3.9540e-02,\n",
      "         2.9824e-02,  7.8882e-03,  1.8458e-02, -6.9927e-03,  1.7262e-02,\n",
      "         7.8801e-03,  1.9897e-02,  2.0535e-02,  1.9433e-02,  9.6293e-05,\n",
      "        -8.5784e-03,  1.8500e-02, -4.7618e-03, -3.8084e-02,  2.0189e-02,\n",
      "         1.5508e-02, -1.3711e-02,  1.8075e-02,  1.9522e-02,  1.7848e-02,\n",
      "         0.0000e+00,  1.7199e-02])\n",
      "Q値の教師データ: tensor([-7.9751e-03, -1.0000e+00,  1.5826e-02, -2.3521e-02, -7.1500e-03,\n",
      "        -1.0000e+00, -3.8158e-02,  1.9886e-02,  3.0836e-02, -3.9145e-02,\n",
      "         2.9526e-02,  7.8093e-03,  1.8273e-02, -6.9228e-03,  1.7089e-02,\n",
      "         7.8013e-03,  1.9698e-02,  2.0330e-02,  1.9238e-02,  9.5330e-05,\n",
      "        -8.4926e-03,  1.8315e-02, -4.7142e-03, -3.7703e-02,  1.9988e-02,\n",
      "         1.5353e-02, -1.3574e-02,  1.7895e-02,  1.9327e-02,  1.7670e-02,\n",
      "        -1.0000e+00,  1.7027e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0188,  0.0181, -0.0048,  0.0158, -0.0045,  0.0205, -0.0239,  0.0191,\n",
      "        -0.0404,  0.0185,  0.0185, -0.0100, -0.0096,  0.0191,  0.0178, -0.0063,\n",
      "         0.0120,  0.0000,  0.0145,  0.0193, -0.0088,  0.0222,  0.0307,  0.0242,\n",
      "        -0.0080,  0.0169, -0.0101,  0.0033,  0.0000,  0.0000, -0.0126,  0.0194])\n",
      "Q値の教師データ: tensor([ 0.0186,  0.0179, -0.0047,  0.0157, -0.0044,  0.0203, -0.0237,  0.0189,\n",
      "        -0.0400,  0.0183,  0.0183, -0.0099, -0.0095,  0.0189,  0.0176, -0.0062,\n",
      "         0.0118, -1.0000,  0.0144,  0.0191, -0.0087,  0.0220,  0.0304,  0.0240,\n",
      "        -0.0080,  0.0168, -0.0100,  0.0032, -1.0000, -1.0000, -0.0124,  0.0192])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0397,  0.0188,  0.0170, -0.0072, -0.0392, -0.0090,  0.0184, -0.0243,\n",
      "        -0.0394,  0.0163,  0.0000, -0.0137,  0.0211,  0.0200,  0.0186, -0.0114,\n",
      "         0.0186,  0.0189,  0.0190,  0.0056,  0.0195, -0.0144,  0.0043, -0.0313,\n",
      "         0.0042, -0.0404,  0.0258, -0.0243, -0.0364,  0.0160,  0.0074, -0.0217])\n",
      "Q値の教師データ: tensor([-0.0393,  0.0186,  0.0168, -0.0071, -0.0388, -0.0090,  0.0182, -0.0241,\n",
      "        -0.0390,  0.0161, -1.0000, -0.0136,  0.0208,  0.0198,  0.0185, -0.0113,\n",
      "         0.0184,  0.0187,  0.0188,  0.0055,  0.0193, -0.0143,  0.0042, -0.0310,\n",
      "         0.0042, -0.0400,  0.0256, -0.0241, -0.0360,  0.0159,  0.0073, -0.0215])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([ 1.8844e-02,  1.7447e-02,  2.5231e-02,  1.5559e-02,  0.0000e+00,\n",
      "         1.8401e-02,  0.0000e+00, -1.0517e-02,  1.7708e-02, -1.4891e-02,\n",
      "        -1.2511e-02,  8.2385e-06,  2.0179e-02, -1.1911e-02,  2.0717e-02,\n",
      "        -4.7522e-03, -8.3492e-03, -4.1648e-02,  0.0000e+00,  1.9549e-02,\n",
      "        -1.4747e-02,  2.1823e-02, -4.2286e-03,  1.9182e-02,  1.7240e-02,\n",
      "        -1.4183e-02,  1.7675e-02,  3.9333e-03, -1.5428e-02, -2.2104e-02,\n",
      "        -8.2392e-03,  0.0000e+00])\n",
      "Q値の教師データ: tensor([ 1.8655e-02,  1.7273e-02,  2.4979e-02,  1.5404e-02, -1.0000e+00,\n",
      "         1.8217e-02, -1.0000e+00, -1.0412e-02,  1.7530e-02, -1.4742e-02,\n",
      "        -1.2386e-02,  8.1561e-06,  1.9977e-02, -1.1791e-02,  2.0510e-02,\n",
      "        -4.7047e-03, -8.2657e-03, -4.1232e-02, -1.0000e+00,  1.9354e-02,\n",
      "        -1.4600e-02,  2.1605e-02, -4.1864e-03,  1.8990e-02,  1.7067e-02,\n",
      "        -1.4041e-02,  1.7498e-02,  3.8940e-03, -1.5274e-02, -2.1883e-02,\n",
      "        -8.1568e-03, -1.0000e+00])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0183,  0.0187, -0.0373, -0.0103,  0.0179, -0.0100,  0.0063, -0.0115,\n",
      "         0.0147,  0.0036, -0.0375,  0.0192,  0.0034,  0.0083, -0.0133, -0.0409,\n",
      "         0.0188,  0.0183, -0.0105,  0.0118, -0.0405, -0.0132,  0.0274,  0.0000,\n",
      "         0.0281,  0.0192, -0.0202,  0.0224,  0.0194,  0.0115,  0.0044,  0.0058])\n",
      "Q値の教師データ: tensor([ 0.0182,  0.0185, -0.0370, -0.0101,  0.0177, -0.0099,  0.0063, -0.0114,\n",
      "         0.0146,  0.0036, -0.0371,  0.0191,  0.0034,  0.0082, -0.0132, -0.0405,\n",
      "         0.0187,  0.0181, -0.0104,  0.0116, -0.0401, -0.0131,  0.0272, -1.0000,\n",
      "         0.0278,  0.0190, -0.0200,  0.0222,  0.0192,  0.0114,  0.0044,  0.0057])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0192, -0.0377,  0.0000, -0.0236,  0.0162,  0.0068, -0.0379,  0.0212,\n",
      "         0.0191,  0.0177,  0.0033,  0.0000,  0.0000,  0.0168, -0.0350,  0.0064,\n",
      "         0.0177,  0.0080, -0.0151, -0.0400,  0.0240, -0.0064, -0.0419,  0.0000,\n",
      "         0.0184, -0.0114,  0.0038,  0.0148,  0.0277,  0.0173,  0.0045, -0.0131])\n",
      "Q値の教師データ: tensor([ 0.0190, -0.0373, -1.0000, -0.0234,  0.0161,  0.0067, -0.0375,  0.0210,\n",
      "         0.0189,  0.0176,  0.0033, -1.0000, -1.0000,  0.0166, -0.0346,  0.0063,\n",
      "         0.0176,  0.0079, -0.0149, -0.0396,  0.0237, -0.0063, -0.0415, -1.0000,\n",
      "         0.0183, -0.0113,  0.0037,  0.0146,  0.0274,  0.0172,  0.0045, -0.0130])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0009,  0.0190, -0.0240,  0.0185,  0.0109, -0.0037,  0.0174, -0.0418,\n",
      "         0.0274, -0.0128,  0.0203, -0.0121,  0.0181,  0.0169,  0.0198,  0.0150,\n",
      "        -0.0147,  0.0032, -0.0365, -0.0037,  0.0178,  0.0000, -0.0125,  0.0037,\n",
      "         0.0212, -0.0328,  0.0000,  0.0083, -0.0112, -0.0405,  0.0047, -0.0156])\n",
      "Q値の教師データ: tensor([-8.7423e-04,  1.8821e-02, -2.3737e-02,  1.8273e-02,  1.0828e-02,\n",
      "        -3.6838e-03,  1.7179e-02, -4.1367e-02,  2.7153e-02, -1.2653e-02,\n",
      "         2.0114e-02, -1.1939e-02,  1.7894e-02,  1.6776e-02,  1.9565e-02,\n",
      "         1.4872e-02, -1.4578e-02,  3.1211e-03, -3.6157e-02, -3.7017e-03,\n",
      "         1.7582e-02, -1.0000e+00, -1.2407e-02,  3.6228e-03,  2.0981e-02,\n",
      "        -3.2443e-02, -1.0000e+00,  8.2187e-03, -1.1072e-02, -4.0108e-02,\n",
      "         4.6071e-03, -1.5449e-02])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0056,  0.0000,  0.0189, -0.0080, -0.0387,  0.0163, -0.0416, -0.0041,\n",
      "         0.0277, -0.0235,  0.0174,  0.0000,  0.0152,  0.0028,  0.0164,  0.0198,\n",
      "         0.0085,  0.0000, -0.0261, -0.0437,  0.0000,  0.0286, -0.0085, -0.0261,\n",
      "        -0.0128, -0.0331,  0.0266,  0.0185,  0.0258, -0.0447,  0.0149,  0.0175])\n",
      "Q値の教師データ: tensor([ 0.0056, -1.0000,  0.0187, -0.0080, -0.0383,  0.0162, -0.0412, -0.0040,\n",
      "         0.0274, -0.0233,  0.0173, -1.0000,  0.0150,  0.0027,  0.0162,  0.0196,\n",
      "         0.0084, -1.0000, -0.0258, -0.0433, -1.0000,  0.0283, -0.0084, -0.0258,\n",
      "        -0.0127, -0.0328,  0.0264,  0.0183,  0.0256, -0.0443,  0.0147,  0.0174])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0165, -0.0107, -0.0185,  0.0167, -0.0162,  0.0196,  0.0192,  0.0000,\n",
      "         0.0160, -0.0239,  0.0192, -0.0033,  0.0194,  0.0190,  0.0121,  0.0155,\n",
      "        -0.0415,  0.0034, -0.0055, -0.0187,  0.0190,  0.0194, -0.0149,  0.0192,\n",
      "        -0.0034,  0.0104, -0.0426, -0.0335,  0.0000,  0.0054, -0.0079, -0.0432])\n",
      "Q値の教師データ: tensor([ 0.0163, -0.0106, -0.0183,  0.0166, -0.0161,  0.0194,  0.0190, -1.0000,\n",
      "         0.0158, -0.0236,  0.0190, -0.0032,  0.0192,  0.0188,  0.0120,  0.0154,\n",
      "        -0.0411,  0.0034, -0.0055, -0.0185,  0.0189,  0.0192, -0.0147,  0.0190,\n",
      "        -0.0033,  0.0103, -0.0422, -0.0331, -1.0000,  0.0053, -0.0078, -0.0428])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0180,  0.0141,  0.0000,  0.0152, -0.0219, -0.0469,  0.0051,  0.0087,\n",
      "        -0.0047, -0.0159,  0.0000,  0.0143,  0.0000,  0.0189,  0.0116, -0.0071,\n",
      "        -0.0168,  0.0040,  0.0163,  0.0073,  0.0000, -0.0396,  0.0187,  0.0182,\n",
      "         0.0043,  0.0153, -0.0197, -0.0073,  0.0035, -0.0394, -0.0194, -0.0199])\n",
      "Q値の教師データ: tensor([-0.0178,  0.0139, -1.0000,  0.0150, -0.0216, -0.0464,  0.0050,  0.0086,\n",
      "        -0.0047, -0.0157, -1.0000,  0.0142, -1.0000,  0.0187,  0.0115, -0.0070,\n",
      "        -0.0166,  0.0040,  0.0161,  0.0072, -1.0000, -0.0392,  0.0185,  0.0180,\n",
      "         0.0042,  0.0151, -0.0195, -0.0072,  0.0035, -0.0390, -0.0192, -0.0197])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0205,  0.0192, -0.0136, -0.0124,  0.0142,  0.0162,  0.0167, -0.0342,\n",
      "         0.0156, -0.0441,  0.0132, -0.0054,  0.0111, -0.0149,  0.0189,  0.0201,\n",
      "         0.0052,  0.0176, -0.0134, -0.0401, -0.0112,  0.0262,  0.0171, -0.0456,\n",
      "         0.0140,  0.0192, -0.0211, -0.0271,  0.0042,  0.0082,  0.0000, -0.0438])\n",
      "Q値の教師データ: tensor([-0.0203,  0.0190, -0.0135, -0.0123,  0.0140,  0.0161,  0.0165, -0.0338,\n",
      "         0.0155, -0.0437,  0.0131, -0.0054,  0.0110, -0.0148,  0.0187,  0.0199,\n",
      "         0.0051,  0.0174, -0.0133, -0.0397, -0.0111,  0.0260,  0.0169, -0.0451,\n",
      "         0.0138,  0.0190, -0.0209, -0.0268,  0.0041,  0.0081, -1.0000, -0.0433])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0131, -0.0141, -0.0077,  0.0129,  0.0153,  0.0187,  0.0043, -0.0199,\n",
      "        -0.0430,  0.0000, -0.0274,  0.0108,  0.0166, -0.0049,  0.0211,  0.0260,\n",
      "         0.0192,  0.0142,  0.0189, -0.0115, -0.0468,  0.0182, -0.0128, -0.0153,\n",
      "         0.0000, -0.0146, -0.0249, -0.0430, -0.0445, -0.0174,  0.0073,  0.0000])\n",
      "Q値の教師データ: tensor([-0.0130, -0.0139, -0.0077,  0.0128,  0.0151,  0.0185,  0.0043, -0.0197,\n",
      "        -0.0426, -1.0000, -0.0271,  0.0107,  0.0165, -0.0049,  0.0209,  0.0257,\n",
      "         0.0190,  0.0140,  0.0187, -0.0114, -0.0464,  0.0180, -0.0127, -0.0152,\n",
      "        -1.0000, -0.0145, -0.0246, -0.0425, -0.0441, -0.0172,  0.0072, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0210,  0.0127,  0.0000,  0.0161,  0.0077, -0.0214,  0.0141, -0.0219,\n",
      "         0.0182,  0.0045,  0.0124, -0.0483, -0.0408,  0.0075,  0.0000, -0.0149,\n",
      "         0.0000,  0.0133, -0.0144,  0.0170,  0.0192,  0.0089,  0.0000, -0.0279,\n",
      "         0.0183, -0.0374, -0.0450,  0.0176,  0.0187, -0.0200, -0.0461,  0.0000])\n",
      "Q値の教師データ: tensor([-0.0208,  0.0125, -1.0000,  0.0160,  0.0076, -0.0212,  0.0140, -0.0217,\n",
      "         0.0180,  0.0044,  0.0123, -0.0478, -0.0404,  0.0074, -1.0000, -0.0148,\n",
      "        -1.0000,  0.0132, -0.0142,  0.0169,  0.0190,  0.0088, -1.0000, -0.0276,\n",
      "         0.0181, -0.0370, -0.0445,  0.0175,  0.0185, -0.0198, -0.0457, -1.0000])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0041,  0.0000,  0.0171, -0.0282, -0.0192,  0.0167, -0.0378,  0.0194,\n",
      "         0.0013, -0.0454,  0.0093,  0.0000, -0.0121, -0.0456,  0.0164,  0.0260,\n",
      "         0.0128, -0.0133,  0.0123, -0.0410,  0.0000,  0.0000,  0.0231,  0.0120,\n",
      "         0.0158, -0.0140,  0.0173,  0.0079, -0.0147,  0.0196,  0.0130,  0.0178])\n",
      "Q値の教師データ: tensor([ 0.0041, -1.0000,  0.0169, -0.0280, -0.0190,  0.0166, -0.0374,  0.0192,\n",
      "         0.0012, -0.0450,  0.0092, -1.0000, -0.0120, -0.0452,  0.0162,  0.0257,\n",
      "         0.0127, -0.0132,  0.0122, -0.0406, -1.0000, -1.0000,  0.0228,  0.0119,\n",
      "         0.0157, -0.0139,  0.0171,  0.0078, -0.0145,  0.0194,  0.0129,  0.0176])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0251,  0.0168,  0.0193, -0.0038,  0.0000, -0.0038,  0.0175,  0.0176,\n",
      "        -0.0189, -0.0143, -0.0284, -0.0452,  0.0184,  0.0000,  0.0049,  0.0104,\n",
      "        -0.0381,  0.0197, -0.0235,  0.0083,  0.0197,  0.0123, -0.0418,  0.0059,\n",
      "         0.0125, -0.0211, -0.0062, -0.0070, -0.0454,  0.0095, -0.0176,  0.0226])\n",
      "Q値の教師データ: tensor([ 0.0248,  0.0166,  0.0191, -0.0037, -1.0000, -0.0037,  0.0174,  0.0174,\n",
      "        -0.0187, -0.0142, -0.0281, -0.0448,  0.0182, -1.0000,  0.0048,  0.0103,\n",
      "        -0.0378,  0.0195, -0.0232,  0.0082,  0.0195,  0.0121, -0.0413,  0.0058,\n",
      "         0.0124, -0.0209, -0.0062, -0.0070, -0.0449,  0.0094, -0.0174,  0.0224])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([ 0.0197, -0.0449, -0.0043,  0.0061,  0.0142,  0.0172, -0.0199,  0.0131,\n",
      "        -0.0068,  0.0128, -0.0145, -0.0140, -0.0252,  0.0254,  0.0122,  0.0079,\n",
      "         0.0131,  0.0177,  0.0176, -0.0118,  0.0189, -0.0262, -0.0032,  0.0159,\n",
      "         0.0097,  0.0121, -0.0140, -0.0243,  0.0107, -0.0288,  0.0080,  0.0051])\n",
      "Q値の教師データ: tensor([ 0.0195, -0.0444, -0.0043,  0.0060,  0.0141,  0.0170, -0.0197,  0.0130,\n",
      "        -0.0067,  0.0127, -0.0143, -0.0138, -0.0249,  0.0252,  0.0121,  0.0078,\n",
      "         0.0129,  0.0176,  0.0174, -0.0117,  0.0187, -0.0260, -0.0032,  0.0157,\n",
      "         0.0096,  0.0120, -0.0139, -0.0241,  0.0106, -0.0285,  0.0080,  0.0050])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0128,  0.0179,  0.0169, -0.0058, -0.0137, -0.0041,  0.0175,  0.0146,\n",
      "        -0.0130,  0.0160,  0.0260, -0.0115, -0.0043,  0.0035,  0.0166,  0.0053,\n",
      "        -0.0460,  0.0000,  0.0175,  0.0000,  0.0194,  0.0000, -0.0452, -0.0265,\n",
      "         0.0198, -0.0052,  0.0131, -0.0417, -0.0425,  0.0199,  0.0145, -0.0203])\n",
      "Q値の教師データ: tensor([ 0.0127,  0.0177,  0.0167, -0.0057, -0.0136, -0.0041,  0.0173,  0.0145,\n",
      "        -0.0129,  0.0159,  0.0257, -0.0114, -0.0042,  0.0034,  0.0165,  0.0052,\n",
      "        -0.0455, -1.0000,  0.0173, -1.0000,  0.0192, -1.0000, -0.0448, -0.0263,\n",
      "         0.0196, -0.0052,  0.0130, -0.0413, -0.0421,  0.0197,  0.0143, -0.0201])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0103,  0.0081,  0.0249, -0.0050,  0.0107,  0.0000, -0.0020, -0.0491,\n",
      "        -0.0405, -0.0014, -0.0009,  0.0144,  0.0032, -0.0180, -0.0031, -0.0425,\n",
      "         0.0083,  0.0192, -0.0239,  0.0174, -0.0429, -0.0235,  0.0000,  0.0000,\n",
      "         0.0173, -0.0063, -0.0159, -0.0036,  0.0136,  0.0200,  0.0159, -0.0248])\n",
      "Q値の教師データ: tensor([ 1.0238e-02,  8.0040e-03,  2.4682e-02, -4.9038e-03,  1.0563e-02,\n",
      "        -1.0000e+00, -1.9357e-03, -4.8594e-02, -4.0114e-02, -1.3945e-03,\n",
      "        -8.5876e-04,  1.4256e-02,  3.1932e-03, -1.7803e-02, -3.0454e-03,\n",
      "        -4.2066e-02,  8.1836e-03,  1.9018e-02, -2.3642e-02,  1.7273e-02,\n",
      "        -4.2432e-02, -2.3241e-02, -1.0000e+00, -1.0000e+00,  1.7139e-02,\n",
      "        -6.2687e-03, -1.5709e-02, -3.5310e-03,  1.3417e-02,  1.9803e-02,\n",
      "         1.5717e-02, -2.4566e-02])\n",
      "reward_batch: tensor([ 0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0012, -0.0424,  0.0000,  0.0000,  0.0210, -0.0176, -0.0246,  0.0131,\n",
      "        -0.0511,  0.0162,  0.0096,  0.0142,  0.0166, -0.0477,  0.0067,  0.0174,\n",
      "         0.0049,  0.0030,  0.0102,  0.0052,  0.0057,  0.0152, -0.0106,  0.0000,\n",
      "         0.0174,  0.0199, -0.0433, -0.0175, -0.0161,  0.0192,  0.0129, -0.0475])\n",
      "Q値の教師データ: tensor([-0.0012, -0.0419, -1.0000, -1.0000,  0.0208, -0.0174, -0.0244,  0.0130,\n",
      "        -0.0506,  0.0160,  0.0095,  0.0141,  0.0165, -0.0472,  0.0066,  0.0172,\n",
      "         0.0049,  0.0030,  0.0101,  0.0051,  0.0056,  0.0151, -0.0105, -1.0000,\n",
      "         0.0172,  0.0197, -0.0429, -0.0173, -0.0160,  0.0190,  0.0127, -0.0471])\n",
      "15 Episode: Finished after 23 steps：10試行の平均step数 = 11.3\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0127, -0.0188, -0.0194, -0.0240,  0.0186,  0.0097,  0.0119,  0.0201,\n",
      "         0.0000, -0.0034, -0.0122,  0.0209,  0.0140,  0.0094, -0.0475, -0.0464,\n",
      "         0.0104,  0.0253,  0.0177, -0.0231,  0.0202,  0.0123,  0.0119,  0.0077,\n",
      "         0.0121,  0.0000,  0.0204, -0.0172, -0.0015, -0.0437, -0.0500,  0.0054])\n",
      "Q値の教師データ: tensor([ 0.0126, -0.0186, -0.0192, -0.0238,  0.0184,  0.0096,  0.0118,  0.0199,\n",
      "        -1.0000, -0.0034, -0.0121,  0.0207,  0.0139,  0.0093, -0.0470, -0.0459,\n",
      "         0.0103,  0.0250,  0.0175, -0.0228,  0.0200,  0.0122,  0.0118,  0.0076,\n",
      "         0.0119, -1.0000,  0.0202, -0.0170, -0.0015, -0.0433, -0.0495,  0.0054])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0159, -0.0189,  0.0194,  0.0172,  0.0130,  0.0117,  0.0096,  0.0026,\n",
      "        -0.0191,  0.0173, -0.0032,  0.0130, -0.0170,  0.0161, -0.0185,  0.0154,\n",
      "        -0.0484, -0.0042, -0.0014, -0.0283, -0.0170, -0.0125,  0.0193,  0.0000,\n",
      "        -0.0439,  0.0075,  0.0138,  0.0000,  0.0060, -0.0002, -0.0049, -0.0105])\n",
      "Q値の教師データ: tensor([ 1.5764e-02, -1.8707e-02,  1.9193e-02,  1.7076e-02,  1.2872e-02,\n",
      "         1.1629e-02,  9.5510e-03,  2.6014e-03, -1.8942e-02,  1.7162e-02,\n",
      "        -3.2174e-03,  1.2836e-02, -1.6840e-02,  1.5922e-02, -1.8354e-02,\n",
      "         1.5249e-02, -4.7962e-02, -4.2032e-03, -1.3603e-03, -2.7975e-02,\n",
      "        -1.6803e-02, -1.2328e-02,  1.9061e-02, -1.0000e+00, -4.3506e-02,\n",
      "         7.4374e-03,  1.3681e-02, -1.0000e+00,  5.9100e-03, -1.7312e-04,\n",
      "        -4.8397e-03, -1.0409e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0191,  0.0159,  0.0118, -0.0279,  0.0110, -0.0432,  0.0100,  0.0127,\n",
      "        -0.0041, -0.0514, -0.0472,  0.0193,  0.0063,  0.0126,  0.0038,  0.0071,\n",
      "         0.0127,  0.0061, -0.0285,  0.0122,  0.0000,  0.0113, -0.0254, -0.0056,\n",
      "         0.0173, -0.0376,  0.0069,  0.0194,  0.0185, -0.0480,  0.0140, -0.0142])\n",
      "Q値の教師データ: tensor([ 0.0189,  0.0157,  0.0117, -0.0276,  0.0109, -0.0428,  0.0099,  0.0125,\n",
      "        -0.0040, -0.0509, -0.0467,  0.0191,  0.0063,  0.0125,  0.0037,  0.0070,\n",
      "         0.0126,  0.0060, -0.0282,  0.0120, -1.0000,  0.0112, -0.0252, -0.0055,\n",
      "         0.0172, -0.0372,  0.0068,  0.0192,  0.0183, -0.0475,  0.0138, -0.0140])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([ 0.0085, -0.0171, -0.0308,  0.0149,  0.0195, -0.0222, -0.0477, -0.0435,\n",
      "         0.0187, -0.0218, -0.0005,  0.0240, -0.0205,  0.0123,  0.0100, -0.0027,\n",
      "         0.0126,  0.0094,  0.0128,  0.0089,  0.0174, -0.0232,  0.0117, -0.0256,\n",
      "         0.0189,  0.0200,  0.0002,  0.0184,  0.0171,  0.0117,  0.0119,  0.0000])\n",
      "Q値の教師データ: tensor([ 8.4540e-03, -1.6908e-02, -3.0503e-02,  1.4706e-02,  1.9324e-02,\n",
      "        -2.1975e-02, -4.7227e-02, -4.3039e-02,  1.8523e-02, -2.1588e-02,\n",
      "        -5.2880e-04,  2.3741e-02, -2.0259e-02,  1.2209e-02,  9.8565e-03,\n",
      "        -2.6514e-03,  1.2442e-02,  9.2714e-03,  1.2671e-02,  8.7864e-03,\n",
      "         1.7233e-02, -2.2943e-02,  1.1584e-02, -2.5390e-02,  1.8758e-02,\n",
      "         1.9760e-02,  1.5401e-04,  1.8202e-02,  1.6908e-02,  1.1600e-02,\n",
      "         1.1819e-02, -1.0000e+00])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0122, -0.0489, -0.0422,  0.0238,  0.0199,  0.0108,  0.0110,  0.0095,\n",
      "        -0.0497,  0.0161,  0.0000, -0.0176,  0.0186,  0.0246,  0.0208, -0.0204,\n",
      "        -0.0521, -0.0493,  0.0125, -0.0053,  0.0003,  0.0070, -0.0098, -0.0486,\n",
      "         0.0197,  0.0000, -0.0478,  0.0134,  0.0092,  0.0123,  0.0099,  0.0111])\n",
      "Q値の教師データ: tensor([ 1.2116e-02, -4.8381e-02, -4.1763e-02,  2.3597e-02,  1.9717e-02,\n",
      "         1.0668e-02,  1.0870e-02,  9.3773e-03, -4.9207e-02,  1.5913e-02,\n",
      "        -1.0000e+00, -1.7405e-02,  1.8381e-02,  2.4313e-02,  2.0548e-02,\n",
      "        -2.0189e-02, -5.1617e-02, -4.8775e-02,  1.2340e-02, -5.2368e-03,\n",
      "         2.9232e-04,  6.9546e-03, -9.7373e-03, -4.8162e-02,  1.9464e-02,\n",
      "        -1.0000e+00, -4.7304e-02,  1.3299e-02,  9.0669e-03,  1.2150e-02,\n",
      "         9.7517e-03,  1.0967e-02])\n",
      "reward_batch: tensor([ 0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0059, -0.0122,  0.0000,  0.0000,  0.0120, -0.0209, -0.0291,  0.0098,\n",
      "        -0.0047,  0.0172, -0.0147,  0.0115,  0.0101, -0.0261, -0.0176, -0.0051,\n",
      "         0.0194,  0.0198,  0.0107, -0.0097, -0.0439,  0.0104,  0.0000,  0.0144,\n",
      "        -0.0453, -0.0286,  0.0209, -0.0057, -0.0175,  0.0110,  0.0000,  0.0036])\n",
      "Q値の教師データ: tensor([ 0.0058, -0.0120, -1.0000, -1.0000,  0.0119, -0.0207, -0.0289,  0.0097,\n",
      "        -0.0046,  0.0171, -0.0146,  0.0114,  0.0100, -0.0258, -0.0174, -0.0051,\n",
      "         0.0192,  0.0196,  0.0106, -0.0096, -0.0435,  0.0103, -1.0000,  0.0143,\n",
      "        -0.0448, -0.0283,  0.0207, -0.0056, -0.0173,  0.0109, -1.0000,  0.0036])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1., -1.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0499, -0.0049, -0.0484,  0.0208,  0.0117, -0.0045,  0.0153, -0.0110,\n",
      "         0.0139,  0.0098,  0.0000,  0.0116,  0.0000,  0.0242,  0.0158, -0.0025,\n",
      "         0.0169,  0.0062,  0.0000,  0.0100,  0.0000,  0.0184, -0.0427,  0.0141,\n",
      "         0.0110,  0.0193, -0.0157,  0.0018,  0.0108,  0.0000,  0.0000, -0.0202])\n",
      "Q値の教師データ: tensor([-0.0494, -0.0049, -0.0479,  0.0206,  0.0116, -0.0044,  0.0151, -0.0109,\n",
      "         0.0138,  0.0097, -1.0000,  0.0115, -1.0000,  0.0240,  0.0157, -0.0025,\n",
      "         0.0167,  0.0062, -1.0000,  0.0099, -1.0000,  0.0182, -0.0423,  0.0139,\n",
      "         0.0108,  0.0191, -0.0156,  0.0018,  0.0107, -1.0000, -1.0000, -0.0200])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0023, -0.0445,  0.0000, -0.0450,  0.0175, -0.0206,  0.0102, -0.0389,\n",
      "         0.0000,  0.0180,  0.0076, -0.0020,  0.0000,  0.0026, -0.0112, -0.0210,\n",
      "        -0.0116, -0.0503, -0.0047,  0.0113,  0.0126, -0.0317, -0.0292, -0.0177,\n",
      "         0.0000,  0.0137, -0.0112, -0.0266,  0.0078,  0.0139,  0.0073, -0.0460])\n",
      "Q値の教師データ: tensor([-0.0022, -0.0441, -1.0000, -0.0445,  0.0174, -0.0204,  0.0101, -0.0385,\n",
      "        -1.0000,  0.0178,  0.0076, -0.0020, -1.0000,  0.0026, -0.0111, -0.0208,\n",
      "        -0.0115, -0.0498, -0.0047,  0.0112,  0.0125, -0.0313, -0.0289, -0.0175,\n",
      "        -1.0000,  0.0136, -0.0110, -0.0263,  0.0078,  0.0137,  0.0073, -0.0455])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0135,  0.0073,  0.0121,  0.0106,  0.0148, -0.0300,  0.0213,  0.0120,\n",
      "         0.0185,  0.0113,  0.0063, -0.0018,  0.0135,  0.0164, -0.0088,  0.0167,\n",
      "         0.0140,  0.0000,  0.0109,  0.0067,  0.0171, -0.0502,  0.0126,  0.0197,\n",
      "        -0.0464,  0.0000,  0.0071, -0.0434,  0.0168,  0.0162,  0.0000,  0.0139])\n",
      "Q値の教師データ: tensor([ 0.0134,  0.0072,  0.0119,  0.0105,  0.0147, -0.0297,  0.0211,  0.0119,\n",
      "         0.0183,  0.0112,  0.0062, -0.0017,  0.0134,  0.0162, -0.0087,  0.0166,\n",
      "         0.0139, -1.0000,  0.0108,  0.0066,  0.0169, -0.0497,  0.0124,  0.0195,\n",
      "        -0.0459, -1.0000,  0.0070, -0.0429,  0.0167,  0.0160, -1.0000,  0.0138])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1., -1.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0147,  0.0103,  0.0121,  0.0171, -0.0519,  0.0095,  0.0028,\n",
      "         0.0167,  0.0124,  0.0191,  0.0164, -0.0110,  0.0032,  0.0167,  0.0075,\n",
      "         0.0112,  0.0190, -0.0147, -0.0324,  0.0100,  0.0054,  0.0118, -0.0158,\n",
      "         0.0191, -0.0457,  0.0116,  0.0163,  0.0184,  0.0000,  0.0000, -0.0508])\n",
      "Q値の教師データ: tensor([-1.0000, -0.0145,  0.0102,  0.0119,  0.0169, -0.0514,  0.0094,  0.0027,\n",
      "         0.0166,  0.0122,  0.0189,  0.0163, -0.0109,  0.0032,  0.0166,  0.0075,\n",
      "         0.0111,  0.0188, -0.0146, -0.0321,  0.0099,  0.0053,  0.0117, -0.0156,\n",
      "         0.0189, -0.0452,  0.0115,  0.0161,  0.0182, -1.0000, -1.0000, -0.0503])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0143, -0.0106,  0.0178,  0.0018,  0.0128,  0.0102,  0.0130,  0.0183,\n",
      "         0.0216,  0.0109, -0.0524, -0.0162,  0.0150, -0.0034, -0.0555,  0.0232,\n",
      "         0.0117,  0.0174,  0.0029,  0.0120,  0.0119,  0.0114,  0.0078,  0.0000,\n",
      "         0.0165,  0.0083, -0.0021,  0.0067, -0.0507, -0.0161,  0.0007,  0.0114])\n",
      "Q値の教師データ: tensor([ 1.4195e-02, -1.0514e-02,  1.7603e-02,  1.7584e-03,  1.2718e-02,\n",
      "         1.0111e-02,  1.2870e-02,  1.8094e-02,  2.1382e-02,  1.0791e-02,\n",
      "        -5.1873e-02, -1.6018e-02,  1.4885e-02, -3.4085e-03, -5.4899e-02,\n",
      "         2.3008e-02,  1.1551e-02,  1.7207e-02,  2.9060e-03,  1.1890e-02,\n",
      "         1.1738e-02,  1.1307e-02,  7.7350e-03, -1.0000e+00,  1.6362e-02,\n",
      "         8.2364e-03, -2.0792e-03,  6.6420e-03, -5.0215e-02, -1.5927e-02,\n",
      "         6.4504e-04,  1.1329e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0166, -0.0076,  0.0100, -0.0511,  0.0059,  0.0133,  0.0000,  0.0119,\n",
      "         0.0099, -0.0193,  0.0088,  0.0066,  0.0006, -0.0305,  0.0168,  0.0110,\n",
      "         0.0185,  0.0216,  0.0000,  0.0118,  0.0051,  0.0108,  0.0204, -0.0159,\n",
      "        -0.0094,  0.0121,  0.0122, -0.0516,  0.0112,  0.0000,  0.0100, -0.0020])\n",
      "Q値の教師データ: tensor([ 1.6420e-02, -7.4963e-03,  9.8703e-03, -5.0602e-02,  5.8835e-03,\n",
      "         1.3128e-02, -1.0000e+00,  1.1790e-02,  9.8166e-03, -1.9128e-02,\n",
      "         8.7003e-03,  6.5704e-03,  5.4916e-04, -3.0153e-02,  1.6617e-02,\n",
      "         1.0865e-02,  1.8272e-02,  2.1382e-02, -1.0000e+00,  1.1700e-02,\n",
      "         5.0569e-03,  1.0669e-02,  2.0198e-02, -1.5719e-02, -9.3398e-03,\n",
      "         1.1973e-02,  1.2111e-02, -5.1041e-02,  1.1137e-02, -1.0000e+00,\n",
      "         9.8874e-03, -1.9861e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0025,  0.0056,  0.0080, -0.0308, -0.0182,  0.0000,  0.0112,  0.0181,\n",
      "        -0.0509, -0.0150,  0.0003,  0.0109,  0.0130,  0.0059,  0.0144, -0.0091,\n",
      "        -0.0520,  0.0083,  0.0181,  0.0000,  0.0194,  0.0023,  0.0075,  0.0012,\n",
      "        -0.0017, -0.0095,  0.0126,  0.0125,  0.0178,  0.0000, -0.0158,  0.0102])\n",
      "Q値の教師データ: tensor([ 2.5001e-03,  5.5396e-03,  7.9590e-03, -3.0468e-02, -1.8056e-02,\n",
      "        -1.0000e+00,  1.1133e-02,  1.7903e-02, -5.0349e-02, -1.4894e-02,\n",
      "         3.2034e-04,  1.0782e-02,  1.2856e-02,  5.8434e-03,  1.4301e-02,\n",
      "        -9.0460e-03, -5.1482e-02,  8.1998e-03,  1.7890e-02, -1.0000e+00,\n",
      "         1.9249e-02,  2.2826e-03,  7.3937e-03,  1.2177e-03, -1.7234e-03,\n",
      "        -9.4387e-03,  1.2476e-02,  1.2330e-02,  1.7671e-02, -1.0000e+00,\n",
      "        -1.5688e-02,  1.0103e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0202,  0.0163, -0.0529,  0.0204, -0.0133,  0.0000,  0.0177,  0.0000,\n",
      "         0.0120, -0.0409, -0.0228,  0.0111,  0.0166,  0.0000,  0.0194,  0.0000,\n",
      "         0.0000, -0.0222,  0.0053, -0.0185,  0.0120,  0.0114,  0.0128, -0.0538,\n",
      "         0.0170,  0.0065,  0.0181,  0.0118, -0.0562,  0.0138, -0.0530, -0.0285])\n",
      "Q値の教師データ: tensor([ 0.0200,  0.0162, -0.0523,  0.0202, -0.0131, -1.0000,  0.0176, -1.0000,\n",
      "         0.0119, -0.0405, -0.0226,  0.0110,  0.0164, -1.0000,  0.0192, -1.0000,\n",
      "        -1.0000, -0.0220,  0.0052, -0.0183,  0.0119,  0.0113,  0.0126, -0.0533,\n",
      "         0.0169,  0.0064,  0.0179,  0.0116, -0.0556,  0.0137, -0.0524, -0.0282])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0186,  0.0194,  0.0049,  0.0000, -0.0001, -0.0339,  0.0115,  0.0052,\n",
      "         0.0102, -0.0529,  0.0165,  0.0167, -0.0070,  0.0172,  0.0115,  0.0085,\n",
      "         0.0122, -0.0066,  0.0178, -0.0200, -0.0176,  0.0045,  0.0029,  0.0194,\n",
      "         0.0000,  0.0080, -0.0543, -0.0002,  0.0095,  0.0113,  0.0124, -0.0340])\n",
      "Q値の教師データ: tensor([ 1.8403e-02,  1.9244e-02,  4.8571e-03, -1.0000e+00, -1.4664e-04,\n",
      "        -3.3600e-02,  1.1395e-02,  5.1788e-03,  1.0144e-02, -5.2383e-02,\n",
      "         1.6346e-02,  1.6504e-02, -6.9499e-03,  1.7070e-02,  1.1382e-02,\n",
      "         8.3922e-03,  1.2037e-02, -6.5456e-03,  1.7656e-02, -1.9814e-02,\n",
      "        -1.7455e-02,  4.4155e-03,  2.8550e-03,  1.9232e-02, -1.0000e+00,\n",
      "         7.8998e-03, -5.3801e-02, -1.5595e-04,  9.4390e-03,  1.1208e-02,\n",
      "         1.2229e-02, -3.3663e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 1.2560e-02,  1.0556e-02, -2.6584e-03, -1.7441e-02, -3.2134e-03,\n",
      "        -5.7700e-02, -8.6034e-03,  4.9471e-03,  0.0000e+00,  8.7990e-03,\n",
      "         1.6268e-02, -5.3343e-02,  1.3742e-03, -1.4370e-02,  1.6584e-02,\n",
      "         1.0872e-02,  2.0248e-02,  6.8396e-03,  0.0000e+00,  1.0173e-02,\n",
      "        -9.0477e-03,  1.3155e-02, -5.3439e-02,  7.0493e-03,  1.1814e-02,\n",
      "         1.7094e-02, -3.4244e-02,  4.7087e-03,  1.1432e-02,  8.3799e-05,\n",
      "        -5.2782e-02,  1.1915e-02])\n",
      "Q値の教師データ: tensor([ 1.2434e-02,  1.0450e-02, -2.6319e-03, -1.7267e-02, -3.1813e-03,\n",
      "        -5.7123e-02, -8.5174e-03,  4.8976e-03, -1.0000e+00,  8.7110e-03,\n",
      "         1.6105e-02, -5.2810e-02,  1.3604e-03, -1.4226e-02,  1.6418e-02,\n",
      "         1.0763e-02,  2.0046e-02,  6.7712e-03, -1.0000e+00,  1.0071e-02,\n",
      "        -8.9573e-03,  1.3024e-02, -5.2904e-02,  6.9788e-03,  1.1696e-02,\n",
      "         1.6923e-02, -3.3902e-02,  4.6616e-03,  1.1317e-02,  8.2961e-05,\n",
      "        -5.2254e-02,  1.1796e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0116,  0.0023, -0.0125,  0.0133, -0.0122,  0.0099,  0.0112,  0.0108,\n",
      "         0.0084,  0.0137,  0.0164,  0.0119,  0.0000, -0.0007,  0.0114,  0.0099,\n",
      "        -0.0025, -0.0553,  0.0174, -0.0345,  0.0120,  0.0133,  0.0112,  0.0000,\n",
      "         0.0040,  0.0219, -0.0531,  0.0206,  0.0045, -0.0320,  0.0089, -0.0061])\n",
      "Q値の教師データ: tensor([ 1.1481e-02,  2.2933e-03, -1.2352e-02,  1.3165e-02, -1.2092e-02,\n",
      "         9.8204e-03,  1.1047e-02,  1.0665e-02,  8.2813e-03,  1.3574e-02,\n",
      "         1.6256e-02,  1.1771e-02, -1.0000e+00, -7.1466e-04,  1.1255e-02,\n",
      "         9.7593e-03, -2.4344e-03, -5.4720e-02,  1.7266e-02, -3.4153e-02,\n",
      "         1.1875e-02,  1.3198e-02,  1.1099e-02, -1.0000e+00,  4.0061e-03,\n",
      "         2.1685e-02, -5.2596e-02,  2.0414e-02,  4.4589e-03, -3.1721e-02,\n",
      "         8.8571e-03, -6.0639e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0117,  0.0177,  0.0110,  0.0202,  0.0160, -0.0135,  0.0000,  0.0015,\n",
      "        -0.0448, -0.0542, -0.0015,  0.0120, -0.0087,  0.0019, -0.0479,  0.0109,\n",
      "        -0.0557,  0.0190,  0.0093, -0.0077,  0.0124,  0.0136,  0.0034,  0.0060,\n",
      "         0.0010,  0.0119,  0.0077,  0.0000, -0.0172, -0.0079,  0.0060,  0.0011])\n",
      "Q値の教師データ: tensor([ 0.0116,  0.0176,  0.0109,  0.0200,  0.0158, -0.0133, -1.0000,  0.0014,\n",
      "        -0.0443, -0.0537, -0.0015,  0.0119, -0.0086,  0.0019, -0.0474,  0.0108,\n",
      "        -0.0551,  0.0188,  0.0092, -0.0077,  0.0123,  0.0134,  0.0034,  0.0059,\n",
      "         0.0010,  0.0118,  0.0076, -1.0000, -0.0171, -0.0078,  0.0059,  0.0011])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0104, -0.0561,  0.0058,  0.0096,  0.0092,  0.0172,  0.0117, -0.0467,\n",
      "         0.0118,  0.0193,  0.0000,  0.0184,  0.0138,  0.0175,  0.0000,  0.0053,\n",
      "         0.0098,  0.0065,  0.0008,  0.0183,  0.0102, -0.0351,  0.0166, -0.0531,\n",
      "        -0.0059, -0.0581, -0.0451,  0.0006,  0.0000, -0.0234, -0.0181, -0.0199])\n",
      "Q値の教師データ: tensor([ 1.0261e-02, -5.5563e-02,  5.7072e-03,  9.4588e-03,  9.1351e-03,\n",
      "         1.7065e-02,  1.1629e-02, -4.6277e-02,  1.1660e-02,  1.9069e-02,\n",
      "        -1.0000e+00,  1.8233e-02,  1.3697e-02,  1.7365e-02, -1.0000e+00,\n",
      "         5.2317e-03,  9.7019e-03,  6.4371e-03,  7.7828e-04,  1.8070e-02,\n",
      "         1.0059e-02, -3.4756e-02,  1.6428e-02, -5.2571e-02, -5.8017e-03,\n",
      "        -5.7553e-02, -4.4604e-02,  6.3965e-04, -1.0000e+00, -2.3210e-02,\n",
      "        -1.7949e-02, -1.9708e-02])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0110, -0.0087,  0.0000, -0.0004, -0.0593,  0.0138,  0.0176,  0.0056,\n",
      "         0.0164,  0.0173, -0.0501,  0.0122,  0.0005,  0.0063,  0.0017,  0.0121,\n",
      "         0.0192,  0.0000,  0.0052, -0.0566,  0.0007,  0.0210,  0.0206, -0.0588,\n",
      "         0.0160,  0.0090,  0.0116, -0.0485,  0.0118, -0.0353,  0.0073, -0.0586])\n",
      "Q値の教師データ: tensor([ 1.0901e-02, -8.6246e-03, -1.0000e+00, -3.8628e-04, -5.8723e-02,\n",
      "         1.3701e-02,  1.7413e-02,  5.5441e-03,  1.6254e-02,  1.7175e-02,\n",
      "        -4.9595e-02,  1.2038e-02,  4.5081e-04,  6.2593e-03,  1.6489e-03,\n",
      "         1.1992e-02,  1.9009e-02, -1.0000e+00,  5.1106e-03, -5.5990e-02,\n",
      "         7.3751e-04,  2.0808e-02,  2.0381e-02, -5.8256e-02,  1.5807e-02,\n",
      "         8.8999e-03,  1.1521e-02, -4.8010e-02,  1.1714e-02, -3.4927e-02,\n",
      "         7.1832e-03, -5.7989e-02])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([ 0.0092,  0.0055,  0.0097, -0.0139, -0.0493,  0.0165,  0.0138,  0.0020,\n",
      "         0.0159,  0.0107,  0.0019,  0.0154,  0.0103, -0.0456, -0.0083, -0.0078,\n",
      "        -0.0019,  0.0124, -0.0331,  0.0094, -0.0556, -0.0129,  0.0033,  0.0077,\n",
      "         0.0109,  0.0194,  0.0201,  0.0169,  0.0104, -0.0604,  0.0096, -0.0245])\n",
      "Q値の教師データ: tensor([ 0.0091,  0.0054,  0.0096, -0.0137, -0.0488,  0.0163,  0.0137,  0.0020,\n",
      "         0.0158,  0.0106,  0.0019,  0.0152,  0.0102, -0.0451, -0.0082, -0.0078,\n",
      "        -0.0019,  0.0123, -0.0328,  0.0093, -0.0550, -0.0127,  0.0033,  0.0077,\n",
      "         0.0108,  0.0192,  0.0199,  0.0168,  0.0103, -0.0598,  0.0095, -0.0243])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0099,  0.0153,  0.0209, -0.0113, -0.0506,  0.0076,  0.0099,  0.0161,\n",
      "         0.0140,  0.0024,  0.0108, -0.0116,  0.0033,  0.0113,  0.0018, -0.0080,\n",
      "         0.0123,  0.0102, -0.0132, -0.0115, -0.0013, -0.0055, -0.0556, -0.0090,\n",
      "         0.0215,  0.0183,  0.0000,  0.0032,  0.0103,  0.0000, -0.0187,  0.0171])\n",
      "Q値の教師データ: tensor([ 0.0098,  0.0152,  0.0207, -0.0112, -0.0501,  0.0075,  0.0098,  0.0159,\n",
      "         0.0138,  0.0024,  0.0107, -0.0115,  0.0033,  0.0112,  0.0017, -0.0080,\n",
      "         0.0121,  0.0101, -0.0131, -0.0114, -0.0013, -0.0054, -0.0551, -0.0089,\n",
      "         0.0213,  0.0181, -1.0000,  0.0031,  0.0102, -1.0000, -0.0185,  0.0169])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0161, -0.0508,  0.0106, -0.0335,  0.0089,  0.0126,  0.0013,  0.0127,\n",
      "        -0.0559, -0.0126, -0.0545,  0.0048,  0.0037, -0.0360,  0.0124,  0.0160,\n",
      "         0.0175, -0.0008, -0.0044,  0.0031,  0.0164,  0.0203,  0.0216,  0.0158,\n",
      "         0.0108,  0.0000, -0.0207, -0.0125,  0.0000,  0.0085,  0.0090,  0.0001])\n",
      "Q値の教師データ: tensor([ 1.5935e-02, -5.0327e-02,  1.0455e-02, -3.3142e-02,  8.7981e-03,\n",
      "         1.2469e-02,  1.2585e-03,  1.2551e-02, -5.5350e-02, -1.2441e-02,\n",
      "        -5.3976e-02,  4.7974e-03,  3.6464e-03, -3.5615e-02,  1.2282e-02,\n",
      "         1.5879e-02,  1.7308e-02, -7.7637e-04, -4.3107e-03,  3.0388e-03,\n",
      "         1.6250e-02,  2.0072e-02,  2.1352e-02,  1.5595e-02,  1.0683e-02,\n",
      "        -1.0000e+00, -2.0532e-02, -1.2365e-02, -1.0000e+00,  8.4526e-03,\n",
      "         8.9339e-03,  1.4605e-04])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0151,  0.0017, -0.0015,  0.0160, -0.0337,  0.0074, -0.0553,  0.0041,\n",
      "         0.0000, -0.0112,  0.0121, -0.0562, -0.0190,  0.0090,  0.0101,  0.0000,\n",
      "         0.0191,  0.0031,  0.0101,  0.0012, -0.0548, -0.0599, -0.0546,  0.0020,\n",
      "         0.0191,  0.0048, -0.0129,  0.0202, -0.0053,  0.0200,  0.0128, -0.0073])\n",
      "Q値の教師データ: tensor([ 0.0149,  0.0017, -0.0014,  0.0159, -0.0333,  0.0074, -0.0547,  0.0041,\n",
      "        -1.0000, -0.0110,  0.0120, -0.0556, -0.0188,  0.0089,  0.0100, -1.0000,\n",
      "         0.0189,  0.0031,  0.0100,  0.0012, -0.0542, -0.0593, -0.0540,  0.0020,\n",
      "         0.0189,  0.0047, -0.0128,  0.0200, -0.0052,  0.0198,  0.0127, -0.0073])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-6.0220e-02,  1.1928e-02, -5.5537e-02,  1.0108e-02, -5.4487e-03,\n",
      "         2.2270e-04, -5.8420e-05,  1.2992e-02, -5.6655e-02, -4.3710e-02,\n",
      "        -1.6662e-02, -4.8161e-04, -9.6173e-03,  1.0714e-02,  1.6712e-02,\n",
      "         1.0346e-02, -1.2245e-02,  1.9989e-02,  1.1148e-02, -3.4605e-03,\n",
      "         1.0067e-02, -3.6317e-02,  1.0542e-02,  3.1052e-03,  1.3125e-02,\n",
      "         1.6357e-02,  5.6712e-03,  1.0155e-02, -7.6274e-03, -7.1927e-03,\n",
      "        -1.2674e-02,  1.2185e-02])\n",
      "Q値の教師データ: tensor([-5.9618e-02,  1.1808e-02, -5.4981e-02,  1.0007e-02, -5.3942e-03,\n",
      "         2.2048e-04, -5.7836e-05,  1.2862e-02, -5.6088e-02, -4.3273e-02,\n",
      "        -1.6496e-02, -4.7680e-04, -9.5212e-03,  1.0607e-02,  1.6544e-02,\n",
      "         1.0242e-02, -1.2123e-02,  1.9790e-02,  1.1037e-02, -3.4259e-03,\n",
      "         9.9662e-03, -3.5954e-02,  1.0436e-02,  3.0742e-03,  1.2994e-02,\n",
      "         1.6193e-02,  5.6145e-03,  1.0054e-02, -7.5511e-03, -7.1208e-03,\n",
      "        -1.2548e-02,  1.2063e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0212, -0.0131,  0.0162, -0.0312, -0.0557,  0.0067,  0.0112,  0.0098,\n",
      "        -0.0192,  0.0116, -0.0365, -0.0222, -0.0087, -0.0085, -0.0151,  0.0016,\n",
      "         0.0128, -0.0110,  0.0000,  0.0200, -0.0569,  0.0178, -0.0005, -0.0552,\n",
      "        -0.0053, -0.0252, -0.0004,  0.0017,  0.0100,  0.0210,  0.0089, -0.0569])\n",
      "Q値の教師データ: tensor([-2.0960e-02, -1.2951e-02,  1.6066e-02, -3.0850e-02, -5.5171e-02,\n",
      "         6.6381e-03,  1.1054e-02,  9.6847e-03, -1.9025e-02,  1.1461e-02,\n",
      "        -3.6086e-02, -2.1950e-02, -8.6364e-03, -8.4393e-03, -1.4964e-02,\n",
      "         1.5644e-03,  1.2714e-02, -1.0903e-02, -1.0000e+00,  1.9770e-02,\n",
      "        -5.6282e-02,  1.7657e-02, -4.9611e-04, -5.4653e-02, -5.2821e-03,\n",
      "        -2.4960e-02, -4.3527e-04,  1.6584e-03,  9.9444e-03,  2.0780e-02,\n",
      "         8.7861e-03, -5.6315e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-8.5606e-03,  3.0314e-03,  1.6217e-02,  1.0429e-02, -2.5299e-02,\n",
      "         1.9975e-02,  0.0000e+00, -9.6829e-03, -2.4668e-02, -3.6564e-02,\n",
      "        -4.8420e-02, -5.0381e-02,  1.2028e-02,  1.6785e-02,  1.7375e-02,\n",
      "         1.1901e-02,  1.2990e-02, -5.6818e-02,  1.3201e-02,  1.2262e-02,\n",
      "         2.1423e-02, -5.7011e-02,  1.0091e-02, -5.2152e-02,  1.1567e-02,\n",
      "        -5.6812e-02, -8.1688e-05,  1.4764e-02,  8.7604e-03,  8.6511e-03,\n",
      "        -1.0315e-03,  1.4677e-02])\n",
      "Q値の教師データ: tensor([-8.4750e-03,  3.0011e-03,  1.6055e-02,  1.0325e-02, -2.5046e-02,\n",
      "         1.9776e-02, -1.0000e+00, -9.5861e-03, -2.4422e-02, -3.6199e-02,\n",
      "        -4.7935e-02, -4.9877e-02,  1.1908e-02,  1.6617e-02,  1.7201e-02,\n",
      "         1.1782e-02,  1.2861e-02, -5.6250e-02,  1.3069e-02,  1.2139e-02,\n",
      "         2.1209e-02, -5.6441e-02,  9.9902e-03, -5.1631e-02,  1.1452e-02,\n",
      "        -5.6244e-02, -8.0871e-05,  1.4616e-02,  8.6728e-03,  8.5645e-03,\n",
      "        -1.0212e-03,  1.4530e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0119,  0.0143,  0.0111,  0.0067, -0.0570,  0.0034,  0.0000, -0.0020,\n",
      "        -0.0088,  0.0001, -0.0148, -0.0118,  0.0146,  0.0132,  0.0035,  0.0159,\n",
      "         0.0131,  0.0017, -0.0564,  0.0000,  0.0050,  0.0089, -0.0116, -0.0019,\n",
      "         0.0190,  0.0005, -0.0366,  0.0098,  0.0123,  0.0074,  0.0046, -0.0161])\n",
      "Q値の教師データ: tensor([ 1.1746e-02,  1.4128e-02,  1.0998e-02,  6.6541e-03, -5.6383e-02,\n",
      "         3.4040e-03, -1.0000e+00, -1.9366e-03, -8.6678e-03,  1.0998e-04,\n",
      "        -1.4606e-02, -1.1686e-02,  1.4484e-02,  1.3024e-02,  3.4703e-03,\n",
      "         1.5751e-02,  1.2995e-02,  1.6574e-03, -5.5868e-02, -1.0000e+00,\n",
      "         4.9057e-03,  8.8000e-03, -1.1507e-02, -1.9162e-03,  1.8841e-02,\n",
      "         5.2158e-04, -3.6282e-02,  9.6713e-03,  1.2154e-02,  7.3365e-03,\n",
      "         4.5297e-03, -1.5970e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0524,  0.0163,  0.0162, -0.0126,  0.0123, -0.0048,  0.0101,  0.0147,\n",
      "        -0.0145,  0.0017,  0.0074, -0.0517, -0.0009,  0.0201, -0.0314, -0.0194,\n",
      "         0.0000,  0.0102,  0.0156, -0.0611,  0.0126, -0.0116, -0.0556,  0.0150,\n",
      "         0.0122,  0.0000,  0.0158,  0.0007, -0.0007, -0.0114,  0.0105, -0.0094])\n",
      "Q値の教師データ: tensor([-5.1838e-02,  1.6090e-02,  1.5990e-02, -1.2498e-02,  1.2157e-02,\n",
      "        -4.7408e-03,  1.0013e-02,  1.4521e-02, -1.4393e-02,  1.6531e-03,\n",
      "         7.3404e-03, -5.1204e-02, -8.4218e-04,  1.9877e-02, -3.1122e-02,\n",
      "        -1.9233e-02, -1.0000e+00,  1.0060e-02,  1.5478e-02, -6.0500e-02,\n",
      "         1.2520e-02, -1.1507e-02, -5.5068e-02,  1.4863e-02,  1.2040e-02,\n",
      "        -1.0000e+00,  1.5687e-02,  6.5201e-04, -6.9397e-04, -1.1308e-02,\n",
      "         1.0387e-02, -9.2762e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0104,  0.0096,  0.0102, -0.0012,  0.0000, -0.0572,  0.0146, -0.0611,\n",
      "         0.0100,  0.0043,  0.0154,  0.0174, -0.0044, -0.0555,  0.0024,  0.0056,\n",
      "        -0.0567, -0.0112,  0.0161,  0.0112,  0.0111,  0.0099,  0.0097,  0.0049,\n",
      "         0.0183,  0.0117,  0.0115, -0.0009, -0.0097, -0.0225, -0.0343,  0.0098])\n",
      "Q値の教師データ: tensor([ 1.0300e-02,  9.4587e-03,  1.0088e-02, -1.1924e-03, -1.0000e+00,\n",
      "        -5.6658e-02,  1.4422e-02, -6.0468e-02,  9.8977e-03,  4.2334e-03,\n",
      "         1.5237e-02,  1.7275e-02, -4.3293e-03, -5.4942e-02,  2.3810e-03,\n",
      "         5.5716e-03, -5.6145e-02, -1.1102e-02,  1.5972e-02,  1.1100e-02,\n",
      "         1.0988e-02,  9.7649e-03,  9.6281e-03,  4.8730e-03,  1.8092e-02,\n",
      "         1.1566e-02,  1.1360e-02, -8.6445e-04, -9.6212e-03, -2.2265e-02,\n",
      "        -3.3966e-02,  9.6686e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0005,  0.0216,  0.0101,  0.0096, -0.0526,  0.0139,  0.0102,  0.0161,\n",
      "         0.0000, -0.0591,  0.0027,  0.0028,  0.0000, -0.0100,  0.0041, -0.0575,\n",
      "         0.0049, -0.0155, -0.0094,  0.0000,  0.0008,  0.0120,  0.0156,  0.0000,\n",
      "        -0.0112,  0.0009,  0.0049,  0.0016,  0.0104, -0.0612,  0.0102,  0.0038])\n",
      "Q値の教師データ: tensor([-5.4232e-04,  2.1341e-02,  9.9523e-03,  9.4744e-03, -5.2027e-02,\n",
      "         1.3800e-02,  1.0060e-02,  1.5960e-02, -1.0000e+00, -5.8546e-02,\n",
      "         2.6980e-03,  2.7321e-03, -1.0000e+00, -9.9145e-03,  4.0130e-03,\n",
      "        -5.6965e-02,  4.8915e-03, -1.5304e-02, -9.2923e-03, -1.0000e+00,\n",
      "         7.4799e-04,  1.1896e-02,  1.5447e-02, -1.0000e+00, -1.1132e-02,\n",
      "         9.2783e-04,  4.8757e-03,  1.6292e-03,  1.0322e-02, -6.0581e-02,\n",
      "         1.0122e-02,  3.7992e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0040,  0.0158,  0.0096,  0.0107, -0.0503, -0.0094,  0.0000,  0.0043,\n",
      "        -0.0570,  0.0210,  0.0035,  0.0035, -0.0489,  0.0027,  0.0108, -0.0049,\n",
      "         0.0030,  0.0176,  0.0000, -0.0002, -0.0527,  0.0192,  0.0161,  0.0171,\n",
      "         0.0190, -0.0020,  0.0156,  0.0104,  0.0114, -0.0066,  0.0101,  0.0049])\n",
      "Q値の教師データ: tensor([ 3.9675e-03,  1.5642e-02,  9.5512e-03,  1.0589e-02, -4.9803e-02,\n",
      "        -9.3261e-03, -1.0000e+00,  4.2364e-03, -5.6416e-02,  2.0809e-02,\n",
      "         3.4546e-03,  3.4481e-03, -4.8436e-02,  2.7153e-03,  1.0657e-02,\n",
      "        -4.8427e-03,  2.9822e-03,  1.7458e-02, -1.0000e+00, -2.1835e-04,\n",
      "        -5.2155e-02,  1.9036e-02,  1.5938e-02,  1.6916e-02,  1.8782e-02,\n",
      "        -2.0121e-03,  1.5444e-02,  1.0339e-02,  1.1263e-02, -6.4850e-03,\n",
      "         9.9668e-03,  4.8934e-03])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([ 0.0161, -0.0001,  0.0032, -0.0050, -0.0020, -0.0149,  0.0107,  0.0012,\n",
      "        -0.0006,  0.0123,  0.0218,  0.0122,  0.0202, -0.0251, -0.0060,  0.0091,\n",
      "         0.0029,  0.0002,  0.0047,  0.0028,  0.0158,  0.0201, -0.0559, -0.0089,\n",
      "         0.0115,  0.0161,  0.0107,  0.0176, -0.0014, -0.0063,  0.0027,  0.0010])\n",
      "Q値の教師データ: tensor([ 1.5965e-02, -9.9188e-05,  3.2041e-03, -4.9202e-03, -2.0005e-03,\n",
      "        -1.4756e-02,  1.0639e-02,  1.2343e-03, -6.2804e-04,  1.2136e-02,\n",
      "         2.1535e-02,  1.2048e-02,  2.0043e-02, -2.4887e-02, -5.9093e-03,\n",
      "         8.9808e-03,  2.8997e-03,  1.5021e-04,  4.6437e-03,  2.8081e-03,\n",
      "         1.5654e-02,  1.9932e-02, -5.5334e-02, -8.7732e-03,  1.1347e-02,\n",
      "         1.5904e-02,  1.0572e-02,  1.7471e-02, -1.3551e-03, -6.2668e-03,\n",
      "         2.6555e-03,  9.5191e-04])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1., -1., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0063,  0.0107,  0.0092,  0.0171,  0.0113,  0.0034, -0.0093, -0.0371,\n",
      "        -0.0133, -0.0116,  0.0034, -0.0579, -0.0002, -0.0572,  0.0100,  0.0000,\n",
      "         0.0000,  0.0000,  0.0010,  0.0102,  0.0000, -0.0091,  0.0166,  0.0067,\n",
      "         0.0128, -0.0035, -0.0621,  0.0000,  0.0116,  0.0015,  0.0123,  0.0177])\n",
      "Q値の教師データ: tensor([-6.2838e-03,  1.0603e-02,  9.0763e-03,  1.6947e-02,  1.1206e-02,\n",
      "         3.3590e-03, -9.2515e-03, -3.6772e-02, -1.3133e-02, -1.1452e-02,\n",
      "         3.4103e-03, -5.7307e-02, -1.5396e-04, -5.6676e-02,  9.8861e-03,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00,  9.5640e-04,  1.0071e-02,\n",
      "        -1.0000e+00, -8.9604e-03,  1.6409e-02,  6.5858e-03,  1.2636e-02,\n",
      "        -3.4835e-03, -6.1505e-02, -1.0000e+00,  1.1440e-02,  1.4761e-03,\n",
      "         1.2143e-02,  1.7549e-02])\n",
      "16 Episode: Finished after 34 steps：10試行の平均step数 = 13.4\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0581,  0.0048,  0.0212,  0.0162,  0.0111, -0.0351,  0.0027,\n",
      "         0.0044,  0.0000, -0.0033,  0.0116,  0.0121, -0.0050, -0.0055,  0.0133,\n",
      "         0.0028, -0.0373,  0.0157, -0.0229,  0.0171, -0.0100,  0.0163, -0.0506,\n",
      "         0.0000, -0.0047,  0.0000, -0.0373, -0.0513,  0.0148,  0.0095,  0.0189])\n",
      "Q値の教師データ: tensor([-1.0000, -0.0576,  0.0047,  0.0210,  0.0161,  0.0110, -0.0348,  0.0026,\n",
      "         0.0043, -1.0000, -0.0033,  0.0115,  0.0120, -0.0050, -0.0055,  0.0132,\n",
      "         0.0028, -0.0370,  0.0156, -0.0227,  0.0170, -0.0099,  0.0161, -0.0501,\n",
      "        -1.0000, -0.0046, -1.0000, -0.0369, -0.0508,  0.0147,  0.0094,  0.0187])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0097, -0.0091,  0.0046,  0.0054,  0.0085,  0.0013, -0.0527, -0.0061,\n",
      "         0.0164,  0.0000, -0.0017,  0.0000, -0.0040,  0.0059, -0.0377, -0.0098,\n",
      "        -0.0351, -0.0101, -0.0585, -0.0022,  0.0050, -0.0624,  0.0106,  0.0028,\n",
      "         0.0172,  0.0125,  0.0098, -0.0376, -0.0323,  0.0179, -0.0053, -0.0056])\n",
      "Q値の教師データ: tensor([ 0.0096, -0.0090,  0.0046,  0.0054,  0.0085,  0.0013, -0.0522, -0.0060,\n",
      "         0.0162, -1.0000, -0.0017, -1.0000, -0.0039,  0.0058, -0.0373, -0.0097,\n",
      "        -0.0348, -0.0100, -0.0579, -0.0022,  0.0050, -0.0617,  0.0105,  0.0028,\n",
      "         0.0171,  0.0124,  0.0097, -0.0373, -0.0320,  0.0177, -0.0053, -0.0055])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0106, -0.0096, -0.0606, -0.0354,  0.0041, -0.0008,  0.0052, -0.0571,\n",
      "         0.0000,  0.0010, -0.0020,  0.0152,  0.0000,  0.0205, -0.0084,  0.0160,\n",
      "        -0.0264,  0.0176,  0.0000,  0.0141,  0.0000,  0.0137,  0.0112,  0.0172,\n",
      "         0.0163,  0.0029, -0.0587, -0.0568, -0.0582,  0.0220,  0.0091,  0.0153])\n",
      "Q値の教師データ: tensor([ 1.0501e-02, -9.4740e-03, -5.9959e-02, -3.5030e-02,  4.0362e-03,\n",
      "        -7.6622e-04,  5.1706e-03, -5.6547e-02, -1.0000e+00,  1.0116e-03,\n",
      "        -2.0155e-03,  1.5051e-02, -1.0000e+00,  2.0271e-02, -8.2937e-03,\n",
      "         1.5837e-02, -2.6170e-02,  1.7379e-02, -1.0000e+00,  1.3954e-02,\n",
      "        -1.0000e+00,  1.3520e-02,  1.1079e-02,  1.7044e-02,  1.6129e-02,\n",
      "         2.8739e-03, -5.8087e-02, -5.6269e-02, -5.7606e-02,  2.1757e-02,\n",
      "         9.0098e-03,  1.5142e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0205,  0.0171,  0.0131,  0.0203,  0.0117, -0.0096, -0.0228, -0.0580,\n",
      "         0.0059,  0.0116,  0.0008,  0.0000,  0.0000,  0.0049, -0.0094,  0.0204,\n",
      "         0.0166, -0.0269,  0.0140, -0.0205, -0.0082,  0.0168,  0.0026,  0.0174,\n",
      "        -0.0024,  0.0024,  0.0095, -0.0328,  0.0000, -0.0018,  0.0104, -0.0533])\n",
      "Q値の教師データ: tensor([ 2.0302e-02,  1.6967e-02,  1.2945e-02,  2.0130e-02,  1.1623e-02,\n",
      "        -9.4567e-03, -2.2596e-02, -5.7394e-02,  5.8050e-03,  1.1501e-02,\n",
      "         7.8986e-04, -1.0000e+00, -1.0000e+00,  4.8320e-03, -9.2939e-03,\n",
      "         2.0218e-02,  1.6445e-02, -2.6640e-02,  1.3909e-02, -2.0324e-02,\n",
      "        -8.1198e-03,  1.6680e-02,  2.6085e-03,  1.7236e-02, -2.4091e-03,\n",
      "         2.3623e-03,  9.3880e-03, -3.2498e-02, -1.0000e+00, -1.8300e-03,\n",
      "         1.0277e-02, -5.2786e-02])\n",
      "reward_batch: tensor([-1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0487,  0.0187,  0.0000,  0.0048,  0.0019,  0.0118,  0.0140,\n",
      "        -0.0096,  0.0055,  0.0043, -0.0080, -0.0066, -0.0520,  0.0050,  0.0000,\n",
      "         0.0112, -0.0460, -0.0526,  0.0100,  0.0178,  0.0075,  0.0168, -0.0637,\n",
      "         0.0123, -0.0099,  0.0045,  0.0000, -0.0100,  0.0027, -0.0591, -0.0027])\n",
      "Q値の教師データ: tensor([-1.0000, -0.0482,  0.0185, -1.0000,  0.0047,  0.0019,  0.0117,  0.0139,\n",
      "        -0.0095,  0.0055,  0.0042, -0.0080, -0.0065, -0.0514,  0.0049, -1.0000,\n",
      "         0.0111, -0.0455, -0.0520,  0.0099,  0.0176,  0.0074,  0.0166, -0.0630,\n",
      "         0.0121, -0.0098,  0.0044, -1.0000, -0.0099,  0.0026, -0.0585, -0.0027])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0146, -0.0098,  0.0119,  0.0077, -0.0048, -0.0052,  0.0128,  0.0118,\n",
      "         0.0019, -0.0046, -0.0102,  0.0169,  0.0107,  0.0087,  0.0120, -0.0013,\n",
      "         0.0197, -0.0584, -0.0389,  0.0114, -0.0644,  0.0000, -0.0065, -0.0580,\n",
      "         0.0150, -0.0049, -0.0600, -0.0243,  0.0090,  0.0028,  0.0110,  0.0155])\n",
      "Q値の教師データ: tensor([ 0.0144, -0.0097,  0.0118,  0.0076, -0.0048, -0.0052,  0.0127,  0.0117,\n",
      "         0.0019, -0.0045, -0.0101,  0.0167,  0.0106,  0.0086,  0.0119, -0.0013,\n",
      "         0.0195, -0.0578, -0.0386,  0.0113, -0.0638, -1.0000, -0.0065, -0.0575,\n",
      "         0.0148, -0.0048, -0.0594, -0.0241,  0.0089,  0.0028,  0.0109,  0.0153])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0120,  0.0044, -0.0214,  0.0102,  0.0164,  0.0161,  0.0083, -0.0055,\n",
      "         0.0216,  0.0056, -0.0605,  0.0144,  0.0142,  0.0119,  0.0000,  0.0000,\n",
      "         0.0002,  0.0111, -0.0625, -0.0584, -0.0551,  0.0162,  0.0118,  0.0049,\n",
      "        -0.0369,  0.0019,  0.0079, -0.0118, -0.0132,  0.0099, -0.0015,  0.0154])\n",
      "Q値の教師データ: tensor([ 1.1876e-02,  4.3671e-03, -2.1157e-02,  1.0100e-02,  1.6197e-02,\n",
      "         1.5909e-02,  8.1910e-03, -5.4201e-03,  2.1408e-02,  5.5359e-03,\n",
      "        -5.9883e-02,  1.4256e-02,  1.4034e-02,  1.1829e-02, -1.0000e+00,\n",
      "        -1.0000e+00,  2.4706e-04,  1.0958e-02, -6.1911e-02, -5.7835e-02,\n",
      "        -5.4571e-02,  1.5996e-02,  1.1634e-02,  4.8339e-03, -3.6577e-02,\n",
      "         1.8547e-03,  7.8332e-03, -1.1718e-02, -1.3095e-02,  9.7745e-03,\n",
      "        -1.4795e-03,  1.5261e-02])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0166, -0.0653,  0.0000, -0.0630,  0.0200,  0.0215,  0.0112, -0.0395,\n",
      "        -0.0025,  0.0014, -0.0548,  0.0088, -0.0074,  0.0053, -0.0037,  0.0116,\n",
      "         0.0070,  0.0105, -0.0081,  0.0117,  0.0175,  0.0082,  0.0024, -0.0016,\n",
      "         0.0092, -0.0077,  0.0046,  0.0000, -0.0107,  0.0107,  0.0160,  0.0112])\n",
      "Q値の教師データ: tensor([ 0.0164, -0.0646, -1.0000, -0.0624,  0.0198,  0.0213,  0.0111, -0.0391,\n",
      "        -0.0024,  0.0014, -0.0542,  0.0087, -0.0073,  0.0053, -0.0036,  0.0115,\n",
      "         0.0069,  0.0104, -0.0080,  0.0116,  0.0173,  0.0081,  0.0024, -0.0016,\n",
      "         0.0091, -0.0076,  0.0046, -1.0000, -0.0106,  0.0106,  0.0159,  0.0111])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0106,  0.0045, -0.0082,  0.0158,  0.0000,  0.0039, -0.0473,  0.0094,\n",
      "         0.0090,  0.0023,  0.0101,  0.0042,  0.0000,  0.0042,  0.0127,  0.0097,\n",
      "         0.0000,  0.0000,  0.0000,  0.0200, -0.0022, -0.0053,  0.0025,  0.0006,\n",
      "         0.0153,  0.0087,  0.0071,  0.0191,  0.0163, -0.0010, -0.0046, -0.0072])\n",
      "Q値の教師データ: tensor([ 1.0487e-02,  4.4121e-03, -8.0865e-03,  1.5615e-02, -1.0000e+00,\n",
      "         3.8791e-03, -4.6841e-02,  9.2634e-03,  8.8898e-03,  2.2378e-03,\n",
      "         9.9642e-03,  4.1300e-03, -1.0000e+00,  4.1683e-03,  1.2543e-02,\n",
      "         9.6052e-03, -1.0000e+00, -1.0000e+00, -1.0000e+00,  1.9755e-02,\n",
      "        -2.1620e-03, -5.2382e-03,  2.4700e-03,  5.6660e-04,  1.5103e-02,\n",
      "         8.6142e-03,  7.0308e-03,  1.8877e-02,  1.6120e-02, -1.0248e-03,\n",
      "        -4.5321e-03, -7.1629e-03])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0084,  0.0000, -0.0025, -0.0119,  0.0000, -0.0002, -0.0069,  0.0000,\n",
      "         0.0134, -0.0561,  0.0066,  0.0026,  0.0028,  0.0000,  0.0143, -0.0032,\n",
      "         0.0100,  0.0027,  0.0093, -0.0090, -0.0537,  0.0201, -0.0086,  0.0000,\n",
      "         0.0042, -0.0476,  0.0000, -0.0661, -0.0246,  0.0039,  0.0039,  0.0169])\n",
      "Q値の教師データ: tensor([ 8.3220e-03, -1.0000e+00, -2.4376e-03, -1.1766e-02, -1.0000e+00,\n",
      "        -2.3233e-04, -6.8372e-03, -1.0000e+00,  1.3256e-02, -5.5540e-02,\n",
      "         6.5318e-03,  2.5486e-03,  2.7317e-03, -1.0000e+00,  1.4142e-02,\n",
      "        -3.2101e-03,  9.9388e-03,  2.6257e-03,  9.1921e-03, -8.9376e-03,\n",
      "        -5.3139e-02,  1.9879e-02, -8.4841e-03, -1.0000e+00,  4.1412e-03,\n",
      "        -4.7146e-02, -1.0000e+00, -6.5421e-02, -2.4388e-02,  3.8285e-03,\n",
      "         3.8626e-03,  1.6748e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0170,  0.0179,  0.0112, -0.0620, -0.0404, -0.0664,  0.0084,  0.0089,\n",
      "         0.0196, -0.0068,  0.0211, -0.0042,  0.0211, -0.0053, -0.0080,  0.0016,\n",
      "         0.0094,  0.0000, -0.0249,  0.0162,  0.0097,  0.0151,  0.0073,  0.0082,\n",
      "        -0.0024,  0.0000, -0.0480, -0.0044, -0.0224, -0.0381, -0.0101, -0.0049])\n",
      "Q値の教師データ: tensor([ 0.0168,  0.0177,  0.0111, -0.0614, -0.0400, -0.0657,  0.0083,  0.0088,\n",
      "         0.0194, -0.0067,  0.0209, -0.0042,  0.0209, -0.0052, -0.0080,  0.0016,\n",
      "         0.0093, -1.0000, -0.0247,  0.0160,  0.0096,  0.0149,  0.0073,  0.0082,\n",
      "        -0.0024, -1.0000, -0.0475, -0.0044, -0.0222, -0.0377, -0.0100, -0.0048])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0624, -0.0005,  0.0127, -0.0670,  0.0031,  0.0054,  0.0066,  0.0023,\n",
      "         0.0062,  0.0147,  0.0158, -0.0050,  0.0090, -0.0022, -0.0407, -0.0032,\n",
      "        -0.0048,  0.0062,  0.0093, -0.0690,  0.0094,  0.0130,  0.0156, -0.0529,\n",
      "        -0.0074, -0.0046, -0.0057, -0.0568,  0.0087,  0.0121,  0.0058, -0.0621])\n",
      "Q値の教師データ: tensor([-0.0618, -0.0005,  0.0126, -0.0663,  0.0030,  0.0054,  0.0066,  0.0022,\n",
      "         0.0062,  0.0146,  0.0157, -0.0049,  0.0089, -0.0021, -0.0403, -0.0032,\n",
      "        -0.0047,  0.0061,  0.0092, -0.0683,  0.0093,  0.0128,  0.0154, -0.0523,\n",
      "        -0.0073, -0.0046, -0.0056, -0.0562,  0.0087,  0.0120,  0.0057, -0.0615])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0062,  0.0124, -0.0077,  0.0006, -0.0674,  0.0001,  0.0168,  0.0088,\n",
      "         0.0000,  0.0047, -0.0012,  0.0069,  0.0184,  0.0057,  0.0063,  0.0103,\n",
      "        -0.0057,  0.0152,  0.0028,  0.0155, -0.0088, -0.0031,  0.0009, -0.0093,\n",
      "         0.0083,  0.0058, -0.0531,  0.0157,  0.0095,  0.0086, -0.0018,  0.0156])\n",
      "Q値の教師データ: tensor([ 6.1546e-03,  1.2234e-02, -7.6229e-03,  5.8256e-04, -6.6729e-02,\n",
      "         1.1358e-04,  1.6656e-02,  8.6785e-03, -1.0000e+00,  4.6393e-03,\n",
      "        -1.2284e-03,  6.8607e-03,  1.8258e-02,  5.6103e-03,  6.2614e-03,\n",
      "         1.0216e-02, -5.6203e-03,  1.5028e-02,  2.7449e-03,  1.5323e-02,\n",
      "        -8.7068e-03, -3.0844e-03,  8.5213e-04, -9.1794e-03,  8.2139e-03,\n",
      "         5.7896e-03, -5.2608e-02,  1.5592e-02,  9.4253e-03,  8.4736e-03,\n",
      "        -1.8275e-03,  1.5474e-02])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0124, -0.0048,  0.0073,  0.0137,  0.0180,  0.0122, -0.0412,\n",
      "        -0.0079,  0.0077, -0.0011,  0.0121,  0.0142, -0.0566,  0.0059, -0.0079,\n",
      "        -0.0033,  0.0175, -0.0412, -0.0001,  0.0023,  0.0132,  0.0025, -0.0090,\n",
      "         0.0017,  0.0120,  0.0056,  0.0075, -0.0090,  0.0026, -0.0534,  0.0154])\n",
      "Q値の教師データ: tensor([-1.0000e+00, -1.2235e-02, -4.7755e-03,  7.2261e-03,  1.3561e-02,\n",
      "         1.7837e-02,  1.2064e-02, -4.0770e-02, -7.8288e-03,  7.6146e-03,\n",
      "        -1.0602e-03,  1.1995e-02,  1.4056e-02, -5.6075e-02,  5.8464e-03,\n",
      "        -7.8241e-03, -3.2277e-03,  1.7347e-02, -4.0810e-02, -1.2031e-04,\n",
      "         2.2894e-03,  1.3082e-02,  2.5222e-03, -8.8683e-03,  1.6613e-03,\n",
      "         1.1924e-02,  5.5572e-03,  7.3953e-03, -8.9447e-03,  2.5667e-03,\n",
      "        -5.2844e-02,  1.5222e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0157,  0.0164, -0.0633, -0.0036,  0.0021, -0.0137,  0.0057, -0.0020,\n",
      "        -0.0082,  0.0000,  0.0010,  0.0087,  0.0024,  0.0089,  0.0092,  0.0094,\n",
      "         0.0151, -0.0678, -0.0631,  0.0147,  0.0055,  0.0000,  0.0149, -0.0113,\n",
      "        -0.0298,  0.0154,  0.0078, -0.0289, -0.0081,  0.0095, -0.0042, -0.0679])\n",
      "Q値の教師データ: tensor([ 1.5550e-02,  1.6266e-02, -6.2686e-02, -3.6023e-03,  2.0740e-03,\n",
      "        -1.3563e-02,  5.6022e-03, -1.9320e-03, -8.1084e-03, -1.0000e+00,\n",
      "         9.6799e-04,  8.5897e-03,  2.3518e-03,  8.7666e-03,  9.0945e-03,\n",
      "         9.3206e-03,  1.4963e-02, -6.7157e-02, -6.2436e-02,  1.4592e-02,\n",
      "         5.3960e-03, -1.0000e+00,  1.4723e-02, -1.1224e-02, -2.9492e-02,\n",
      "         1.5247e-02,  7.7093e-03, -2.8659e-02, -8.0404e-03,  9.3954e-03,\n",
      "        -4.1708e-03, -6.7221e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0005, -0.0075,  0.0026,  0.0153, -0.0130,  0.0055, -0.0520, -0.0571,\n",
      "        -0.0108,  0.0048,  0.0051, -0.0636, -0.0613,  0.0173, -0.0089,  0.0018,\n",
      "         0.0091, -0.0055, -0.0129,  0.0092,  0.0145,  0.0193,  0.0073, -0.0638,\n",
      "         0.0157, -0.0057, -0.0139,  0.0123,  0.0000,  0.0090,  0.0205,  0.0070])\n",
      "Q値の教師データ: tensor([-5.3359e-04, -7.4181e-03,  2.5360e-03,  1.5176e-02, -1.2916e-02,\n",
      "         5.4388e-03, -5.1434e-02, -5.6486e-02, -1.0664e-02,  4.7992e-03,\n",
      "         5.0475e-03, -6.3008e-02, -6.0727e-02,  1.7148e-02, -8.7788e-03,\n",
      "         1.8292e-03,  8.9781e-03, -5.4916e-03, -1.2769e-02,  9.0939e-03,\n",
      "         1.4382e-02,  1.9063e-02,  7.2096e-03, -6.3138e-02,  1.5584e-02,\n",
      "        -5.6472e-03, -1.3734e-02,  1.2130e-02, -1.0000e+00,  8.9282e-03,\n",
      "         2.0289e-02,  6.9745e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0126,  0.0055,  0.0164,  0.0050,  0.0000,  0.0022,  0.0026, -0.0097,\n",
      "        -0.0075, -0.0638,  0.0073, -0.0044, -0.0097, -0.0130,  0.0111,  0.0018,\n",
      "        -0.0684,  0.0061, -0.0419,  0.0106,  0.0030,  0.0000,  0.0053,  0.0150,\n",
      "        -0.0417, -0.0067, -0.0032,  0.0018,  0.0000,  0.0050,  0.0000,  0.0051])\n",
      "Q値の教師データ: tensor([ 0.0125,  0.0054,  0.0162,  0.0050, -1.0000,  0.0022,  0.0026, -0.0096,\n",
      "        -0.0075, -0.0631,  0.0073, -0.0044, -0.0096, -0.0129,  0.0110,  0.0018,\n",
      "        -0.0677,  0.0061, -0.0414,  0.0105,  0.0030, -1.0000,  0.0052,  0.0149,\n",
      "        -0.0413, -0.0066, -0.0032,  0.0018, -1.0000,  0.0050, -1.0000,  0.0051])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0171, -0.0667, -0.0069,  0.0161,  0.0062,  0.0103,  0.0011,  0.0045,\n",
      "        -0.0055,  0.0087, -0.0667,  0.0003, -0.0096,  0.0089,  0.0017, -0.0523,\n",
      "        -0.0541,  0.0097,  0.0072, -0.0012,  0.0000,  0.0114, -0.0044, -0.0396,\n",
      "         0.0110,  0.0078,  0.0089, -0.0023, -0.0038,  0.0077, -0.0087, -0.0040])\n",
      "Q値の教師データ: tensor([ 1.6887e-02, -6.6000e-02, -6.8082e-03,  1.5913e-02,  6.1633e-03,\n",
      "         1.0184e-02,  1.0508e-03,  4.4441e-03, -5.4191e-03,  8.5864e-03,\n",
      "        -6.6061e-02,  3.4208e-04, -9.4992e-03,  8.7706e-03,  1.7303e-03,\n",
      "        -5.1779e-02, -5.3589e-02,  9.5935e-03,  7.0906e-03, -1.2082e-03,\n",
      "        -1.0000e+00,  1.1244e-02, -4.3587e-03, -3.9168e-02,  1.0859e-02,\n",
      "         7.6758e-03,  8.8291e-03, -2.2627e-03, -3.7304e-03,  7.5766e-03,\n",
      "        -8.6299e-03, -4.0017e-03])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 2.7388e-05,  0.0000e+00, -2.0457e-04,  9.5736e-04, -4.3787e-03,\n",
      "         1.7463e-02,  1.9008e-02,  9.5219e-03, -1.3375e-02, -9.6469e-03,\n",
      "        -6.0622e-03, -1.9115e-03,  1.1299e-02, -3.7523e-03,  5.7843e-03,\n",
      "        -1.2565e-02, -7.0398e-03, -1.8631e-03, -1.0209e-02, -7.9510e-03,\n",
      "         5.6463e-03,  0.0000e+00, -7.8500e-03, -4.5913e-04, -8.6622e-03,\n",
      "         6.7483e-03, -1.1032e-02, -5.8556e-03, -6.6955e-02, -5.6438e-03,\n",
      "        -1.0533e-02,  1.5530e-03])\n",
      "Q値の教師データ: tensor([ 2.7114e-05, -1.0000e+00, -2.0252e-04,  9.4779e-04, -4.3349e-03,\n",
      "         1.7289e-02,  1.8818e-02,  9.4267e-03, -1.3241e-02, -9.5504e-03,\n",
      "        -6.0016e-03, -1.8923e-03,  1.1186e-02, -3.7148e-03,  5.7265e-03,\n",
      "        -1.2440e-02, -6.9694e-03, -1.8444e-03, -1.0107e-02, -7.8715e-03,\n",
      "         5.5899e-03, -1.0000e+00, -7.7715e-03, -4.5454e-04, -8.5756e-03,\n",
      "         6.6808e-03, -1.0922e-02, -5.7970e-03, -6.6286e-02, -5.5873e-03,\n",
      "        -1.0428e-02,  1.5374e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0155, -0.0083,  0.0150, -0.0423,  0.0073,  0.0086,  0.0106, -0.0304,\n",
      "        -0.0009,  0.0078, -0.0546, -0.0128,  0.0095, -0.0014, -0.0586,  0.0147,\n",
      "        -0.0038,  0.0000, -0.0086, -0.0045, -0.0109, -0.0368, -0.0242,  0.0170,\n",
      "        -0.0093, -0.0004,  0.0062,  0.0000, -0.0398, -0.0136,  0.0142,  0.0007])\n",
      "Q値の教師データ: tensor([ 1.5362e-02, -8.1971e-03,  1.4878e-02, -4.1893e-02,  7.2481e-03,\n",
      "         8.5479e-03,  1.0526e-02, -3.0083e-02, -8.7858e-04,  7.6805e-03,\n",
      "        -5.4045e-02, -1.2661e-02,  9.3591e-03, -1.3769e-03, -5.8023e-02,\n",
      "         1.4528e-02, -3.7917e-03, -1.0000e+00, -8.5281e-03, -4.4413e-03,\n",
      "        -1.0790e-02, -3.6443e-02, -2.3975e-02,  1.6816e-02, -9.2264e-03,\n",
      "        -3.9634e-04,  6.1677e-03, -1.0000e+00, -3.9434e-02, -1.3463e-02,\n",
      "         1.4075e-02,  6.9706e-04])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0099,  0.0011, -0.0627,  0.0048,  0.0171,  0.0005,  0.0169,  0.0097,\n",
      "        -0.0053,  0.0064,  0.0060,  0.0137,  0.0031,  0.0156,  0.0188, -0.0401,\n",
      "        -0.0112, -0.0041,  0.0057,  0.0159,  0.0062,  0.0185, -0.0138,  0.0069,\n",
      "        -0.0569,  0.0201, -0.0276, -0.0105,  0.0000, -0.0370,  0.0116, -0.0019])\n",
      "Q値の教師データ: tensor([ 9.8091e-03,  1.1285e-03, -6.2026e-02,  4.7934e-03,  1.6931e-02,\n",
      "         5.0126e-04,  1.6703e-02,  9.6435e-03, -5.2224e-03,  6.2985e-03,\n",
      "         5.9684e-03,  1.3553e-02,  3.0990e-03,  1.5440e-02,  1.8607e-02,\n",
      "        -3.9664e-02, -1.1050e-02, -4.0233e-03,  5.6706e-03,  1.5772e-02,\n",
      "         6.0902e-03,  1.8347e-02, -1.3639e-02,  6.8247e-03, -5.6344e-02,\n",
      "         1.9853e-02, -2.7350e-02, -1.0354e-02, -1.0000e+00, -3.6664e-02,\n",
      "         1.1507e-02, -1.8969e-03])\n",
      "reward_batch: tensor([-1.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000,  0.0084,  0.0000, -0.0271,  0.0089,  0.0000, -0.0012,  0.0135,\n",
      "         0.0062, -0.0010,  0.0155,  0.0107,  0.0078, -0.0040,  0.0044,  0.0046,\n",
      "        -0.0086, -0.0571,  0.0048,  0.0000,  0.0084, -0.0095,  0.0029,  0.0000,\n",
      "         0.0000,  0.0098, -0.0246, -0.0067,  0.0051,  0.0068, -0.0149, -0.0030])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0083, -1.0000, -0.0269,  0.0089, -1.0000, -0.0012,  0.0134,\n",
      "         0.0061, -0.0010,  0.0153,  0.0106,  0.0077, -0.0040,  0.0043,  0.0045,\n",
      "        -0.0085, -0.0566,  0.0047, -1.0000,  0.0083, -0.0094,  0.0029, -1.0000,\n",
      "        -1.0000,  0.0097, -0.0243, -0.0066,  0.0050,  0.0068, -0.0148, -0.0030])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0407, -0.0092,  0.0021, -0.0574, -0.0685,  0.0183, -0.0071,  0.0046,\n",
      "         0.0080,  0.0032,  0.0096, -0.0057,  0.0076,  0.0000,  0.0053, -0.0310,\n",
      "        -0.0430,  0.0007,  0.0127, -0.0077,  0.0056, -0.0012, -0.0032, -0.0660,\n",
      "         0.0094,  0.0132, -0.0085, -0.0275,  0.0147, -0.0106,  0.0134, -0.0594])\n",
      "Q値の教師データ: tensor([-4.0244e-02, -9.1499e-03,  2.0396e-03, -5.6850e-02, -6.7809e-02,\n",
      "         1.8102e-02, -7.0687e-03,  4.5734e-03,  7.9471e-03,  3.1985e-03,\n",
      "         9.5231e-03, -5.6001e-03,  7.5299e-03, -1.0000e+00,  5.2438e-03,\n",
      "        -3.0672e-02, -4.2617e-02,  7.1749e-04,  1.2531e-02, -7.6218e-03,\n",
      "         5.4965e-03, -1.1827e-03, -3.1448e-03, -6.5320e-02,  9.2968e-03,\n",
      "         1.3079e-02, -8.4270e-03, -2.7264e-02,  1.4597e-02, -1.0478e-02,\n",
      "         1.3300e-02, -5.8808e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0039,  0.0045, -0.0095, -0.0434, -0.0043,  0.0061, -0.0021, -0.0103,\n",
      "        -0.0010, -0.0597, -0.0108,  0.0000, -0.0045,  0.0077, -0.0017,  0.0045,\n",
      "        -0.0377, -0.0071, -0.0556,  0.0115,  0.0057,  0.0048,  0.0000,  0.0000,\n",
      "         0.0134,  0.0126, -0.0036, -0.0012,  0.0081, -0.0347,  0.0019,  0.0006])\n",
      "Q値の教師データ: tensor([ 3.8855e-03,  4.4173e-03, -9.3722e-03, -4.2972e-02, -4.2767e-03,\n",
      "         5.9917e-03, -2.1248e-03, -1.0217e-02, -1.0129e-03, -5.9057e-02,\n",
      "        -1.0697e-02, -1.0000e+00, -4.4813e-03,  7.6004e-03, -1.7025e-03,\n",
      "         4.4554e-03, -3.7314e-02, -7.0406e-03, -5.5030e-02,  1.1410e-02,\n",
      "         5.6042e-03,  4.7669e-03, -1.0000e+00, -1.0000e+00,  1.3297e-02,\n",
      "         1.2458e-02, -3.5159e-03, -1.1864e-03,  8.0428e-03, -3.4324e-02,\n",
      "         1.8945e-03,  6.0484e-04])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000,  0.0197, -0.0075, -0.0043, -0.0711, -0.0015,  0.0077,  0.0074,\n",
      "         0.0127,  0.0111,  0.0080, -0.0014, -0.0662, -0.0095,  0.0073,  0.0000,\n",
      "        -0.0435, -0.0379, -0.0014, -0.0102,  0.0074,  0.0150,  0.0067, -0.0284,\n",
      "         0.0054, -0.0645,  0.0139, -0.0103, -0.0558, -0.0716, -0.0108,  0.0078])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0196, -0.0074, -0.0042, -0.0704, -0.0015,  0.0077,  0.0073,\n",
      "         0.0125,  0.0110,  0.0079, -0.0013, -0.0656, -0.0094,  0.0073, -1.0000,\n",
      "        -0.0431, -0.0375, -0.0014, -0.0101,  0.0073,  0.0149,  0.0066, -0.0281,\n",
      "         0.0054, -0.0638,  0.0137, -0.0102, -0.0553, -0.0709, -0.0106,  0.0078])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 1.0764e-02, -7.2727e-03, -7.6855e-03, -6.9550e-02, -8.7433e-03,\n",
      "         0.0000e+00,  3.4205e-03,  1.2896e-02,  5.0011e-03,  5.4035e-03,\n",
      "         7.2334e-03,  1.2680e-02, -2.7954e-02, -8.0251e-03, -5.7536e-02,\n",
      "        -8.5421e-03, -2.8981e-03,  7.1151e-03,  1.4899e-05,  1.7943e-02,\n",
      "        -4.7763e-02,  1.5766e-02, -5.8134e-02,  5.3070e-03, -3.9132e-03,\n",
      "         1.2211e-02,  8.0837e-03,  0.0000e+00,  1.4166e-02,  6.3723e-03,\n",
      "        -9.6284e-03,  1.9692e-02])\n",
      "Q値の教師データ: tensor([ 1.0657e-02, -7.2000e-03, -7.6087e-03, -6.8855e-02, -8.6558e-03,\n",
      "        -1.0000e+00,  3.3863e-03,  1.2767e-02,  4.9511e-03,  5.3495e-03,\n",
      "         7.1611e-03,  1.2553e-02, -2.7674e-02, -7.9448e-03, -5.6961e-02,\n",
      "        -8.4567e-03, -2.8692e-03,  7.0440e-03,  1.4750e-05,  1.7764e-02,\n",
      "        -4.7286e-02,  1.5608e-02, -5.7552e-02,  5.2539e-03, -3.8740e-03,\n",
      "         1.2089e-02,  8.0028e-03, -1.0000e+00,  1.4025e-02,  6.3085e-03,\n",
      "        -9.5321e-03,  1.9495e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 1.2854e-02, -1.2551e-03, -3.9242e-03,  1.0699e-05, -1.8060e-02,\n",
      "         1.6378e-02, -1.5899e-03,  1.4846e-02,  1.4380e-02,  8.0166e-03,\n",
      "        -1.1439e-02, -3.1674e-02, -1.1409e-02, -1.4674e-02,  5.8528e-03,\n",
      "        -3.1841e-02, -1.1927e-02,  5.3183e-03,  1.3446e-02, -7.8146e-03,\n",
      "         5.3286e-03,  6.4290e-03,  7.8129e-03,  4.9263e-03,  1.8408e-02,\n",
      "         1.6440e-02,  0.0000e+00, -6.5404e-02, -6.7077e-02,  2.4792e-03,\n",
      "         6.3008e-03, -1.4775e-03])\n",
      "Q値の教師データ: tensor([ 1.2726e-02, -1.2425e-03, -3.8849e-03,  1.0592e-05, -1.7879e-02,\n",
      "         1.6215e-02, -1.5740e-03,  1.4697e-02,  1.4236e-02,  7.9364e-03,\n",
      "        -1.1324e-02, -3.1357e-02, -1.1295e-02, -1.4527e-02,  5.7943e-03,\n",
      "        -3.1523e-02, -1.1808e-02,  5.2651e-03,  1.3312e-02, -7.7364e-03,\n",
      "         5.2753e-03,  6.3647e-03,  7.7348e-03,  4.8771e-03,  1.8224e-02,\n",
      "         1.6276e-02, -1.0000e+00, -6.4750e-02, -6.6406e-02,  2.4544e-03,\n",
      "         6.2378e-03, -1.4627e-03])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0121,  0.0069,  0.0000, -0.0701,  0.0178, -0.0095,  0.0148,  0.0043,\n",
      "        -0.0016, -0.0653,  0.0137,  0.0184,  0.0123,  0.0056,  0.0000, -0.0158,\n",
      "        -0.0255, -0.0722,  0.0063, -0.0004,  0.0038,  0.0158, -0.0125,  0.0065,\n",
      "        -0.0672, -0.0017,  0.0071,  0.0036,  0.0004,  0.0134,  0.0033,  0.0036])\n",
      "Q値の教師データ: tensor([ 1.2023e-02,  6.8453e-03, -1.0000e+00, -6.9415e-02,  1.7573e-02,\n",
      "        -9.4073e-03,  1.4608e-02,  4.2631e-03, -1.6292e-03, -6.4610e-02,\n",
      "         1.3586e-02,  1.8209e-02,  1.2159e-02,  5.5083e-03, -1.0000e+00,\n",
      "        -1.5605e-02, -2.5243e-02, -7.1439e-02,  6.1905e-03, -4.3252e-04,\n",
      "         3.7518e-03,  1.5688e-02, -1.2402e-02,  6.3894e-03, -6.6494e-02,\n",
      "        -1.6449e-03,  7.0428e-03,  3.5614e-03,  4.3350e-04,  1.3292e-02,\n",
      "         3.2892e-03,  3.6116e-03])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([ 0.0037,  0.0040,  0.0151,  0.0052,  0.0083, -0.0022, -0.0158, -0.0123,\n",
      "         0.0055, -0.0037,  0.0036,  0.0174, -0.0482,  0.0121,  0.0123,  0.0042,\n",
      "        -0.0038,  0.0030,  0.0134,  0.0038,  0.0061,  0.0060,  0.0065,  0.0129,\n",
      "         0.0076, -0.0072, -0.0012, -0.0314, -0.0184, -0.0005,  0.0004, -0.0014])\n",
      "Q値の教師データ: tensor([ 0.0037,  0.0039,  0.0150,  0.0052,  0.0082, -0.0021, -0.0157, -0.0122,\n",
      "         0.0054, -0.0037,  0.0035,  0.0172, -0.0477,  0.0120,  0.0121,  0.0042,\n",
      "        -0.0037,  0.0030,  0.0133,  0.0037,  0.0060,  0.0060,  0.0065,  0.0128,\n",
      "         0.0075, -0.0072, -0.0012, -0.0310, -0.0182, -0.0005,  0.0004, -0.0014])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0037, -0.0103, -0.0443, -0.0066,  0.0181, -0.0139,  0.0173, -0.0014,\n",
      "        -0.0002,  0.0128, -0.0601,  0.0057, -0.0054, -0.0395,  0.0174, -0.0034,\n",
      "        -0.0098, -0.0320, -0.0112,  0.0000,  0.0197,  0.0039, -0.0013,  0.0151,\n",
      "         0.0149,  0.0144, -0.0097,  0.0196, -0.0444, -0.0548, -0.0026,  0.0164])\n",
      "Q値の教師データ: tensor([-3.6904e-03, -1.0165e-02, -4.3835e-02, -6.5567e-03,  1.7918e-02,\n",
      "        -1.3722e-02,  1.7103e-02, -1.4161e-03, -1.6381e-04,  1.2626e-02,\n",
      "        -5.9478e-02,  5.6699e-03, -5.3502e-03, -3.9109e-02,  1.7178e-02,\n",
      "        -3.3694e-03, -9.7113e-03, -3.1663e-02, -1.1067e-02, -1.0000e+00,\n",
      "         1.9470e-02,  3.8782e-03, -1.3133e-03,  1.4926e-02,  1.4782e-02,\n",
      "         1.4213e-02, -9.6068e-03,  1.9436e-02, -4.3993e-02, -5.4291e-02,\n",
      "        -2.5735e-03,  1.6282e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0028,  0.0110,  0.0113,  0.0135, -0.0256, -0.0443, -0.0054,  0.0088,\n",
      "         0.0000,  0.0091, -0.0014, -0.0083,  0.0075, -0.0290,  0.0141, -0.0677,\n",
      "         0.0149, -0.0026,  0.0062, -0.0068, -0.0314,  0.0000,  0.0000,  0.0000,\n",
      "         0.0070,  0.0138,  0.0107, -0.0568,  0.0000, -0.0015,  0.0043, -0.0609])\n",
      "Q値の教師データ: tensor([-0.0028,  0.0109,  0.0112,  0.0134, -0.0254, -0.0439, -0.0054,  0.0087,\n",
      "        -1.0000,  0.0090, -0.0014, -0.0082,  0.0074, -0.0287,  0.0140, -0.0671,\n",
      "         0.0147, -0.0026,  0.0061, -0.0067, -0.0311, -1.0000, -1.0000, -1.0000,\n",
      "         0.0070,  0.0136,  0.0106, -0.0562, -1.0000, -0.0015,  0.0042, -0.0603])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0079,  0.0071,  0.0150,  0.0000, -0.0008,  0.0175, -0.0036, -0.0048,\n",
      "        -0.0017,  0.0076,  0.0000, -0.0444, -0.0016, -0.0420, -0.0728, -0.0098,\n",
      "        -0.0521,  0.0067,  0.0166,  0.0075, -0.0761,  0.0072, -0.0123, -0.0446,\n",
      "        -0.0004, -0.0709, -0.0098,  0.0016, -0.0683, -0.0069, -0.0044,  0.0032])\n",
      "Q値の教師データ: tensor([ 7.8584e-03,  6.9941e-03,  1.4840e-02, -1.0000e+00, -8.1571e-04,\n",
      "         1.7331e-02, -3.5161e-03, -4.7830e-03, -1.6855e-03,  7.5085e-03,\n",
      "        -1.0000e+00, -4.3993e-02, -1.5838e-03, -4.1587e-02, -7.2037e-02,\n",
      "        -9.6707e-03, -5.1626e-02,  6.6171e-03,  1.6402e-02,  7.4119e-03,\n",
      "        -7.5351e-02,  7.1740e-03, -1.2209e-02, -4.4173e-02, -3.5247e-04,\n",
      "        -7.0207e-02, -9.6707e-03,  1.5717e-03, -6.7644e-02, -6.8518e-03,\n",
      "        -4.3180e-03,  3.2002e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0023,  0.0172, -0.0257,  0.0067,  0.0182, -0.0124,  0.0070,  0.0135,\n",
      "        -0.0002,  0.0135,  0.0026,  0.0038,  0.0000,  0.0197, -0.0094,  0.0070,\n",
      "         0.0133, -0.0003,  0.0005,  0.0174,  0.0128,  0.0075, -0.0016,  0.0079,\n",
      "         0.0141, -0.0447, -0.0012,  0.0080, -0.0005, -0.0679, -0.0069,  0.0075])\n",
      "Q値の教師データ: tensor([-2.2846e-03,  1.6997e-02, -2.5488e-02,  6.6034e-03,  1.8013e-02,\n",
      "        -1.2279e-02,  6.9174e-03,  1.3406e-02, -1.8262e-04,  1.3407e-02,\n",
      "         2.5994e-03,  3.8068e-03, -1.0000e+00,  1.9490e-02, -9.3124e-03,\n",
      "         6.9431e-03,  1.3168e-02, -3.3594e-04,  4.7101e-04,  1.7260e-02,\n",
      "         1.2720e-02,  7.3821e-03, -1.5621e-03,  7.8367e-03,  1.3966e-02,\n",
      "        -4.4291e-02, -1.2060e-03,  7.9360e-03, -4.6411e-04, -6.7252e-02,\n",
      "        -6.7824e-03,  7.3816e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0085, -0.0524, -0.0023, -0.0016,  0.0036,  0.0021,  0.0091, -0.0086,\n",
      "         0.0184,  0.0162,  0.0079,  0.0124,  0.0142, -0.0011, -0.0447, -0.0258,\n",
      "        -0.0005, -0.0072,  0.0048,  0.0009, -0.0422,  0.0171,  0.0179,  0.0016,\n",
      "        -0.0032,  0.0043,  0.0048,  0.0083,  0.0000,  0.0133, -0.0605, -0.0003])\n",
      "Q値の教師データ: tensor([ 8.3704e-03, -5.1842e-02, -2.3073e-03, -1.5863e-03,  3.5496e-03,\n",
      "         2.0993e-03,  9.0216e-03, -8.4705e-03,  1.8188e-02,  1.6061e-02,\n",
      "         7.7969e-03,  1.2290e-02,  1.4083e-02, -1.1136e-03, -4.4230e-02,\n",
      "        -2.5527e-02, -5.2144e-04, -7.1463e-03,  4.7466e-03,  9.1400e-04,\n",
      "        -4.1796e-02,  1.6945e-02,  1.7721e-02,  1.5447e-03, -3.1575e-03,\n",
      "         4.2708e-03,  4.7152e-03,  8.2404e-03, -1.0000e+00,  1.3210e-02,\n",
      "        -5.9935e-02, -2.6709e-04])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000,  0.0083, -0.0099, -0.0086,  0.0134, -0.0097, -0.0735, -0.0059,\n",
      "         0.0181, -0.0088, -0.0074,  0.0000, -0.0129,  0.0047,  0.0102,  0.0066,\n",
      "        -0.0017,  0.0085, -0.0050,  0.0185,  0.0030,  0.0143,  0.0081, -0.0714,\n",
      "         0.0049,  0.0130, -0.0030, -0.0524, -0.0685,  0.0129,  0.0053, -0.0022])\n",
      "Q値の教師データ: tensor([-1.0000,  0.0082, -0.0098, -0.0085,  0.0133, -0.0096, -0.0728, -0.0058,\n",
      "         0.0179, -0.0087, -0.0073, -1.0000, -0.0128,  0.0046,  0.0101,  0.0066,\n",
      "        -0.0017,  0.0084, -0.0049,  0.0183,  0.0030,  0.0142,  0.0080, -0.0707,\n",
      "         0.0049,  0.0129, -0.0030, -0.0519, -0.0678,  0.0128,  0.0053, -0.0022])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0054,  0.0111, -0.0045,  0.0036, -0.0015, -0.0007, -0.0073, -0.0066,\n",
      "         0.0085, -0.0669,  0.0102,  0.0038,  0.0086, -0.0074,  0.0080, -0.0086,\n",
      "         0.0038,  0.0185, -0.0032,  0.0119,  0.0042, -0.0099,  0.0127,  0.0137,\n",
      "         0.0074, -0.0067,  0.0000,  0.0071,  0.0047, -0.0448,  0.0170,  0.0070])\n",
      "Q値の教師データ: tensor([ 5.3810e-03,  1.1038e-02, -4.4398e-03,  3.5283e-03, -1.5098e-03,\n",
      "        -6.7730e-04, -7.2456e-03, -6.5265e-03,  8.4042e-03, -6.6220e-02,\n",
      "         1.0103e-02,  3.7787e-03,  8.5576e-03, -7.3017e-03,  7.8800e-03,\n",
      "        -8.5460e-03,  3.7927e-03,  1.8322e-02, -3.1743e-03,  1.1808e-02,\n",
      "         4.1419e-03, -9.8467e-03,  1.2597e-02,  1.3532e-02,  7.3471e-03,\n",
      "        -6.5861e-03, -1.0000e+00,  7.0236e-03,  4.6440e-03, -4.4346e-02,\n",
      "         1.6845e-02,  6.9750e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0172, -0.0080, -0.0030, -0.0114, -0.0129, -0.0016,  0.0055, -0.0004,\n",
      "         0.0068,  0.0080,  0.0185, -0.0022,  0.0000,  0.0137, -0.0028, -0.0105,\n",
      "         0.0173,  0.0021, -0.0095, -0.0073,  0.0155,  0.0075, -0.0003,  0.0032,\n",
      "        -0.0050,  0.0092, -0.0595, -0.0015, -0.0057,  0.0151,  0.0165, -0.0070])\n",
      "Q値の教師データ: tensor([ 1.7063e-02, -7.9504e-03, -2.9787e-03, -1.1302e-02, -1.2737e-02,\n",
      "        -1.5909e-03,  5.4876e-03, -3.9110e-04,  6.7178e-03,  7.8836e-03,\n",
      "         1.8282e-02, -2.2269e-03, -1.0000e+00,  1.3555e-02, -2.7609e-03,\n",
      "        -1.0371e-02,  1.7139e-02,  2.0299e-03, -9.3779e-03, -7.2349e-03,\n",
      "         1.5297e-02,  7.4250e-03, -2.9277e-04,  3.1499e-03, -4.9510e-03,\n",
      "         9.0597e-03, -5.8901e-02, -1.4495e-03, -5.6855e-03,  1.4990e-02,\n",
      "         1.6304e-02, -6.8849e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0029,  0.0046,  0.0031,  0.0000,  0.0071,  0.0039,  0.0052, -0.0010,\n",
      "         0.0084, -0.0596,  0.0142, -0.0324, -0.0394,  0.0090, -0.0186, -0.0288,\n",
      "        -0.0031, -0.0054, -0.0688,  0.0131,  0.0181,  0.0000, -0.0129, -0.0136,\n",
      "         0.0044, -0.0024, -0.0065,  0.0069,  0.0164,  0.0053, -0.0574, -0.0074])\n",
      "Q値の教師データ: tensor([-2.9089e-03,  4.5671e-03,  3.1007e-03, -1.0000e+00,  7.0144e-03,\n",
      "         3.8757e-03,  5.1434e-03, -9.6994e-04,  8.2861e-03, -5.8955e-02,\n",
      "         1.4030e-02, -3.2048e-02, -3.9039e-02,  8.9357e-03, -1.8422e-02,\n",
      "        -2.8556e-02, -3.0708e-03, -5.3048e-03, -6.8131e-02,  1.2931e-02,\n",
      "         1.7932e-02, -1.0000e+00, -1.2728e-02, -1.3491e-02,  4.3601e-03,\n",
      "        -2.3983e-03, -6.4483e-03,  6.7946e-03,  1.6195e-02,  5.2663e-03,\n",
      "        -5.6820e-02, -7.3125e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0085, -0.0015, -0.0005,  0.0000, -0.0031, -0.0004,  0.0000, -0.0023,\n",
      "         0.0171, -0.0084, -0.0686,  0.0056,  0.0070,  0.0090,  0.0150,  0.0142,\n",
      "        -0.0008, -0.0450, -0.0101,  0.0102,  0.0120,  0.0137,  0.0135, -0.0064,\n",
      "        -0.0425, -0.0017,  0.0000, -0.0009, -0.0095, -0.0014,  0.0052, -0.0661])\n",
      "Q値の教師データ: tensor([ 8.3921e-03, -1.4639e-03, -5.0744e-04, -1.0000e+00, -3.1103e-03,\n",
      "        -4.1736e-04, -1.0000e+00, -2.2990e-03,  1.6884e-02, -8.3536e-03,\n",
      "        -6.7876e-02,  5.5786e-03,  6.9770e-03,  8.8755e-03,  1.4834e-02,\n",
      "         1.4067e-02, -8.1695e-04, -4.4513e-02, -1.0009e-02,  1.0099e-02,\n",
      "         1.1834e-02,  1.3519e-02,  1.3341e-02, -6.3837e-03, -4.2062e-02,\n",
      "        -1.7160e-03, -1.0000e+00, -8.6543e-04, -9.4373e-03, -1.4117e-03,\n",
      "         5.1102e-03, -6.5473e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0005, -0.0008,  0.0060, -0.0489,  0.0058,  0.0078, -0.0025,  0.0139,\n",
      "        -0.0748,  0.0050,  0.0159,  0.0082, -0.0527,  0.0000, -0.0146, -0.0128,\n",
      "        -0.0033, -0.0450, -0.0092, -0.0056, -0.0029,  0.0169,  0.0070, -0.0008,\n",
      "        -0.0048, -0.0066, -0.0029, -0.0135,  0.0061,  0.0132, -0.0101,  0.0083])\n",
      "Q値の教師データ: tensor([-5.3967e-04, -8.3582e-04,  5.9436e-03, -4.8377e-02,  5.6980e-03,\n",
      "         7.7001e-03, -2.4723e-03,  1.3810e-02, -7.4022e-02,  4.9383e-03,\n",
      "         1.5719e-02,  8.1148e-03, -5.2160e-02, -1.0000e+00, -1.4410e-02,\n",
      "        -1.2719e-02, -3.2815e-03, -4.4561e-02, -9.1280e-03, -5.5828e-03,\n",
      "        -2.8458e-03,  1.6744e-02,  6.9372e-03, -8.3836e-04, -4.7740e-03,\n",
      "        -6.5360e-03, -2.9059e-03, -1.3352e-02,  6.0808e-03,  1.3021e-02,\n",
      "        -1.0002e-02,  8.2274e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 3.5043e-03,  7.1468e-03,  2.6272e-03, -1.3485e-02,  4.8222e-03,\n",
      "        -4.5048e-02, -3.4852e-03,  7.4400e-03, -5.9100e-02,  1.5578e-02,\n",
      "         2.7613e-03,  4.6327e-03, -1.0448e-02, -3.0126e-03,  1.5586e-03,\n",
      "         0.0000e+00,  1.6606e-02, -6.2427e-03, -4.7112e-05, -6.9120e-02,\n",
      "        -3.6874e-03, -7.2503e-02,  9.5679e-04, -7.2209e-02,  4.8177e-03,\n",
      "         8.6138e-03,  1.6018e-02,  1.4751e-02,  8.6285e-03,  8.8051e-03,\n",
      "        -6.5051e-03,  3.3965e-03])\n",
      "Q値の教師データ: tensor([ 3.4692e-03,  7.0753e-03,  2.6009e-03, -1.3351e-02,  4.7739e-03,\n",
      "        -4.4598e-02, -3.4504e-03,  7.3656e-03, -5.8509e-02,  1.5422e-02,\n",
      "         2.7337e-03,  4.5864e-03, -1.0344e-02, -2.9824e-03,  1.5430e-03,\n",
      "        -1.0000e+00,  1.6440e-02, -6.1803e-03, -4.6641e-05, -6.8429e-02,\n",
      "        -3.6505e-03, -7.1778e-02,  9.4722e-04, -7.1487e-02,  4.7696e-03,\n",
      "         8.5277e-03,  1.5858e-02,  1.4604e-02,  8.5422e-03,  8.7171e-03,\n",
      "        -6.4401e-03,  3.3626e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0038,  0.0011,  0.0046,  0.0092, -0.0007, -0.0028,  0.0047,  0.0017,\n",
      "        -0.0741, -0.0610, -0.0013, -0.0101,  0.0162,  0.0129, -0.0033,  0.0133,\n",
      "        -0.0065, -0.0027,  0.0021, -0.0138,  0.0123,  0.0066, -0.0095,  0.0159,\n",
      "        -0.0221, -0.0028,  0.0079,  0.0049,  0.0055, -0.0023,  0.0000, -0.0675])\n",
      "Q値の教師データ: tensor([-3.7996e-03,  1.0440e-03,  4.5639e-03,  9.1330e-03, -7.3104e-04,\n",
      "        -2.7782e-03,  4.6369e-03,  1.6460e-03, -7.3358e-02, -6.0416e-02,\n",
      "        -1.3036e-03, -1.0011e-02,  1.6076e-02,  1.2762e-02, -3.2850e-03,\n",
      "         1.3128e-02, -6.4453e-03, -2.6738e-03,  2.1203e-03, -1.3691e-02,\n",
      "         1.2202e-02,  6.5495e-03, -9.4408e-03,  1.5749e-02, -2.1880e-02,\n",
      "        -2.7396e-03,  7.7867e-03,  4.8786e-03,  5.3975e-03, -2.2306e-03,\n",
      "        -1.0000e+00, -6.6778e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0053, -0.0106, -0.0052,  0.0175,  0.0058,  0.0163,  0.0048,  0.0051,\n",
      "         0.0120,  0.0136,  0.0157,  0.0000, -0.0488,  0.0128,  0.0016,  0.0045,\n",
      "         0.0037, -0.0010, -0.0085, -0.0062, -0.0451, -0.0063, -0.0289,  0.0046,\n",
      "        -0.0062, -0.0026, -0.0064, -0.0598, -0.0033,  0.0077, -0.0144,  0.0132])\n",
      "Q値の教師データ: tensor([-0.0052, -0.0105, -0.0051,  0.0173,  0.0057,  0.0161,  0.0048,  0.0051,\n",
      "         0.0119,  0.0135,  0.0155, -1.0000, -0.0483,  0.0127,  0.0015,  0.0045,\n",
      "         0.0037, -0.0010, -0.0084, -0.0062, -0.0446, -0.0063, -0.0286,  0.0046,\n",
      "        -0.0062, -0.0026, -0.0064, -0.0592, -0.0033,  0.0076, -0.0142,  0.0130])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0028,  0.0122, -0.0256,  0.0024, -0.0314, -0.0013,  0.0013, -0.0025,\n",
      "         0.0174, -0.0005,  0.0059, -0.0665, -0.0021,  0.0000,  0.0057, -0.0060,\n",
      "        -0.0062, -0.0072, -0.0062,  0.0177,  0.0185,  0.0173,  0.0118, -0.0026,\n",
      "         0.0078,  0.0002, -0.0120, -0.0143,  0.0073,  0.0076,  0.0041,  0.0045])\n",
      "Q値の教師データ: tensor([ 2.7829e-03,  1.2033e-02, -2.5388e-02,  2.3783e-03, -3.1040e-02,\n",
      "        -1.3235e-03,  1.3193e-03, -2.4333e-03,  1.7202e-02, -5.3296e-04,\n",
      "         5.8315e-03, -6.5818e-02, -2.0306e-03, -1.0000e+00,  5.6431e-03,\n",
      "        -5.9170e-03, -6.1837e-03, -7.1436e-03, -6.1582e-03,  1.7481e-02,\n",
      "         1.8347e-02,  1.7174e-02,  1.1648e-02, -2.6166e-03,  7.7223e-03,\n",
      "         1.5342e-04, -1.1919e-02, -1.4124e-02,  7.2624e-03,  7.5707e-03,\n",
      "         4.0812e-03,  4.5038e-03])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000,  0.0134,  0.0063,  0.0068,  0.0149,  0.0039, -0.0489, -0.0316,\n",
      "        -0.0136, -0.0592, -0.0106, -0.0288, -0.0073,  0.0091,  0.0157, -0.0068,\n",
      "        -0.0126,  0.0000,  0.0121,  0.0064, -0.0066, -0.0451,  0.0008, -0.0746,\n",
      "        -0.0090,  0.0124, -0.0665,  0.0070, -0.0699, -0.0112,  0.0148, -0.0016])\n",
      "Q値の教師データ: tensor([-1.0000e+00,  1.3280e-02,  6.2220e-03,  6.7042e-03,  1.4729e-02,\n",
      "         3.8727e-03, -4.8386e-02, -3.1313e-02, -1.3466e-02, -5.8572e-02,\n",
      "        -1.0510e-02, -2.8482e-02, -7.2012e-03,  8.9690e-03,  1.5499e-02,\n",
      "        -6.7297e-03, -1.2472e-02, -1.0000e+00,  1.1944e-02,  6.3024e-03,\n",
      "        -6.4975e-03, -4.4604e-02,  7.8765e-04, -7.3805e-02, -8.9134e-03,\n",
      "         1.2244e-02, -6.5854e-02,  6.9642e-03, -6.9168e-02, -1.1086e-02,\n",
      "         1.4693e-02, -1.5498e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0171,  0.0003,  0.0054,  0.0061, -0.0256, -0.0126, -0.0013,  0.0030,\n",
      "        -0.0053, -0.0692, -0.0042, -0.0013,  0.0055,  0.0085,  0.0027,  0.0090,\n",
      "         0.0000, -0.0006, -0.0062, -0.0612, -0.0151, -0.0034, -0.0066,  0.0021,\n",
      "        -0.0122,  0.0130, -0.0593,  0.0072,  0.0000, -0.0137,  0.0047,  0.0175])\n",
      "Q値の教師データ: tensor([ 1.6968e-02,  3.4176e-04,  5.3033e-03,  6.0738e-03, -2.5352e-02,\n",
      "        -1.2504e-02, -1.2776e-03,  2.9345e-03, -5.2395e-03, -6.8552e-02,\n",
      "        -4.1986e-03, -1.3306e-03,  5.4547e-03,  8.3888e-03,  2.6425e-03,\n",
      "         8.9046e-03, -1.0000e+00, -6.2589e-04, -6.1703e-03, -6.0566e-02,\n",
      "        -1.4924e-02, -3.3502e-03, -6.5068e-03,  2.0974e-03, -1.2119e-02,\n",
      "         1.2855e-02, -5.8693e-02,  7.1136e-03, -1.0000e+00, -1.3564e-02,\n",
      "         4.6915e-03,  1.7317e-02])\n",
      "17 Episode: Finished after 46 steps：10試行の平均step数 = 17.0\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0., -1., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0058,  0.0010, -0.0294, -0.0095,  0.0127,  0.0043, -0.0107, -0.0062,\n",
      "        -0.0694,  0.0123,  0.0022, -0.0071,  0.0000, -0.0699,  0.0073,  0.0093,\n",
      "         0.0112,  0.0000, -0.0074,  0.0000, -0.0043, -0.0621,  0.0187,  0.0057,\n",
      "         0.0000,  0.0061,  0.0000,  0.0000, -0.0062, -0.0324, -0.0019,  0.0150])\n",
      "Q値の教師データ: tensor([ 5.7518e-03,  9.5412e-04, -2.9110e-02, -9.3713e-03,  1.2590e-02,\n",
      "         4.2436e-03, -1.0546e-02, -6.1620e-03, -6.8722e-02,  1.2131e-02,\n",
      "         2.1551e-03, -7.0031e-03, -1.0000e+00, -6.9214e-02,  7.2035e-03,\n",
      "         9.2080e-03,  1.1058e-02, -1.0000e+00, -7.3712e-03, -1.0000e+00,\n",
      "        -4.2732e-03, -6.1432e-02,  1.8535e-02,  5.6544e-03, -1.0000e+00,\n",
      "         6.0160e-03, -1.0000e+00, -1.0000e+00, -6.1495e-03, -3.2070e-02,\n",
      "        -1.8455e-03,  1.4864e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0011,  0.0041, -0.0064, -0.0028,  0.0155,  0.0041,  0.0000,  0.0063,\n",
      "        -0.0007, -0.0029, -0.0059,  0.0116, -0.0068, -0.0063,  0.0103,  0.0042,\n",
      "        -0.0076,  0.0061, -0.0059, -0.0063, -0.0024,  0.0170, -0.0133,  0.0066,\n",
      "         0.0057, -0.0080,  0.0005, -0.0147, -0.0060, -0.0045,  0.0174, -0.0015])\n",
      "Q値の教師データ: tensor([-1.0899e-03,  4.0542e-03, -6.3223e-03, -2.7297e-03,  1.5393e-02,\n",
      "         4.0862e-03, -1.0000e+00,  6.2327e-03, -6.8872e-04, -2.8914e-03,\n",
      "        -5.8279e-03,  1.1519e-02, -6.7253e-03, -6.2322e-03,  1.0148e-02,\n",
      "         4.1715e-03, -7.5668e-03,  6.0433e-03, -5.8401e-03, -6.2084e-03,\n",
      "        -2.3314e-03,  1.6850e-02, -1.3136e-02,  6.5273e-03,  5.5993e-03,\n",
      "        -7.8741e-03,  4.7268e-04, -1.4528e-02, -5.9046e-03, -4.4380e-03,\n",
      "         1.7183e-02, -1.5183e-03])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0701, -0.0112,  0.0073, -0.0036, -0.0494, -0.0013,  0.0015,\n",
      "        -0.0149, -0.0015,  0.0082, -0.0065,  0.0000,  0.0111, -0.0071, -0.0768,\n",
      "         0.0000,  0.0131,  0.0007,  0.0118, -0.0433,  0.0045,  0.0064, -0.0143,\n",
      "         0.0172, -0.0021, -0.0328, -0.0082,  0.0090, -0.0038,  0.0143, -0.0187])\n",
      "Q値の教師データ: tensor([-1.0000e+00, -6.9387e-02, -1.1124e-02,  7.1822e-03, -3.5279e-03,\n",
      "        -4.8926e-02, -1.3247e-03,  1.5289e-03, -1.4706e-02, -1.5091e-03,\n",
      "         8.1300e-03, -6.4105e-03, -1.0000e+00,  1.1002e-02, -7.0097e-03,\n",
      "        -7.5999e-02, -1.0000e+00,  1.2920e-02,  6.8372e-04,  1.1648e-02,\n",
      "        -4.2870e-02,  4.4941e-03,  6.3197e-03, -1.4199e-02,  1.7033e-02,\n",
      "        -2.0553e-03, -3.2497e-02, -8.1553e-03,  8.9217e-03, -3.7285e-03,\n",
      "         1.4170e-02, -1.8537e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0761, -0.0150, -0.0079,  0.0064,  0.0021,  0.0124,  0.0098,  0.0079,\n",
      "        -0.0115,  0.0098, -0.0066, -0.0067, -0.0023, -0.0026,  0.0154, -0.0019,\n",
      "        -0.0622,  0.0085,  0.0055,  0.0000, -0.0072, -0.0041,  0.0000, -0.0104,\n",
      "        -0.0189, -0.0057,  0.0000, -0.0147,  0.0038, -0.0461, -0.0072, -0.0096])\n",
      "Q値の教師データ: tensor([-0.0753, -0.0149, -0.0078,  0.0063,  0.0021,  0.0123,  0.0097,  0.0078,\n",
      "        -0.0114,  0.0097, -0.0065, -0.0066, -0.0023, -0.0026,  0.0152, -0.0019,\n",
      "        -0.0616,  0.0084,  0.0055, -1.0000, -0.0072, -0.0040, -1.0000, -0.0103,\n",
      "        -0.0187, -0.0057, -1.0000, -0.0145,  0.0038, -0.0457, -0.0071, -0.0095])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0124,  0.0000, -0.0153,  0.0022,  0.0123,  0.0156,  0.0017, -0.0063,\n",
      "         0.0057, -0.0060,  0.0150,  0.0000,  0.0101,  0.0120,  0.0098,  0.0050,\n",
      "        -0.0146, -0.0080,  0.0016, -0.0059,  0.0164,  0.0088, -0.0501,  0.0043,\n",
      "        -0.0035, -0.0749, -0.0010,  0.0000,  0.0000,  0.0013,  0.0096, -0.0105])\n",
      "Q値の教師データ: tensor([ 1.2235e-02, -1.0000e+00, -1.5170e-02,  2.1611e-03,  1.2187e-02,\n",
      "         1.5427e-02,  1.7041e-03, -6.2844e-03,  5.6205e-03, -5.9169e-03,\n",
      "         1.4825e-02, -1.0000e+00,  9.9634e-03,  1.1852e-02,  9.7165e-03,\n",
      "         4.9823e-03, -1.4412e-02, -7.9532e-03,  1.5840e-03, -5.8746e-03,\n",
      "         1.6196e-02,  8.6806e-03, -4.9621e-02,  4.2881e-03, -3.4438e-03,\n",
      "        -7.4144e-02, -9.5596e-04, -1.0000e+00, -1.0000e+00,  1.2459e-03,\n",
      "         9.5151e-03, -1.0419e-02])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([ 0.0054,  0.0061, -0.0081,  0.0058, -0.0618, -0.0087,  0.0057,  0.0035,\n",
      "         0.0098,  0.0053, -0.0413,  0.0062,  0.0114,  0.0066,  0.0044, -0.0024,\n",
      "        -0.0443, -0.0163, -0.0761, -0.0085,  0.0077,  0.0015, -0.0073, -0.0002,\n",
      "         0.0168, -0.0013,  0.0080, -0.0100, -0.0420, -0.0143,  0.0041,  0.0032])\n",
      "Q値の教師データ: tensor([ 0.0053,  0.0060, -0.0080,  0.0058, -0.0612, -0.0086,  0.0056,  0.0035,\n",
      "         0.0097,  0.0052, -0.0409,  0.0061,  0.0113,  0.0065,  0.0043, -0.0023,\n",
      "        -0.0438, -0.0161, -0.0754, -0.0084,  0.0076,  0.0015, -0.0072, -0.0002,\n",
      "         0.0166, -0.0013,  0.0080, -0.0099, -0.0416, -0.0141,  0.0041,  0.0032])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0054,  0.0160,  0.0115,  0.0000, -0.0507,  0.0079, -0.0122,  0.0098,\n",
      "        -0.0087, -0.0079,  0.0124, -0.0090, -0.0026,  0.0119,  0.0036, -0.0767,\n",
      "        -0.0424, -0.0070, -0.0019,  0.0163,  0.0141, -0.0076,  0.0031, -0.0035,\n",
      "         0.0012,  0.0115, -0.0333, -0.0016, -0.0040,  0.0090, -0.0025,  0.0096])\n",
      "Q値の教師データ: tensor([-0.0054,  0.0158,  0.0114, -1.0000, -0.0502,  0.0078, -0.0121,  0.0097,\n",
      "        -0.0086, -0.0079,  0.0123, -0.0089, -0.0026,  0.0118,  0.0036, -0.0760,\n",
      "        -0.0419, -0.0070, -0.0019,  0.0161,  0.0139, -0.0075,  0.0030, -0.0035,\n",
      "         0.0012,  0.0114, -0.0330, -0.0015, -0.0039,  0.0089, -0.0025,  0.0095])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0077, -0.0081,  0.0099, -0.0727, -0.0065, -0.0073,  0.0000,  0.0057,\n",
      "        -0.0619,  0.0148, -0.0121,  0.0024,  0.0031, -0.0344,  0.0006, -0.0799,\n",
      "         0.0026, -0.0639,  0.0013, -0.0014,  0.0137, -0.0061,  0.0113,  0.0023,\n",
      "         0.0015,  0.0084,  0.0059, -0.0079, -0.0029,  0.0135, -0.0015,  0.0165])\n",
      "Q値の教師データ: tensor([ 7.6481e-03, -7.9716e-03,  9.8277e-03, -7.1946e-02, -6.4045e-03,\n",
      "        -7.1904e-03, -1.0000e+00,  5.6574e-03, -6.1275e-02,  1.4700e-02,\n",
      "        -1.1981e-02,  2.3911e-03,  3.0644e-03, -3.4051e-02,  5.6864e-04,\n",
      "        -7.9104e-02,  2.5673e-03, -6.3260e-02,  1.3351e-03, -1.3950e-03,\n",
      "         1.3531e-02, -5.9899e-03,  1.1228e-02,  2.2647e-03,  1.4460e-03,\n",
      "         8.3390e-03,  5.8342e-03, -7.8417e-03, -2.8877e-03,  1.3413e-02,\n",
      "        -1.4654e-03,  1.6325e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0140, -0.0588,  0.0164,  0.0093, -0.0067,  0.0112, -0.0275,  0.0007,\n",
      "         0.0038,  0.0104, -0.0155,  0.0064, -0.0031,  0.0141, -0.0041,  0.0109,\n",
      "         0.0026,  0.0098, -0.0087,  0.0048,  0.0011,  0.0113, -0.0731,  0.0055,\n",
      "         0.0013,  0.0000,  0.0089, -0.0154,  0.0027,  0.0068,  0.0005, -0.0081])\n",
      "Q値の教師データ: tensor([-1.3868e-02, -5.8177e-02,  1.6220e-02,  9.1712e-03, -6.6682e-03,\n",
      "         1.1094e-02, -2.7266e-02,  7.2728e-04,  3.8068e-03,  1.0267e-02,\n",
      "        -1.5296e-02,  6.3271e-03, -3.0584e-03,  1.4000e-02, -4.0912e-03,\n",
      "         1.0761e-02,  2.5271e-03,  9.6775e-03, -8.5745e-03,  4.7260e-03,\n",
      "         1.1058e-03,  1.1228e-02, -7.2335e-02,  5.4870e-03,  1.2752e-03,\n",
      "        -1.0000e+00,  8.7697e-03, -1.5222e-02,  2.6686e-03,  6.6911e-03,\n",
      "         5.2361e-04, -8.0220e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0455,  0.0041,  0.0140,  0.0135,  0.0026, -0.0059,  0.0151,  0.0108,\n",
      "         0.0059,  0.0097, -0.0082,  0.0012, -0.0097, -0.0315, -0.0027, -0.0122,\n",
      "         0.0021, -0.0061,  0.0000, -0.0021, -0.0718, -0.0738, -0.0782, -0.0031,\n",
      "         0.0105,  0.0156, -0.0714, -0.0071,  0.0085, -0.0609,  0.0164,  0.0099])\n",
      "Q値の教師データ: tensor([-0.0450,  0.0040,  0.0139,  0.0133,  0.0025, -0.0059,  0.0150,  0.0106,\n",
      "         0.0059,  0.0096, -0.0082,  0.0012, -0.0096, -0.0312, -0.0026, -0.0121,\n",
      "         0.0021, -0.0060, -1.0000, -0.0021, -0.0711, -0.0730, -0.0774, -0.0031,\n",
      "         0.0104,  0.0155, -0.0707, -0.0070,  0.0084, -0.0602,  0.0162,  0.0098])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0164,  0.0107, -0.0077, -0.0130, -0.0245, -0.0647,  0.0003, -0.0634,\n",
      "         0.0046, -0.0029,  0.0011,  0.0111,  0.0011,  0.0040,  0.0066,  0.0073,\n",
      "        -0.0032,  0.0000,  0.0096, -0.0174, -0.0750,  0.0156, -0.0086,  0.0044,\n",
      "         0.0010, -0.0795,  0.0075, -0.0100, -0.0025, -0.0141,  0.0033, -0.0801])\n",
      "Q値の教師データ: tensor([-1.6244e-02,  1.0568e-02, -7.6466e-03, -1.2866e-02, -2.4270e-02,\n",
      "        -6.4081e-02,  2.8547e-04, -6.2734e-02,  4.5111e-03, -2.8491e-03,\n",
      "         1.1101e-03,  1.1035e-02,  1.0773e-03,  3.9412e-03,  6.5170e-03,\n",
      "         7.2564e-03, -3.1866e-03, -1.0000e+00,  9.4793e-03, -1.7232e-02,\n",
      "        -7.4213e-02,  1.5464e-02, -8.5117e-03,  4.3213e-03,  1.0309e-03,\n",
      "        -7.8660e-02,  7.4066e-03, -9.8933e-03, -2.4769e-03, -1.3943e-02,\n",
      "         3.2373e-03, -7.9267e-02])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0082,  0.0065, -0.0564,  0.0072, -0.0342,  0.0095, -0.0485, -0.0438,\n",
      "        -0.0033, -0.0160, -0.0099, -0.0435, -0.0087,  0.0067,  0.0006, -0.0595,\n",
      "        -0.0087,  0.0113, -0.0063, -0.0012,  0.0081,  0.0058,  0.0024,  0.0145,\n",
      "        -0.0011, -0.0034, -0.0109, -0.0485,  0.0035, -0.0024, -0.0073,  0.0008])\n",
      "Q値の教師データ: tensor([-0.0082,  0.0064, -0.0558,  0.0072, -0.0339,  0.0094, -0.0480, -0.0433,\n",
      "        -0.0032, -0.0159, -0.0098, -0.0431, -0.0086,  0.0066,  0.0005, -0.0589,\n",
      "        -0.0086,  0.0112, -0.0063, -0.0012,  0.0080,  0.0058,  0.0023,  0.0143,\n",
      "        -0.0010, -0.0033, -0.0107, -0.0480,  0.0035, -0.0024, -0.0072,  0.0008])\n",
      "reward_batch: tensor([ 0.,  0., -1., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1., -1.])\n",
      "次状態の最大Q値: tensor([-0.0127,  0.0161,  0.0000,  0.0000,  0.0133,  0.0154,  0.0000, -0.0048,\n",
      "        -0.0630, -0.0083, -0.0064, -0.0025, -0.0061, -0.0565,  0.0053,  0.0057,\n",
      "         0.0003, -0.0113,  0.0042,  0.0115,  0.0023,  0.0018,  0.0145, -0.0351,\n",
      "        -0.0080, -0.0440,  0.0032,  0.0058, -0.0437, -0.0087,  0.0000,  0.0000])\n",
      "Q値の教師データ: tensor([-1.2582e-02,  1.5964e-02, -1.0000e+00, -1.0000e+00,  1.3156e-02,\n",
      "         1.5220e-02, -1.0000e+00, -4.7773e-03, -6.2402e-02, -8.2620e-03,\n",
      "        -6.3715e-03, -2.4524e-03, -6.0751e-03, -5.5960e-02,  5.2390e-03,\n",
      "         5.5943e-03,  3.4301e-04, -1.1169e-02,  4.1298e-03,  1.1430e-02,\n",
      "         2.2552e-03,  1.7666e-03,  1.4339e-02, -3.4783e-02, -7.9406e-03,\n",
      "        -4.3520e-02,  3.1709e-03,  5.7307e-03, -4.3225e-02, -8.6184e-03,\n",
      "        -1.0000e+00, -1.0000e+00])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0003, -0.0859, -0.0345,  0.0053, -0.0161, -0.0167,  0.0117, -0.0057,\n",
      "         0.0104, -0.0164,  0.0075,  0.0000, -0.0617,  0.0021, -0.0037,  0.0097,\n",
      "         0.0000, -0.0177, -0.0107, -0.0744,  0.0055,  0.0102,  0.0140,  0.0075,\n",
      "        -0.0132,  0.0107,  0.0000, -0.0104, -0.0662,  0.0104,  0.0000,  0.0139])\n",
      "Q値の教師データ: tensor([ 2.5565e-04, -8.5000e-02, -3.4124e-02,  5.2504e-03, -1.5912e-02,\n",
      "        -1.6563e-02,  1.1566e-02, -5.6652e-03,  1.0284e-02, -1.6239e-02,\n",
      "         7.3894e-03, -1.0000e+00, -6.1059e-02,  2.1119e-03, -3.6387e-03,\n",
      "         9.6335e-03, -1.0000e+00, -1.7560e-02, -1.0588e-02, -7.3669e-02,\n",
      "         5.4456e-03,  1.0118e-02,  1.3822e-02,  7.3958e-03, -1.3033e-02,\n",
      "         1.0612e-02, -1.0000e+00, -1.0338e-02, -6.5537e-02,  1.0277e-02,\n",
      "        -1.0000e+00,  1.3807e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0094, -0.0081,  0.0159,  0.0145, -0.0003, -0.0046,  0.0006,  0.0109,\n",
      "        -0.0137, -0.0792, -0.0571,  0.0020, -0.0620,  0.0097,  0.0166,  0.0005,\n",
      "         0.0024,  0.0018, -0.0010, -0.0134, -0.0101, -0.0110,  0.0019, -0.0006,\n",
      "        -0.0040,  0.0014, -0.0749,  0.0000,  0.0109, -0.0073, -0.0037,  0.0014])\n",
      "Q値の教師データ: tensor([-9.2642e-03, -7.9985e-03,  1.5713e-02,  1.4393e-02, -2.4788e-04,\n",
      "        -4.5472e-03,  5.7310e-04,  1.0776e-02, -1.3604e-02, -7.8439e-02,\n",
      "        -5.6502e-02,  1.9368e-03, -6.1336e-02,  9.6165e-03,  1.6387e-02,\n",
      "         5.0819e-04,  2.3955e-03,  1.7598e-03, -1.0363e-03, -1.3217e-02,\n",
      "        -9.9887e-03, -1.0855e-02,  1.8524e-03, -5.5458e-04, -3.9244e-03,\n",
      "         1.3389e-03, -7.4169e-02, -1.0000e+00,  1.0822e-02, -7.2709e-03,\n",
      "        -3.6362e-03,  1.4283e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0072, -0.0638,  0.0060, -0.0320,  0.0097,  0.0000, -0.0527,  0.0044,\n",
      "        -0.0042, -0.0072, -0.0285,  0.0105, -0.0058, -0.0447, -0.0079,  0.0136,\n",
      "         0.0018, -0.0025, -0.0067, -0.0106,  0.0000,  0.0031, -0.0464, -0.0003,\n",
      "         0.0000,  0.0064,  0.0065, -0.0668, -0.0158, -0.0031,  0.0108, -0.0008])\n",
      "Q値の教師データ: tensor([ 7.1605e-03, -6.3206e-02,  5.9479e-03, -3.1712e-02,  9.5780e-03,\n",
      "        -1.0000e+00, -5.2186e-02,  4.3579e-03, -4.1443e-03, -7.1304e-03,\n",
      "        -2.8225e-02,  1.0431e-02, -5.7853e-03, -4.4291e-02, -7.8542e-03,\n",
      "         1.3493e-02,  1.7814e-03, -2.4957e-03, -6.6787e-03, -1.0474e-02,\n",
      "        -1.0000e+00,  3.0671e-03, -4.5975e-02, -2.6094e-04, -1.0000e+00,\n",
      "         6.3126e-03,  6.4775e-03, -6.6100e-02, -1.5689e-02, -3.0449e-03,\n",
      "         1.0680e-02, -7.6606e-04])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0081,  0.0065,  0.0104,  0.0000, -0.0102, -0.0009, -0.0140,  0.0157,\n",
      "        -0.0109,  0.0099,  0.0092, -0.0098, -0.0438,  0.0000, -0.0106, -0.0760,\n",
      "        -0.0466,  0.0068,  0.0100, -0.0823,  0.0079,  0.0075,  0.0029, -0.0768,\n",
      "         0.0104,  0.0030,  0.0017,  0.0048,  0.0007,  0.0078,  0.0038,  0.0064])\n",
      "Q値の教師データ: tensor([ 8.0194e-03,  6.4829e-03,  1.0340e-02, -1.0000e+00, -1.0146e-02,\n",
      "        -8.4830e-04, -1.3909e-02,  1.5503e-02, -1.0759e-02,  9.8482e-03,\n",
      "         9.1287e-03, -9.7014e-03, -4.3377e-02, -1.0000e+00, -1.0513e-02,\n",
      "        -7.5194e-02, -4.6173e-02,  6.7750e-03,  9.8626e-03, -8.1435e-02,\n",
      "         7.8342e-03,  7.4215e-03,  2.8415e-03, -7.6009e-02,  1.0344e-02,\n",
      "         2.9484e-03,  1.6398e-03,  4.7932e-03,  6.7718e-04,  7.6989e-03,\n",
      "         3.7734e-03,  6.3418e-03])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0113,  0.0108, -0.0112, -0.0044, -0.0086,  0.0000, -0.0095,\n",
      "         0.0101, -0.0006,  0.0095,  0.0081,  0.0046,  0.0084,  0.0093,  0.0016,\n",
      "        -0.0008, -0.0113,  0.0040, -0.0162,  0.0098,  0.0049, -0.0011, -0.0738,\n",
      "        -0.0075,  0.0102,  0.0026,  0.0000, -0.0034, -0.0067, -0.0449, -0.0098])\n",
      "Q値の教師データ: tensor([-1.0000e+00, -1.1152e-02,  1.0707e-02, -1.1099e-02, -4.3096e-03,\n",
      "        -8.5446e-03, -1.0000e+00, -9.4034e-03,  1.0014e-02, -6.1797e-04,\n",
      "         9.3573e-03,  7.9720e-03,  4.5478e-03,  8.3465e-03,  9.2433e-03,\n",
      "         1.6307e-03, -7.8378e-04, -1.1228e-02,  3.9745e-03, -1.6058e-02,\n",
      "         9.6784e-03,  4.8704e-03, -1.1193e-03, -7.3072e-02, -7.4302e-03,\n",
      "         1.0060e-02,  2.5646e-03, -1.0000e+00, -3.3926e-03, -6.6434e-03,\n",
      "        -4.4421e-02, -9.6982e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0050, -0.0440,  0.0095, -0.0094,  0.0125,  0.0000,  0.0093,  0.0054,\n",
      "         0.0092,  0.0106,  0.0147, -0.0645,  0.0049, -0.0123, -0.0116,  0.0036,\n",
      "        -0.0165, -0.0109, -0.0047,  0.0009, -0.0002,  0.0000,  0.0000, -0.0093,\n",
      "        -0.0158,  0.0000, -0.0326, -0.0173,  0.0076, -0.0118, -0.0010,  0.0097])\n",
      "Q値の教師データ: tensor([ 4.9753e-03, -4.3560e-02,  9.4082e-03, -9.2883e-03,  1.2331e-02,\n",
      "        -1.0000e+00,  9.2489e-03,  5.3667e-03,  9.0888e-03,  1.0495e-02,\n",
      "         1.4585e-02, -6.3899e-02,  4.8800e-03, -1.2162e-02, -1.1463e-02,\n",
      "         3.5223e-03, -1.6361e-02, -1.0821e-02, -4.6716e-03,  8.4406e-04,\n",
      "        -1.9079e-04, -1.0000e+00, -1.0000e+00, -9.2007e-03, -1.5687e-02,\n",
      "        -1.0000e+00, -3.2256e-02, -1.7120e-02,  7.4801e-03, -1.1712e-02,\n",
      "        -9.4481e-04,  9.5580e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0020,  0.0088,  0.0042,  0.0096,  0.0054, -0.0035, -0.0066, -0.0064,\n",
      "        -0.0031, -0.0185,  0.0000,  0.0030,  0.0046,  0.0060, -0.0030,  0.0135,\n",
      "        -0.0051, -0.0089,  0.0010,  0.0123, -0.0190,  0.0095, -0.0072, -0.0026,\n",
      "         0.0138, -0.0007, -0.0582,  0.0007, -0.0175,  0.0010, -0.0053,  0.0022])\n",
      "Q値の教師データ: tensor([ 1.9827e-03,  8.7013e-03,  4.1278e-03,  9.5422e-03,  5.3767e-03,\n",
      "        -3.4329e-03, -6.5404e-03, -6.3377e-03, -3.0791e-03, -1.8359e-02,\n",
      "        -1.0000e+00,  2.9601e-03,  4.5419e-03,  5.9021e-03, -2.9622e-03,\n",
      "         1.3330e-02, -5.0296e-03, -8.8299e-03,  9.7861e-04,  1.2218e-02,\n",
      "        -1.8806e-02,  9.4149e-03, -7.1747e-03, -2.5819e-03,  1.3674e-02,\n",
      "        -7.1553e-04, -5.7667e-02,  7.3004e-04, -1.7327e-02,  1.0057e-03,\n",
      "        -5.2369e-03,  2.1934e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0059, -0.0366, -0.0006,  0.0031, -0.0192,  0.0020,  0.0088, -0.0505,\n",
      "         0.0002,  0.0000, -0.0078, -0.0107,  0.0046,  0.0149,  0.0122, -0.0118,\n",
      "         0.0006,  0.0087,  0.0012, -0.0823,  0.0000,  0.0000, -0.0022, -0.0018,\n",
      "         0.0055, -0.0109, -0.0671, -0.0066,  0.0029,  0.0010,  0.0095, -0.0537])\n",
      "Q値の教師データ: tensor([ 5.8356e-03, -3.6231e-02, -5.6498e-04,  3.1089e-03, -1.9033e-02,\n",
      "         1.9424e-03,  8.7244e-03, -4.9954e-02,  2.1947e-04, -1.0000e+00,\n",
      "        -7.7677e-03, -1.0629e-02,  4.5179e-03,  1.4728e-02,  1.2115e-02,\n",
      "        -1.1713e-02,  5.6072e-04,  8.6318e-03,  1.2334e-03, -8.1455e-02,\n",
      "        -1.0000e+00, -1.0000e+00, -2.1357e-03, -1.8251e-03,  5.4604e-03,\n",
      "        -1.0824e-02, -6.6457e-02, -6.5233e-03,  2.8747e-03,  1.0317e-03,\n",
      "         9.3857e-03, -5.3170e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0007, -0.0839, -0.0100, -0.0112, -0.0673, -0.0366, -0.0119,  0.0006,\n",
      "         0.0030, -0.0019, -0.0269,  0.0027, -0.0129, -0.0009,  0.0096,  0.0017,\n",
      "        -0.0156,  0.0029,  0.0056, -0.0359,  0.0000,  0.0135, -0.0029, -0.0018,\n",
      "        -0.0005,  0.0000,  0.0109,  0.0086,  0.0021,  0.0000,  0.0012, -0.0171])\n",
      "Q値の教師データ: tensor([ 7.3986e-04, -8.3079e-02, -9.9339e-03, -1.1119e-02, -6.6676e-02,\n",
      "        -3.6264e-02, -1.1805e-02,  6.2258e-04,  2.9552e-03, -1.9009e-03,\n",
      "        -2.6584e-02,  2.7050e-03, -1.2760e-02, -9.2986e-04,  9.5419e-03,\n",
      "         1.7152e-03, -1.5488e-02,  2.9179e-03,  5.5490e-03, -3.5576e-02,\n",
      "        -1.0000e+00,  1.3316e-02, -2.9097e-03, -1.7383e-03, -5.1432e-04,\n",
      "        -1.0000e+00,  1.0765e-02,  8.4856e-03,  2.1172e-03, -1.0000e+00,\n",
      "         1.1858e-03, -1.6912e-02])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0190, -0.0040, -0.0089, -0.0113,  0.0009, -0.0751,  0.0052,\n",
      "        -0.0020,  0.0023,  0.0131,  0.0000,  0.0041, -0.0174,  0.0092, -0.0112,\n",
      "        -0.0132,  0.0008,  0.0013, -0.0833, -0.0024,  0.0040,  0.0115,  0.0026,\n",
      "        -0.0041, -0.0448,  0.0106, -0.0774,  0.0114,  0.0004,  0.0019,  0.0058])\n",
      "Q値の教師データ: tensor([-1.0000e+00, -1.8841e-02, -3.9886e-03, -8.8202e-03, -1.1172e-02,\n",
      "         8.5695e-04, -7.4323e-02,  5.1707e-03, -2.0011e-03,  2.2991e-03,\n",
      "         1.2940e-02, -1.0000e+00,  4.0579e-03, -1.7260e-02,  9.1249e-03,\n",
      "        -1.1099e-02, -1.3031e-02,  8.0191e-04,  1.2605e-03, -8.2490e-02,\n",
      "        -2.3745e-03,  3.9257e-03,  1.1362e-02,  2.5773e-03, -4.0190e-03,\n",
      "        -4.4397e-02,  1.0450e-02, -7.6577e-02,  1.1257e-02,  4.1249e-04,\n",
      "         1.8501e-03,  5.7837e-03])\n",
      "reward_batch: tensor([ 0., -1.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0135,  0.0000, -0.0754,  0.0000, -0.0191, -0.0067,  0.0000,  0.0032,\n",
      "        -0.0087, -0.0373,  0.0067,  0.0134, -0.0481, -0.0021, -0.0025, -0.0105,\n",
      "         0.0087, -0.0084, -0.0486, -0.0122,  0.0119,  0.0016,  0.0010,  0.0101,\n",
      "         0.0073, -0.0002,  0.0076, -0.0067,  0.0000,  0.0012,  0.0037,  0.0133])\n",
      "Q値の教師データ: tensor([-1.3379e-02, -1.0000e+00, -7.4684e-02, -1.0000e+00, -1.8944e-02,\n",
      "        -6.6296e-03, -1.0000e+00,  3.1502e-03, -8.6458e-03, -3.6930e-02,\n",
      "         6.6244e-03,  1.3308e-02, -4.7633e-02, -2.0648e-03, -2.4421e-03,\n",
      "        -1.0390e-02,  8.6352e-03, -8.3383e-03, -4.8120e-02, -1.2042e-02,\n",
      "         1.1804e-02,  1.5393e-03,  9.6923e-04,  1.0045e-02,  7.1777e-03,\n",
      "        -2.4334e-04,  7.4978e-03, -6.6114e-03, -1.0000e+00,  1.1411e-03,\n",
      "         3.6219e-03,  1.3172e-02])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0019, -0.0095,  0.0000, -0.0484, -0.0548,  0.0126,  0.0014, -0.0646,\n",
      "        -0.0085, -0.0139, -0.0010, -0.0180,  0.0124,  0.0091,  0.0077,  0.0035,\n",
      "        -0.0166, -0.0189, -0.0029, -0.0343,  0.0109, -0.0069, -0.0035,  0.0000,\n",
      "         0.0067, -0.0368, -0.0798, -0.0074, -0.0021, -0.0467, -0.0203,  0.0029])\n",
      "Q値の教師データ: tensor([ 0.0019, -0.0094, -1.0000, -0.0479, -0.0543,  0.0125,  0.0014, -0.0639,\n",
      "        -0.0084, -0.0137, -0.0010, -0.0179,  0.0123,  0.0090,  0.0076,  0.0034,\n",
      "        -0.0164, -0.0187, -0.0029, -0.0340,  0.0108, -0.0068, -0.0035, -1.0000,\n",
      "         0.0066, -0.0364, -0.0790, -0.0074, -0.0021, -0.0463, -0.0201,  0.0029])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0687,  0.0070,  0.0038,  0.0000, -0.0765, -0.0121, -0.0744, -0.0118,\n",
      "         0.0011, -0.0039, -0.0380, -0.0035,  0.0070,  0.0051,  0.0041,  0.0090,\n",
      "        -0.0487,  0.0070, -0.0470,  0.0105,  0.0110, -0.0142,  0.0107, -0.0141,\n",
      "        -0.0053,  0.0132,  0.0025,  0.0000,  0.0115, -0.0245, -0.0014,  0.0060])\n",
      "Q値の教師データ: tensor([-0.0680,  0.0070,  0.0038, -1.0000, -0.0757, -0.0120, -0.0736, -0.0117,\n",
      "         0.0010, -0.0038, -0.0376, -0.0034,  0.0069,  0.0050,  0.0041,  0.0089,\n",
      "        -0.0482,  0.0069, -0.0466,  0.0104,  0.0109, -0.0141,  0.0106, -0.0140,\n",
      "        -0.0052,  0.0131,  0.0024, -1.0000,  0.0114, -0.0243, -0.0014,  0.0060])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-9.7690e-03, -1.6980e-02, -7.6325e-03,  1.2674e-02, -5.0031e-05,\n",
      "         0.0000e+00, -2.6402e-03, -2.1068e-03,  0.0000e+00, -1.4811e-02,\n",
      "         3.8806e-03,  0.0000e+00,  2.7105e-03,  5.3100e-03,  0.0000e+00,\n",
      "        -9.2985e-03, -1.9069e-03,  8.4039e-03, -4.6037e-02, -1.3963e-02,\n",
      "        -8.6043e-02,  7.2826e-03, -4.4153e-03, -3.8293e-02, -1.9433e-02,\n",
      "         0.0000e+00,  9.7215e-03, -8.0378e-03,  2.9826e-03, -2.3914e-03,\n",
      "        -1.1535e-02, -6.0858e-04])\n",
      "Q値の教師データ: tensor([-9.6713e-03, -1.6810e-02, -7.5562e-03,  1.2548e-02, -4.9530e-05,\n",
      "        -1.0000e+00, -2.6138e-03, -2.0858e-03, -1.0000e+00, -1.4663e-02,\n",
      "         3.8418e-03, -1.0000e+00,  2.6834e-03,  5.2569e-03, -1.0000e+00,\n",
      "        -9.2055e-03, -1.8878e-03,  8.3198e-03, -4.5577e-02, -1.3824e-02,\n",
      "        -8.5182e-02,  7.2098e-03, -4.3711e-03, -3.7910e-02, -1.9239e-02,\n",
      "        -1.0000e+00,  9.6242e-03, -7.9574e-03,  2.9527e-03, -2.3675e-03,\n",
      "        -1.1419e-02, -6.0250e-04])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0056, -0.0042,  0.0100, -0.0045, -0.0216,  0.0057, -0.0118, -0.0017,\n",
      "        -0.0023, -0.0103,  0.0055,  0.0125, -0.0044, -0.0499,  0.0063,  0.0026,\n",
      "        -0.0100, -0.0017, -0.0150,  0.0059, -0.0092, -0.0041, -0.0376,  0.0017,\n",
      "        -0.0006,  0.0000, -0.0079, -0.0011,  0.0103,  0.0048, -0.0175,  0.0091])\n",
      "Q値の教師データ: tensor([ 5.5189e-03, -4.2040e-03,  9.9275e-03, -4.4257e-03, -2.1345e-02,\n",
      "         5.6777e-03, -1.1670e-02, -1.7199e-03, -2.3143e-03, -1.0158e-02,\n",
      "         5.4624e-03,  1.2386e-02, -4.3306e-03, -4.9379e-02,  6.2077e-03,\n",
      "         2.5262e-03, -9.8887e-03, -1.6835e-03, -1.4803e-02,  5.8354e-03,\n",
      "        -9.1519e-03, -4.0438e-03, -3.7200e-02,  1.6800e-03, -6.0868e-04,\n",
      "        -1.0000e+00, -7.8168e-03, -1.0657e-03,  1.0154e-02,  4.7772e-03,\n",
      "        -1.7283e-02,  9.0083e-03])\n",
      "18 Episode: Finished after 28 steps：10試行の平均step数 = 18.9\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0043, -0.0525,  0.0090,  0.0044,  0.0083, -0.0479, -0.0061, -0.0798,\n",
      "        -0.0002, -0.0081, -0.0258, -0.0027,  0.0000, -0.0019, -0.0887, -0.0773,\n",
      "         0.0031, -0.0472, -0.0776,  0.0078,  0.0048, -0.0392,  0.0000, -0.0139,\n",
      "        -0.0049, -0.0070, -0.0083, -0.0083, -0.0038, -0.0141, -0.0092, -0.0008])\n",
      "Q値の教師データ: tensor([-4.2325e-03, -5.1950e-02,  8.8933e-03,  4.3619e-03,  8.2446e-03,\n",
      "        -4.7430e-02, -6.0154e-03, -7.9012e-02, -2.3435e-04, -8.0182e-03,\n",
      "        -2.5513e-02, -2.6819e-03, -1.0000e+00, -1.9184e-03, -8.7804e-02,\n",
      "        -7.6518e-02,  3.0991e-03, -4.6760e-02, -7.6844e-02,  7.7679e-03,\n",
      "         4.7148e-03, -3.8762e-02, -1.0000e+00, -1.3741e-02, -4.8552e-03,\n",
      "        -6.8938e-03, -8.2347e-03, -8.1736e-03, -3.7630e-03, -1.3966e-02,\n",
      "        -9.0936e-03, -8.0681e-04])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 5.2267e-03, -8.8457e-03, -4.4804e-03,  4.0175e-03, -1.6102e-02,\n",
      "         8.4559e-03, -8.5132e-03, -8.3813e-03,  3.4454e-03, -1.4293e-02,\n",
      "        -3.6170e-03,  5.2555e-03,  9.9786e-03, -8.6313e-02, -8.6727e-04,\n",
      "        -1.1221e-02,  0.0000e+00, -9.1828e-03, -9.5156e-03, -3.2803e-03,\n",
      "        -4.4087e-03, -2.0145e-02,  1.0688e-02, -1.0957e-02, -1.2824e-02,\n",
      "        -8.0685e-02, -8.7132e-03, -1.3059e-02,  2.4765e-03, -4.1740e-03,\n",
      "        -7.9453e-05, -2.8942e-03])\n",
      "Q値の教師データ: tensor([ 5.1744e-03, -8.7572e-03, -4.4356e-03,  3.9773e-03, -1.5941e-02,\n",
      "         8.3713e-03, -8.4281e-03, -8.2975e-03,  3.4110e-03, -1.4150e-02,\n",
      "        -3.5809e-03,  5.2030e-03,  9.8788e-03, -8.5450e-02, -8.5860e-04,\n",
      "        -1.1109e-02, -1.0000e+00, -9.0910e-03, -9.4205e-03, -3.2475e-03,\n",
      "        -4.3646e-03, -1.9944e-02,  1.0581e-02, -1.0847e-02, -1.2696e-02,\n",
      "        -7.9878e-02, -8.6261e-03, -1.2929e-02,  2.4517e-03, -4.1322e-03,\n",
      "        -7.8658e-05, -2.8653e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0072, -0.0324, -0.0015, -0.0094,  0.0102, -0.0013, -0.0366, -0.0116,\n",
      "        -0.0392, -0.0090, -0.0083, -0.0022,  0.0066, -0.0007, -0.0019, -0.0027,\n",
      "         0.0000, -0.0165, -0.0094, -0.0202,  0.0000,  0.0024, -0.0210,  0.0012,\n",
      "        -0.0193, -0.0095,  0.0097, -0.0200,  0.0114, -0.0683,  0.0085, -0.0879])\n",
      "Q値の教師データ: tensor([ 7.1043e-03, -3.2077e-02, -1.4606e-03, -9.2767e-03,  1.0110e-02,\n",
      "        -1.2898e-03, -3.6259e-02, -1.1488e-02, -3.8761e-02, -8.8617e-03,\n",
      "        -8.2262e-03, -2.1928e-03,  6.4857e-03, -7.2581e-04, -1.8329e-03,\n",
      "        -2.6239e-03, -1.0000e+00, -1.6286e-02, -9.3107e-03, -2.0030e-02,\n",
      "        -1.0000e+00,  2.3813e-03, -2.0812e-02,  1.1576e-03, -1.9084e-02,\n",
      "        -9.3949e-03,  9.5833e-03, -1.9787e-02,  1.1241e-02, -6.7578e-02,\n",
      "         8.4527e-03, -8.7065e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-2.2811e-02,  4.2980e-03, -3.6931e-02, -2.2806e-03,  4.0297e-03,\n",
      "        -7.6412e-02,  3.3943e-03,  4.8684e-03, -1.1667e-02, -1.9083e-02,\n",
      "        -8.1468e-03,  0.0000e+00, -5.6979e-02, -7.0787e-02, -5.6936e-02,\n",
      "        -8.2588e-02, -4.8204e-03, -1.6963e-02, -2.0255e-02, -9.1358e-03,\n",
      "        -2.1138e-02, -5.0877e-03, -1.3934e-02, -1.1662e-02,  0.0000e+00,\n",
      "         6.6236e-03, -9.3772e-03,  5.8819e-05, -9.8605e-03, -1.7079e-03,\n",
      "         7.2004e-03, -1.2567e-02])\n",
      "Q値の教師データ: tensor([-2.2582e-02,  4.2551e-03, -3.6562e-02, -2.2578e-03,  3.9894e-03,\n",
      "        -7.5648e-02,  3.3603e-03,  4.8197e-03, -1.1551e-02, -1.8893e-02,\n",
      "        -8.0654e-03, -1.0000e+00, -5.6409e-02, -7.0079e-02, -5.6366e-02,\n",
      "        -8.1762e-02, -4.7722e-03, -1.6794e-02, -2.0052e-02, -9.0444e-03,\n",
      "        -2.0926e-02, -5.0368e-03, -1.3795e-02, -1.1546e-02, -1.0000e+00,\n",
      "         6.5574e-03, -9.2834e-03,  5.8230e-05, -9.7619e-03, -1.6908e-03,\n",
      "         7.1283e-03, -1.2442e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0068, -0.0054, -0.0169, -0.0129, -0.0050, -0.0099, -0.0539, -0.0031,\n",
      "        -0.0054, -0.0721, -0.0090,  0.0095,  0.0068, -0.0130, -0.0123, -0.0031,\n",
      "        -0.0091, -0.0063,  0.0063, -0.0076,  0.0079,  0.0000, -0.0065, -0.0164,\n",
      "         0.0020, -0.0139,  0.0033,  0.0021, -0.0036, -0.0021, -0.0010, -0.0396])\n",
      "Q値の教師データ: tensor([-6.7494e-03, -5.3565e-03, -1.6779e-02, -1.2793e-02, -4.9868e-03,\n",
      "        -9.7604e-03, -5.3362e-02, -3.0766e-03, -5.3583e-03, -7.1333e-02,\n",
      "        -8.8803e-03,  9.3770e-03,  6.7759e-03, -1.2892e-02, -1.2195e-02,\n",
      "        -3.0490e-03, -8.9737e-03, -6.2724e-03,  6.1905e-03, -7.5087e-03,\n",
      "         7.8697e-03, -1.0000e+00, -6.4041e-03, -1.6257e-02,  1.9367e-03,\n",
      "        -1.3735e-02,  3.2978e-03,  2.0908e-03, -3.5370e-03, -2.0831e-03,\n",
      "        -9.5881e-04, -3.9243e-02])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([ 0.0052, -0.0066, -0.0077, -0.0073,  0.0066, -0.0072,  0.0028,  0.0017,\n",
      "         0.0006, -0.0173, -0.0078,  0.0077, -0.0040, -0.0164,  0.0030, -0.0575,\n",
      "        -0.0104, -0.0240, -0.0243, -0.0176, -0.0153, -0.0047, -0.0009, -0.0027,\n",
      "        -0.0034,  0.0075, -0.0015, -0.0332, -0.0143, -0.0097, -0.0542, -0.0229])\n",
      "Q値の教師データ: tensor([ 0.0052, -0.0065, -0.0076, -0.0072,  0.0065, -0.0071,  0.0028,  0.0017,\n",
      "         0.0006, -0.0171, -0.0078,  0.0076, -0.0040, -0.0163,  0.0030, -0.0569,\n",
      "        -0.0102, -0.0237, -0.0240, -0.0174, -0.0151, -0.0047, -0.0009, -0.0027,\n",
      "        -0.0034,  0.0074, -0.0014, -0.0328, -0.0142, -0.0096, -0.0536, -0.0227])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0105,  0.0000, -0.0793,  0.0020, -0.0067, -0.0027, -0.0050, -0.0039,\n",
      "        -0.0030, -0.0402, -0.0104, -0.0145,  0.0040, -0.0150, -0.0005, -0.0156,\n",
      "        -0.0071, -0.0726, -0.0137, -0.0180,  0.0023, -0.0058, -0.0394, -0.0052,\n",
      "        -0.0238,  0.0029, -0.0773, -0.0334, -0.0200, -0.0131, -0.0578, -0.0164])\n",
      "Q値の教師データ: tensor([-1.0395e-02, -1.0000e+00, -7.8468e-02,  1.9333e-03, -6.6487e-03,\n",
      "        -2.6382e-03, -4.9228e-03, -3.8861e-03, -2.9446e-03, -3.9762e-02,\n",
      "        -1.0276e-02, -1.4322e-02,  3.9767e-03, -1.4853e-02, -4.4682e-04,\n",
      "        -1.5413e-02, -7.0541e-03, -7.1907e-02, -1.3536e-02, -1.7831e-02,\n",
      "         2.2424e-03, -5.6990e-03, -3.8959e-02, -5.1358e-03, -2.3529e-02,\n",
      "         2.9042e-03, -7.6555e-02, -3.3032e-02, -1.9762e-02, -1.3008e-02,\n",
      "        -5.7199e-02, -1.6226e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0179,  0.0029, -0.0096, -0.0146,  0.0000,  0.0023, -0.0092, -0.0023,\n",
      "        -0.0521,  0.0013, -0.0003,  0.0000, -0.0110, -0.0160, -0.0061,  0.0073,\n",
      "        -0.0898, -0.0147, -0.0110, -0.0029, -0.0213, -0.0079, -0.0324, -0.0120,\n",
      "         0.0039,  0.0067, -0.0031, -0.0108, -0.0028, -0.0697, -0.0061, -0.0236])\n",
      "Q値の教師データ: tensor([-1.7747e-02,  2.9195e-03, -9.4822e-03, -1.4453e-02, -1.0000e+00,\n",
      "         2.2971e-03, -9.0639e-03, -2.2962e-03, -5.1596e-02,  1.2909e-03,\n",
      "        -3.2366e-04, -1.0000e+00, -1.0869e-02, -1.5886e-02, -6.0738e-03,\n",
      "         7.1985e-03, -8.8946e-02, -1.4584e-02, -1.0917e-02, -2.8572e-03,\n",
      "        -2.1073e-02, -7.8597e-03, -3.2072e-02, -1.1842e-02,  3.8814e-03,\n",
      "         6.6440e-03, -3.0559e-03, -1.0682e-02, -2.7333e-03, -6.8963e-02,\n",
      "        -6.0244e-03, -2.3410e-02])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([ 4.7703e-03,  1.9808e-03, -7.0678e-02, -2.6338e-03,  8.7360e-03,\n",
      "         6.4939e-03,  8.1453e-03,  5.1648e-04, -1.8731e-03, -1.0028e-02,\n",
      "        -4.0992e-03, -1.8217e-02, -4.9896e-02, -1.6968e-02, -6.3678e-03,\n",
      "        -5.6136e-03, -2.2512e-02, -3.5792e-03,  6.9256e-03, -1.0405e-02,\n",
      "        -1.1603e-02,  3.6541e-05, -3.0962e-03,  6.9689e-03, -5.2978e-03,\n",
      "        -1.8667e-02, -1.1416e-02, -3.4407e-03,  1.2285e-03, -8.8192e-03,\n",
      "        -8.4908e-03, -1.2157e-02])\n",
      "Q値の教師データ: tensor([ 4.7226e-03,  1.9610e-03, -6.9972e-02, -2.6074e-03,  8.6487e-03,\n",
      "         6.4289e-03,  8.0638e-03,  5.1132e-04, -1.8544e-03, -9.9276e-03,\n",
      "        -4.0583e-03, -1.8035e-02, -4.9397e-02, -1.6798e-02, -6.3041e-03,\n",
      "        -5.5575e-03, -2.2287e-02, -3.5434e-03,  6.8564e-03, -1.0301e-02,\n",
      "        -1.1487e-02,  3.6176e-05, -3.0652e-03,  6.8992e-03, -5.2448e-03,\n",
      "        -1.8480e-02, -1.1302e-02, -3.4063e-03,  1.2162e-03, -8.7311e-03,\n",
      "        -8.4059e-03, -1.2036e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([-7.0863e-02, -8.6295e-03, -1.4461e-02, -6.0863e-03, -5.0052e-02,\n",
      "        -2.3835e-02,  7.3587e-03,  1.9208e-03, -2.3114e-02, -6.4478e-03,\n",
      "        -1.0463e-02, -6.3188e-03,  6.1131e-03, -2.1383e-03, -1.1485e-02,\n",
      "        -1.8023e-02, -1.7453e-02, -4.9585e-02, -7.9937e-02, -1.1316e-02,\n",
      "         6.2216e-03, -8.4404e-05, -4.8849e-03, -7.7875e-03, -1.8405e-02,\n",
      "         0.0000e+00, -1.1352e-02,  0.0000e+00, -8.2801e-02, -4.3152e-03,\n",
      "         0.0000e+00, -2.0228e-02])\n",
      "Q値の教師データ: tensor([-7.0154e-02, -8.5432e-03, -1.4316e-02, -6.0254e-03, -4.9552e-02,\n",
      "        -2.3597e-02,  7.2852e-03,  1.9016e-03, -2.2883e-02, -6.3833e-03,\n",
      "        -1.0358e-02, -6.2556e-03,  6.0519e-03, -2.1169e-03, -1.1370e-02,\n",
      "        -1.7843e-02, -1.7278e-02, -4.9089e-02, -7.9138e-02, -1.1203e-02,\n",
      "         6.1594e-03, -8.3560e-05, -4.8361e-03, -7.7097e-03, -1.8221e-02,\n",
      "        -1.0000e+00, -1.1239e-02, -1.0000e+00, -8.1973e-02, -4.2721e-03,\n",
      "        -1.0000e+00, -2.0026e-02])\n",
      "reward_batch: tensor([-1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0111, -0.0087,  0.0000, -0.0192, -0.0372, -0.0408, -0.0108,\n",
      "        -0.0928, -0.0638, -0.0086, -0.0051, -0.0196, -0.0080, -0.0015, -0.0212,\n",
      "        -0.0074,  0.0073, -0.0035, -0.0242, -0.0177, -0.0041,  0.0068, -0.0219,\n",
      "        -0.0505, -0.0712,  0.0000,  0.0000, -0.0036, -0.0208, -0.0066, -0.0040])\n",
      "Q値の教師データ: tensor([-1.0000, -0.0110, -0.0086, -1.0000, -0.0190, -0.0369, -0.0404, -0.0107,\n",
      "        -0.0919, -0.0631, -0.0085, -0.0050, -0.0194, -0.0080, -0.0015, -0.0210,\n",
      "        -0.0073,  0.0073, -0.0035, -0.0239, -0.0176, -0.0041,  0.0068, -0.0217,\n",
      "        -0.0500, -0.0705, -1.0000, -1.0000, -0.0036, -0.0206, -0.0065, -0.0039])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0214, -0.0506, -0.0053, -0.0091, -0.0172, -0.0120,  0.0050, -0.0521,\n",
      "        -0.0060, -0.0165, -0.0143, -0.0403, -0.0641, -0.0128, -0.0048, -0.0078,\n",
      "        -0.0552,  0.0054, -0.0028,  0.0064, -0.0080, -0.0979,  0.0003, -0.0053,\n",
      "        -0.0034,  0.0000, -0.0425, -0.0027, -0.0914,  0.0013, -0.0070, -0.0092])\n",
      "Q値の教師データ: tensor([-2.1161e-02, -5.0107e-02, -5.2149e-03, -8.9947e-03, -1.6985e-02,\n",
      "        -1.1923e-02,  4.9452e-03, -5.1565e-02, -5.9799e-03, -1.6367e-02,\n",
      "        -1.4153e-02, -3.9927e-02, -6.3469e-02, -1.2694e-02, -4.7772e-03,\n",
      "        -7.6735e-03, -5.4684e-02,  5.3734e-03, -2.8048e-03,  6.3802e-03,\n",
      "        -7.9695e-03, -9.6917e-02,  3.1592e-04, -5.2342e-03, -3.3309e-03,\n",
      "        -1.0000e+00, -4.2120e-02, -2.7161e-03, -9.0455e-02,  1.2693e-03,\n",
      "        -6.9685e-03, -9.1500e-03])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0066, -0.0094, -0.0224, -0.0243, -0.0076, -0.0070,  0.0002, -0.0110,\n",
      "        -0.0219, -0.0813, -0.0039, -0.0195, -0.0064, -0.0168, -0.0040, -0.0054,\n",
      "         0.0013, -0.0130, -0.0069, -0.0121, -0.0046, -0.0063,  0.0026, -0.0085,\n",
      "        -0.0157, -0.0217, -0.0027, -0.0984, -0.0231, -0.0088,  0.0061, -0.0267])\n",
      "Q値の教師データ: tensor([-0.0065, -0.0093, -0.0222, -0.0241, -0.0076, -0.0069,  0.0002, -0.0109,\n",
      "        -0.0217, -0.0805, -0.0039, -0.0193, -0.0064, -0.0167, -0.0039, -0.0054,\n",
      "         0.0013, -0.0128, -0.0068, -0.0120, -0.0046, -0.0063,  0.0026, -0.0084,\n",
      "        -0.0155, -0.0214, -0.0027, -0.0974, -0.0228, -0.0087,  0.0060, -0.0264])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0378, -0.0270, -0.0848, -0.0009,  0.0050, -0.0511, -0.0053, -0.0124,\n",
      "        -0.0198,  0.0020, -0.0203, -0.0032, -0.0096, -0.0097, -0.0215, -0.0022,\n",
      "        -0.0041, -0.0066, -0.0244, -0.0091, -0.0431,  0.0031, -0.0680, -0.0235,\n",
      "        -0.0040, -0.0145, -0.0841, -0.0075, -0.0084, -0.0594, -0.0159, -0.0083])\n",
      "Q値の教師データ: tensor([-0.0375, -0.0267, -0.0839, -0.0009,  0.0049, -0.0506, -0.0052, -0.0122,\n",
      "        -0.0196,  0.0020, -0.0201, -0.0032, -0.0095, -0.0096, -0.0212, -0.0022,\n",
      "        -0.0040, -0.0065, -0.0241, -0.0090, -0.0427,  0.0030, -0.0673, -0.0233,\n",
      "        -0.0040, -0.0144, -0.0832, -0.0075, -0.0083, -0.0588, -0.0157, -0.0082])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0037, -0.0244,  0.0049, -0.0128,  0.0069, -0.0207, -0.0097, -0.0200,\n",
      "        -0.0020,  0.0000, -0.0265, -0.0120, -0.0178, -0.0093, -0.0086, -0.0081,\n",
      "        -0.0091, -0.0417, -0.0115, -0.0092,  0.0038,  0.0069,  0.0000, -0.0254,\n",
      "         0.0020, -0.0433, -0.0010,  0.0000, -0.0914,  0.0000, -0.0052, -0.0005])\n",
      "Q値の教師データ: tensor([-3.6331e-03, -2.4156e-02,  4.8225e-03, -1.2685e-02,  6.7931e-03,\n",
      "        -2.0501e-02, -9.6461e-03, -1.9837e-02, -2.0257e-03, -1.0000e+00,\n",
      "        -2.6239e-02, -1.1846e-02, -1.7626e-02, -9.2105e-03, -8.5178e-03,\n",
      "        -8.0415e-03, -9.0356e-03, -4.1242e-02, -1.1357e-02, -9.0895e-03,\n",
      "         3.7251e-03,  6.8080e-03, -1.0000e+00, -2.5126e-02,  1.9413e-03,\n",
      "        -4.2869e-02, -9.8157e-04, -1.0000e+00, -9.0517e-02, -1.0000e+00,\n",
      "        -5.1762e-03, -4.6206e-04])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0081, -0.0052, -0.0118, -0.0130, -0.0045,  0.0000, -0.0032, -0.0717,\n",
      "        -0.0006, -0.0846, -0.0028, -0.0045, -0.0087, -0.0181, -0.0120, -0.0052,\n",
      "        -0.0181,  0.0002,  0.0007, -0.0598, -0.0127, -0.0515, -0.0203, -0.0042,\n",
      "        -0.0726, -0.0035, -0.0159,  0.0006, -0.0096,  0.0000, -0.0537,  0.0000])\n",
      "Q値の教師データ: tensor([-8.0495e-03, -5.1176e-03, -1.1724e-02, -1.2853e-02, -4.4181e-03,\n",
      "        -1.0000e+00, -3.1210e-03, -7.1016e-02, -6.2857e-04, -8.3777e-02,\n",
      "        -2.8099e-03, -4.4588e-03, -8.5932e-03, -1.7906e-02, -1.1914e-02,\n",
      "        -5.1139e-03, -1.7927e-02,  2.2996e-04,  6.5299e-04, -5.9237e-02,\n",
      "        -1.2597e-02, -5.1033e-02, -2.0077e-02, -4.1414e-03, -7.1829e-02,\n",
      "        -3.4881e-03, -1.5736e-02,  5.9268e-04, -9.5359e-03, -1.0000e+00,\n",
      "        -5.3125e-02, -1.0000e+00])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-7.2776e-02, -3.1618e-03, -2.6729e-02, -2.0990e-02, -2.3346e-02,\n",
      "        -3.6818e-03, -7.1951e-02,  3.3932e-03,  6.2691e-03,  6.2281e-03,\n",
      "        -9.6784e-03, -1.2962e-02,  0.0000e+00, -3.4892e-04, -9.9904e-04,\n",
      "        -6.6985e-03, -9.4345e-03, -2.0575e-02, -2.0786e-02, -1.7782e-02,\n",
      "        -6.5207e-03,  6.2745e-05,  5.4890e-03, -5.2700e-03,  0.0000e+00,\n",
      "        -1.9500e-02, -2.2935e-02, -5.6328e-02, -7.4499e-03, -5.5925e-03,\n",
      "        -4.0035e-03,  0.0000e+00])\n",
      "Q値の教師データ: tensor([-7.2048e-02, -3.1301e-03, -2.6461e-02, -2.0780e-02, -2.3112e-02,\n",
      "        -3.6450e-03, -7.1232e-02,  3.3593e-03,  6.2064e-03,  6.1658e-03,\n",
      "        -9.5816e-03, -1.2832e-02, -1.0000e+00, -3.4543e-04, -9.8905e-04,\n",
      "        -6.6315e-03, -9.3401e-03, -2.0369e-02, -2.0578e-02, -1.7604e-02,\n",
      "        -6.4555e-03,  6.2118e-05,  5.4341e-03, -5.2173e-03, -1.0000e+00,\n",
      "        -1.9305e-02, -2.2706e-02, -5.5765e-02, -7.3754e-03, -5.5366e-03,\n",
      "        -3.9634e-03, -1.0000e+00])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0082, -0.0004,  0.0021,  0.0000, -0.0252, -0.0089, -0.0144, -0.0282,\n",
      "        -0.0095, -0.0060, -0.0067, -0.0262,  0.0024,  0.0020, -0.0421, -0.0121,\n",
      "        -0.0236,  0.0000, -0.0094, -0.0019, -0.0083, -0.0064, -0.0104, -0.0562,\n",
      "         0.0000, -0.0045, -0.0022,  0.0050, -0.0532, -0.0104,  0.0000, -0.0104])\n",
      "Q値の教師データ: tensor([-8.1162e-03, -3.8815e-04,  2.0963e-03, -1.0000e+00, -2.4930e-02,\n",
      "        -8.8228e-03, -1.4305e-02, -2.7903e-02, -9.3681e-03, -5.9716e-03,\n",
      "        -6.6731e-03, -2.5952e-02,  2.3469e-03,  1.9638e-03, -4.1716e-02,\n",
      "        -1.2021e-02, -2.3399e-02, -1.0000e+00, -9.3470e-03, -1.8364e-03,\n",
      "        -8.2603e-03, -6.2922e-03, -1.0278e-02, -5.5622e-02, -1.0000e+00,\n",
      "        -4.4193e-03, -2.1529e-03,  4.9536e-03, -5.2654e-02, -1.0334e-02,\n",
      "        -1.0000e+00, -1.0329e-02])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0184, -0.0088,  0.0000, -0.0172, -0.0191,  0.0022, -0.0145, -0.0139,\n",
      "        -0.0135, -0.0009, -0.0107, -0.0190, -0.0112, -0.0188, -0.0080, -0.0167,\n",
      "         0.0000, -0.0517, -0.0108, -0.0037, -0.0002, -0.0140, -0.0089, -0.0123,\n",
      "        -0.0123, -0.0016,  0.0019, -0.0124, -0.0262, -0.0083, -0.0096,  0.0042])\n",
      "Q値の教師データ: tensor([-1.8212e-02, -8.7226e-03, -1.0000e+00, -1.7076e-02, -1.8864e-02,\n",
      "         2.1418e-03, -1.4378e-02, -1.3732e-02, -1.3371e-02, -9.3848e-04,\n",
      "        -1.0616e-02, -1.8803e-02, -1.1073e-02, -1.8657e-02, -7.9582e-03,\n",
      "        -1.6568e-02, -1.0000e+00, -5.1165e-02, -1.0697e-02, -3.7108e-03,\n",
      "        -2.2650e-04, -1.3819e-02, -8.8022e-03, -1.2142e-02, -1.2212e-02,\n",
      "        -1.6081e-03,  1.9099e-03, -1.2266e-02, -2.5958e-02, -8.2491e-03,\n",
      "        -9.5524e-03,  4.1226e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0027, -0.0427, -0.0753,  0.0039, -0.0126, -0.0102, -0.0105, -0.0061,\n",
      "        -0.0139, -0.0100,  0.0033, -0.0065, -0.0111, -0.0088, -0.0028, -0.0153,\n",
      "        -0.0194, -0.0663,  0.0000, -0.0236, -0.0134, -0.0394,  0.0000, -0.0237,\n",
      "        -0.0279, -0.0013, -0.0109, -0.0239, -0.0084, -0.0359, -0.0199, -0.0057])\n",
      "Q値の教師データ: tensor([ 0.0027, -0.0423, -0.0746,  0.0039, -0.0125, -0.0101, -0.0104, -0.0061,\n",
      "        -0.0138, -0.0099,  0.0032, -0.0064, -0.0110, -0.0088, -0.0028, -0.0152,\n",
      "        -0.0192, -0.0656, -1.0000, -0.0233, -0.0132, -0.0390, -1.0000, -0.0234,\n",
      "        -0.0276, -0.0013, -0.0108, -0.0237, -0.0083, -0.0355, -0.0197, -0.0056])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0361, -0.0103, -0.0091, -0.0197, -0.0065, -0.0765, -0.0155, -0.0143,\n",
      "        -0.0032, -0.0245,  0.0007, -0.0528, -0.0132, -0.0103, -0.0033,  0.0000,\n",
      "        -0.0113, -0.0197, -0.0246, -0.0093, -0.0549,  0.0034, -0.0121,  0.0023,\n",
      "        -0.0039, -0.0032, -0.0060, -0.0111, -0.0430, -0.0222, -0.0083, -0.0969])\n",
      "Q値の教師データ: tensor([-3.5764e-02, -1.0241e-02, -9.0453e-03, -1.9526e-02, -6.4097e-03,\n",
      "        -7.5762e-02, -1.5354e-02, -1.4149e-02, -3.1460e-03, -2.4255e-02,\n",
      "         6.5596e-04, -5.2270e-02, -1.3079e-02, -1.0173e-02, -3.2305e-03,\n",
      "        -1.0000e+00, -1.1198e-02, -1.9491e-02, -2.4362e-02, -9.1619e-03,\n",
      "        -5.4334e-02,  3.3178e-03, -1.1936e-02,  2.2787e-03, -3.8637e-03,\n",
      "        -3.1755e-03, -5.8920e-03, -1.0954e-02, -4.2577e-02, -2.1969e-02,\n",
      "        -8.2034e-03, -9.5962e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0030, -0.1023,  0.0037, -0.0077, -0.0178, -0.0143, -0.0245, -0.0013,\n",
      "        -0.0418, -0.0054, -0.0181, -0.0046, -0.0139,  0.0000,  0.0007, -0.0198,\n",
      "        -0.0137,  0.0044, -0.0012, -0.0098, -0.0028,  0.0023, -0.0233, -0.0180,\n",
      "        -0.0101, -0.0180, -0.0089, -0.0669, -0.0105, -0.0143, -0.0068, -0.0151])\n",
      "Q値の教師データ: tensor([-2.9866e-03, -1.0126e-01,  3.6970e-03, -7.5923e-03, -1.7631e-02,\n",
      "        -1.4175e-02, -2.4247e-02, -1.2854e-03, -4.1402e-02, -5.3771e-03,\n",
      "        -1.7967e-02, -4.5771e-03, -1.3799e-02, -1.0000e+00,  7.3704e-04,\n",
      "        -1.9649e-02, -1.3520e-02,  4.3976e-03, -1.2026e-03, -9.6669e-03,\n",
      "        -2.7416e-03,  2.2648e-03, -2.3087e-02, -1.7800e-02, -9.9727e-03,\n",
      "        -1.7783e-02, -8.8507e-03, -6.6254e-02, -1.0429e-02, -1.4179e-02,\n",
      "        -6.7365e-03, -1.4923e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0103, -0.0013, -0.0035,  0.0034, -0.0191, -0.0276,  0.0012, -0.0199,\n",
      "         0.0013, -0.0068, -0.0163, -0.0092,  0.0000, -0.0149, -0.0203, -0.0459,\n",
      "        -0.0242, -0.0289, -0.0879, -0.0288, -0.0020, -0.0201, -0.0014, -0.0052,\n",
      "        -0.0016, -0.0673, -0.0159,  0.0025, -0.0840, -0.0121, -0.0043,  0.0000])\n",
      "Q値の教師データ: tensor([-0.0102, -0.0013, -0.0035,  0.0034, -0.0189, -0.0273,  0.0012, -0.0197,\n",
      "         0.0013, -0.0067, -0.0161, -0.0091, -1.0000, -0.0148, -0.0201, -0.0455,\n",
      "        -0.0240, -0.0286, -0.0870, -0.0285, -0.0020, -0.0199, -0.0013, -0.0051,\n",
      "        -0.0016, -0.0666, -0.0157,  0.0025, -0.0832, -0.0119, -0.0042, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0124, -0.0252, -0.0751, -0.0279, -0.0068, -0.0206, -0.0180, -0.0069,\n",
      "        -0.0248, -0.0774, -0.0134, -0.0122, -0.0098, -0.0059,  0.0000, -0.0251,\n",
      "        -0.0848, -0.0119, -0.0259, -0.0127,  0.0000, -0.0580, -0.0097, -0.0227,\n",
      "         0.0028, -0.0031, -0.0290, -0.0116, -0.0150,  0.0000, -0.0148, -0.0199])\n",
      "Q値の教師データ: tensor([-0.0123, -0.0250, -0.0743, -0.0276, -0.0068, -0.0204, -0.0178, -0.0068,\n",
      "        -0.0246, -0.0766, -0.0132, -0.0121, -0.0097, -0.0059, -1.0000, -0.0248,\n",
      "        -0.0839, -0.0117, -0.0257, -0.0126, -1.0000, -0.0574, -0.0096, -0.0225,\n",
      "         0.0028, -0.0031, -0.0287, -0.0114, -0.0148, -1.0000, -0.0146, -0.0197])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0714, -0.0184, -0.0061, -0.0281, -0.0026, -0.0152, -0.0777, -0.0287,\n",
      "        -0.0150, -0.0009, -0.0121, -0.0020, -0.0208, -0.0956, -0.0056, -0.0118,\n",
      "        -0.0879, -0.0534, -0.0118, -0.0169, -0.0128, -0.0082, -0.0212, -0.0263,\n",
      "        -0.0060, -0.0102, -0.0153, -0.0002, -0.0146, -0.0185,  0.0034,  0.0018])\n",
      "Q値の教師データ: tensor([-0.0707, -0.0182, -0.0060, -0.0279, -0.0026, -0.0151, -0.0769, -0.0284,\n",
      "        -0.0149, -0.0009, -0.0120, -0.0019, -0.0206, -0.0947, -0.0055, -0.0116,\n",
      "        -0.0871, -0.0528, -0.0117, -0.0167, -0.0127, -0.0081, -0.0210, -0.0261,\n",
      "        -0.0060, -0.0101, -0.0152, -0.0002, -0.0144, -0.0183,  0.0033,  0.0017])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0023, -0.0065, -0.0284, -0.0148, -0.0011, -0.0053, -0.0042, -0.0081,\n",
      "         0.0000, -0.0132, -0.0074, -0.0187, -0.0212, -0.0039, -0.0150,  0.0014,\n",
      "        -0.0020,  0.0037, -0.0171, -0.0124, -0.0131, -0.0208, -0.0026, -0.0883,\n",
      "        -0.0682, -0.0063, -0.0240, -0.0212,  0.0016, -0.0004, -0.0155, -0.0095])\n",
      "Q値の教師データ: tensor([-2.2345e-03, -6.4267e-03, -2.8137e-02, -1.4653e-02, -1.1282e-03,\n",
      "        -5.2862e-03, -4.1836e-03, -8.0318e-03, -1.0000e+00, -1.3083e-02,\n",
      "        -7.3253e-03, -1.8477e-02, -2.0964e-02, -3.8389e-03, -1.4821e-02,\n",
      "         1.3474e-03, -2.0007e-03,  3.6961e-03, -1.6944e-02, -1.2260e-02,\n",
      "        -1.2938e-02, -2.0613e-02, -2.5558e-03, -8.7391e-02, -6.7495e-02,\n",
      "        -6.2261e-03, -2.3712e-02, -2.0952e-02,  1.5578e-03, -3.5351e-04,\n",
      "        -1.5345e-02, -9.3645e-03])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-2.5978e-02, -1.5081e-02, -8.5787e-02, -1.8006e-02, -2.8660e-02,\n",
      "        -5.4447e-02, -3.9299e-03,  0.0000e+00, -9.0787e-02, -1.5777e-02,\n",
      "        -8.8794e-02, -1.2613e-02, -2.6070e-02,  0.0000e+00, -2.4997e-03,\n",
      "        -9.9670e-03,  2.6306e-03, -5.8741e-02, -1.5966e-02,  1.8097e-03,\n",
      "         0.0000e+00, -7.5166e-02,  3.3914e-04, -2.8230e-03,  1.2656e-03,\n",
      "        -5.0886e-05, -1.1241e-02, -7.8340e-03, -1.2538e-02, -2.6614e-02,\n",
      "        -8.1119e-03, -4.4717e-03])\n",
      "Q値の教師データ: tensor([-2.5718e-02, -1.4931e-02, -8.4929e-02, -1.7826e-02, -2.8374e-02,\n",
      "        -5.3903e-02, -3.8906e-03, -1.0000e+00, -8.9879e-02, -1.5619e-02,\n",
      "        -8.7907e-02, -1.2487e-02, -2.5809e-02, -1.0000e+00, -2.4747e-03,\n",
      "        -9.8674e-03,  2.6043e-03, -5.8153e-02, -1.5806e-02,  1.7916e-03,\n",
      "        -1.0000e+00, -7.4414e-02,  3.3575e-04, -2.7948e-03,  1.2529e-03,\n",
      "        -5.0377e-05, -1.1129e-02, -7.7556e-03, -1.2413e-02, -2.6348e-02,\n",
      "        -8.0308e-03, -4.4270e-03])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0546, -0.0176, -0.0080, -0.0271, -0.0121, -0.0174, -0.0413, -0.0016,\n",
      "        -0.0027, -0.0105, -0.0167, -0.0033, -0.0527, -0.0105, -0.0157, -0.0144,\n",
      "         0.0004, -0.0128, -0.0130, -0.0431, -0.0029, -0.0977, -0.0042,  0.0011,\n",
      "        -0.0174, -0.0036, -0.0125, -0.0161, -0.0154, -0.0091, -0.0167, -0.0210])\n",
      "Q値の教師データ: tensor([-0.0541, -0.0174, -0.0080, -0.0268, -0.0120, -0.0172, -0.0409, -0.0016,\n",
      "        -0.0026, -0.0104, -0.0165, -0.0032, -0.0522, -0.0104, -0.0156, -0.0143,\n",
      "         0.0004, -0.0127, -0.0128, -0.0426, -0.0028, -0.0967, -0.0041,  0.0011,\n",
      "        -0.0172, -0.0036, -0.0124, -0.0160, -0.0152, -0.0090, -0.0165, -0.0208])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0159,  0.0004, -0.0191, -0.0156, -0.0012, -0.0200, -0.0128, -0.0079,\n",
      "        -0.0915, -0.0212, -0.0104, -0.0015, -0.0128, -0.0412, -0.0159, -0.0053,\n",
      "        -0.0161, -0.0959, -0.0127, -0.0166, -0.0269, -0.0266, -0.1056, -0.0111,\n",
      "        -0.0003, -0.0031, -0.0099, -0.0093, -0.0044, -0.0219, -0.0033, -0.0070])\n",
      "Q値の教師データ: tensor([-0.0157,  0.0004, -0.0189, -0.0154, -0.0012, -0.0198, -0.0127, -0.0078,\n",
      "        -0.0906, -0.0210, -0.0103, -0.0014, -0.0126, -0.0408, -0.0158, -0.0052,\n",
      "        -0.0160, -0.0950, -0.0125, -0.0164, -0.0267, -0.0264, -0.1046, -0.0110,\n",
      "        -0.0003, -0.0031, -0.0098, -0.0092, -0.0044, -0.0217, -0.0032, -0.0070])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0143, -0.0158, -0.0443,  0.0016, -0.0252, -0.0136,  0.0000, -0.0863,\n",
      "        -0.0133, -0.0090, -0.0125, -0.0014, -0.0263, -0.0047, -0.0070, -0.0069,\n",
      "         0.0000, -0.0145,  0.0028, -0.0229, -0.0092, -0.0193, -0.0057, -0.0976,\n",
      "        -0.0093, -0.0268, -0.0184,  0.0001, -0.0128, -0.0072, -0.0051, -0.0161])\n",
      "Q値の教師データ: tensor([-1.4120e-02, -1.5673e-02, -4.3873e-02,  1.5919e-03, -2.4973e-02,\n",
      "        -1.3493e-02, -1.0000e+00, -8.5428e-02, -1.3177e-02, -8.8638e-03,\n",
      "        -1.2415e-02, -1.3546e-03, -2.6041e-02, -4.6124e-03, -6.9640e-03,\n",
      "        -6.7961e-03, -1.0000e+00, -1.4340e-02,  2.8032e-03, -2.2672e-02,\n",
      "        -9.1327e-03, -1.9093e-02, -5.6074e-03, -9.6595e-02, -9.2365e-03,\n",
      "        -2.6567e-02, -1.8246e-02,  1.0257e-04, -1.2630e-02, -7.1363e-03,\n",
      "        -5.0538e-03, -1.5943e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0085, -0.0532, -0.0780, -0.0085, -0.0046, -0.0049, -0.0013, -0.0069,\n",
      "        -0.0137, -0.0224, -0.0072, -0.0093, -0.0314, -0.0312, -0.0030, -0.0024,\n",
      "        -0.0005,  0.0000, -0.0593, -0.0130, -0.0004, -0.0104, -0.0247, -0.0096,\n",
      "        -0.0022, -0.0084, -0.0034, -0.1063, -0.0015, -0.0140, -0.0060, -0.0116])\n",
      "Q値の教師データ: tensor([-8.4145e-03, -5.2652e-02, -7.7242e-02, -8.3761e-03, -4.5652e-03,\n",
      "        -4.8547e-03, -1.3280e-03, -6.8089e-03, -1.3555e-02, -2.2218e-02,\n",
      "        -7.1053e-03, -9.2528e-03, -3.1079e-02, -3.0899e-02, -3.0129e-03,\n",
      "        -2.3298e-03, -5.3391e-04, -1.0000e+00, -5.8733e-02, -1.2876e-02,\n",
      "        -4.3737e-04, -1.0307e-02, -2.4430e-02, -9.4861e-03, -2.1387e-03,\n",
      "        -8.3547e-03, -3.3727e-03, -1.0526e-01, -1.4383e-03, -1.3819e-02,\n",
      "        -5.9465e-03, -1.1467e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0096, -0.0989, -0.0185, -0.0446, -0.0037, -0.0125, -0.0397, -0.0050,\n",
      "        -0.0096, -0.0147, -0.0257, -0.0279, -0.0027, -0.0012, -0.0197, -0.0098,\n",
      "        -0.0147, -0.0061, -0.0001,  0.0000, -0.0064, -0.0148, -0.0226, -0.0009,\n",
      "         0.0000, -0.0142, -0.0791, -0.0141, -0.0174, -0.0982, -0.0190, -0.0005])\n",
      "Q値の教師データ: tensor([-9.5315e-03, -9.7958e-02, -1.8270e-02, -4.4187e-02, -3.6425e-03,\n",
      "        -1.2372e-02, -3.9339e-02, -4.9214e-03, -9.4600e-03, -1.4557e-02,\n",
      "        -2.5404e-02, -2.7670e-02, -2.6454e-03, -1.2303e-03, -1.9544e-02,\n",
      "        -9.6914e-03, -1.4545e-02, -6.0599e-03, -1.2844e-04, -1.0000e+00,\n",
      "        -6.3808e-03, -1.4663e-02, -2.2360e-02, -8.9419e-04, -1.0000e+00,\n",
      "        -1.4012e-02, -7.8337e-02, -1.3914e-02, -1.7179e-02, -9.7221e-02,\n",
      "        -1.8805e-02, -4.8838e-04])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-2.4011e-05, -1.4159e-02, -2.9955e-03, -1.7474e-02,  3.8861e-03,\n",
      "        -3.4460e-03, -4.6186e-03, -1.3300e-02, -7.1406e-03, -4.2170e-02,\n",
      "        -3.2077e-02, -1.6896e-02, -1.8372e-04, -3.1632e-02, -3.9521e-03,\n",
      "        -9.7863e-03, -9.6466e-02,  0.0000e+00, -1.6803e-02, -1.8780e-02,\n",
      "        -9.0186e-02, -2.7068e-02, -6.4203e-03, -3.1288e-02, -1.8338e-02,\n",
      "        -4.9013e-02, -1.6515e-02, -2.1000e-02, -1.0079e-02,  1.4039e-03,\n",
      "        -2.2540e-03, -3.2240e-02])\n",
      "Q値の教師データ: tensor([-2.3771e-05, -1.4018e-02, -2.9655e-03, -1.7299e-02,  3.8472e-03,\n",
      "        -3.4116e-03, -4.5724e-03, -1.3167e-02, -7.0692e-03, -4.1748e-02,\n",
      "        -3.1756e-02, -1.6727e-02, -1.8188e-04, -3.1316e-02, -3.9126e-03,\n",
      "        -9.6884e-03, -9.5502e-02, -1.0000e+00, -1.6635e-02, -1.8592e-02,\n",
      "        -8.9284e-02, -2.6797e-02, -6.3561e-03, -3.0975e-02, -1.8154e-02,\n",
      "        -4.8523e-02, -1.6350e-02, -2.0790e-02, -9.9787e-03,  1.3899e-03,\n",
      "        -2.2315e-03, -3.1918e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0457, -0.0092, -0.0005, -0.0600, -0.0115,  0.0022, -0.0320, -0.0174,\n",
      "         0.0000, -0.0254, -0.0052, -0.0772, -0.0552,  0.0000, -0.0086, -0.0145,\n",
      "        -0.0229, -0.0204, -0.0165, -0.0165, -0.0273, -0.0263, -0.0172, -0.0021,\n",
      "        -0.0233, -0.0204, -0.0176, -0.0140, -0.0019, -0.0106, -0.0058, -0.0235])\n",
      "Q値の教師データ: tensor([-4.5249e-02, -9.1156e-03, -5.0368e-04, -5.9403e-02, -1.1422e-02,\n",
      "         2.2214e-03, -3.1682e-02, -1.7263e-02, -1.0000e+00, -2.5108e-02,\n",
      "        -5.1124e-03, -7.6437e-02, -5.4625e-02, -1.0000e+00, -8.4705e-03,\n",
      "        -1.4337e-02, -2.2682e-02, -2.0227e-02, -1.6376e-02, -1.6337e-02,\n",
      "        -2.7068e-02, -2.5997e-02, -1.7068e-02, -2.0530e-03, -2.3044e-02,\n",
      "        -2.0167e-02, -1.7384e-02, -1.3853e-02, -1.9063e-03, -1.0501e-02,\n",
      "        -5.7202e-03, -2.3309e-02])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0136, -0.0242, -0.0537, -0.0158,  0.0000, -0.0181, -0.0322, -0.0174,\n",
      "        -0.0102, -0.0082, -0.0118, -0.0050, -0.0166, -0.0048, -0.0458, -0.0329,\n",
      "        -0.0279, -0.0171, -0.0877, -0.0082,  0.0004, -0.0292, -0.0018, -0.0029,\n",
      "        -0.0930, -0.0148, -0.0089, -0.0117, -0.0012,  0.0000, -0.0095, -0.0054])\n",
      "Q値の教師データ: tensor([-1.3426e-02, -2.3910e-02, -5.3185e-02, -1.5659e-02, -1.0000e+00,\n",
      "        -1.7942e-02, -3.1858e-02, -1.7203e-02, -1.0064e-02, -8.1640e-03,\n",
      "        -1.1720e-02, -4.9862e-03, -1.6474e-02, -4.7958e-03, -4.5371e-02,\n",
      "        -3.2541e-02, -2.7610e-02, -1.6950e-02, -8.6844e-02, -8.1620e-03,\n",
      "         4.1569e-04, -2.8876e-02, -1.8241e-03, -2.8713e-03, -9.2055e-02,\n",
      "        -1.4655e-02, -8.8370e-03, -1.1549e-02, -1.1440e-03, -1.0000e+00,\n",
      "        -9.4468e-03, -5.3285e-03])\n",
      "19 Episode: Finished after 35 steps：10試行の平均step数 = 21.1\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0197, -0.0209, -0.0150, -0.0162, -0.0126, -0.0126, -0.0062,\n",
      "        -0.0065, -0.0145, -0.0147, -0.0112, -0.0241, -0.0030, -0.0303, -0.0083,\n",
      "        -0.0264, -0.0105, -0.0114, -0.0051, -0.0907, -0.0065, -0.0130,  0.0000,\n",
      "        -0.0111, -0.0235, -0.0086, -0.0160, -0.0149, -0.0459, -0.0283, -0.0331])\n",
      "Q値の教師データ: tensor([-1.0000, -0.0195, -0.0207, -0.0148, -0.0161, -0.0125, -0.0125, -0.0062,\n",
      "        -0.0064, -0.0144, -0.0146, -0.0111, -0.0239, -0.0030, -0.0300, -0.0082,\n",
      "        -0.0261, -0.0104, -0.0113, -0.0050, -0.0897, -0.0065, -0.0129, -1.0000,\n",
      "        -0.0109, -0.0233, -0.0085, -0.0158, -0.0148, -0.0455, -0.0280, -0.0327])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0153, -0.0183, -0.0322, -0.0111, -0.0100, -0.0069, -0.0178, -0.0167,\n",
      "        -0.0087, -0.0276, -0.0176, -0.0338,  0.0000, -0.0570, -0.0212, -0.0121,\n",
      "        -0.0788,  0.0000, -0.0295, -0.0054, -0.0063, -0.0167, -0.0034, -0.0563,\n",
      "        -0.0184, -0.0045, -0.0911, -0.0142, -0.0107, -0.0460, -0.0938, -0.0188])\n",
      "Q値の教師データ: tensor([-0.0152, -0.0181, -0.0319, -0.0110, -0.0099, -0.0068, -0.0177, -0.0165,\n",
      "        -0.0086, -0.0274, -0.0174, -0.0334, -1.0000, -0.0564, -0.0210, -0.0120,\n",
      "        -0.0780, -1.0000, -0.0292, -0.0053, -0.0063, -0.0165, -0.0034, -0.0558,\n",
      "        -0.0182, -0.0045, -0.0902, -0.0141, -0.0106, -0.0456, -0.0929, -0.0186])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0650, -0.0066, -0.0055, -0.0132, -0.0060, -0.0046, -0.0158, -0.0186,\n",
      "        -0.0111, -0.0156, -0.0212, -0.0149, -0.0092, -0.0306,  0.0000, -0.0278,\n",
      "        -0.0110, -0.0607, -0.0098, -0.0206, -0.0776, -0.0072, -0.0331, -0.0704,\n",
      "        -0.0540, -0.0156, -0.0121, -0.0106,  0.0000, -0.0179, -0.0940,  0.0006])\n",
      "Q値の教師データ: tensor([-6.4379e-02, -6.5057e-03, -5.4903e-03, -1.3097e-02, -5.9636e-03,\n",
      "        -4.5887e-03, -1.5664e-02, -1.8381e-02, -1.0951e-02, -1.5421e-02,\n",
      "        -2.0979e-02, -1.4709e-02, -9.1538e-03, -3.0271e-02, -1.0000e+00,\n",
      "        -2.7530e-02, -1.0853e-02, -6.0087e-02, -9.6663e-03, -2.0409e-02,\n",
      "        -7.6848e-02, -7.1148e-03, -3.2814e-02, -6.9700e-02, -5.3463e-02,\n",
      "        -1.5474e-02, -1.2006e-02, -1.0489e-02, -1.0000e+00, -1.7721e-02,\n",
      "        -9.3082e-02,  5.4988e-04])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0337, -0.0098, -0.0077, -0.0320, -0.0455, -0.0181, -0.0103, -0.0185,\n",
      "        -0.0153, -0.0285, -0.0214, -0.0112, -0.0292, -0.0216, -0.0184,  0.0000,\n",
      "        -0.0150, -0.0044, -0.0244, -0.0054, -0.0221, -0.0096, -0.0032, -0.0857,\n",
      "        -0.0027, -0.0203, -0.0134, -0.0060, -0.0115,  0.0000, -0.0115, -0.0705])\n",
      "Q値の教師データ: tensor([-0.0333, -0.0097, -0.0076, -0.0317, -0.0450, -0.0179, -0.0102, -0.0183,\n",
      "        -0.0151, -0.0282, -0.0212, -0.0111, -0.0289, -0.0214, -0.0183, -1.0000,\n",
      "        -0.0148, -0.0044, -0.0242, -0.0054, -0.0219, -0.0095, -0.0032, -0.0849,\n",
      "        -0.0027, -0.0201, -0.0133, -0.0059, -0.0114, -1.0000, -0.0114, -0.0698])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0916, -0.0330, -0.0245, -0.0076, -0.0122, -0.0153, -0.0156, -0.0131,\n",
      "        -0.0034, -0.0024, -0.0162, -0.0288, -0.0218, -0.0211, -0.0045, -0.0157,\n",
      "        -0.0092,  0.0000, -0.0299,  0.0000, -0.0301, -0.0172, -0.0201, -0.0064,\n",
      "        -0.0082, -0.0939,  0.0000, -0.0111, -0.0042, -0.0456, -0.0090, -0.0079])\n",
      "Q値の教師データ: tensor([-0.0907, -0.0327, -0.0243, -0.0075, -0.0120, -0.0151, -0.0154, -0.0130,\n",
      "        -0.0034, -0.0024, -0.0160, -0.0285, -0.0216, -0.0209, -0.0044, -0.0155,\n",
      "        -0.0091, -1.0000, -0.0296, -1.0000, -0.0298, -0.0170, -0.0199, -0.0064,\n",
      "        -0.0081, -0.0929, -1.0000, -0.0110, -0.0041, -0.0451, -0.0089, -0.0078])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0985, -0.0163, -0.0252, -0.0160, -0.0256, -0.0184,  0.0030, -0.0159,\n",
      "        -0.0088, -0.0144, -0.0170, -0.0181, -0.0072, -0.0072, -0.0030, -0.0458,\n",
      "        -0.0273,  0.0000, -0.0174, -0.0093, -0.0093, -0.0315, -0.0066, -0.0278,\n",
      "        -0.0099, -0.0257, -0.0309, -0.0129, -0.0114, -0.0209, -0.0756, -0.0190])\n",
      "Q値の教師データ: tensor([-0.0975, -0.0161, -0.0249, -0.0158, -0.0253, -0.0182,  0.0030, -0.0157,\n",
      "        -0.0087, -0.0143, -0.0168, -0.0179, -0.0071, -0.0071, -0.0030, -0.0454,\n",
      "        -0.0271, -1.0000, -0.0172, -0.0092, -0.0092, -0.0312, -0.0065, -0.0275,\n",
      "        -0.0098, -0.0254, -0.0306, -0.0127, -0.0113, -0.0207, -0.0749, -0.0188])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0093, -0.0948, -0.0281,  0.0000, -0.0337,  0.0000,  0.0000, -0.0568,\n",
      "        -0.0029, -0.0140, -0.0058, -0.0059, -0.0276, -0.0345, -0.0127, -0.0191,\n",
      "        -0.0368, -0.0249, -0.0178, -0.0154, -0.0183, -0.0193, -0.0051, -0.0273,\n",
      "        -0.0307, -0.0252, -0.0305, -0.0432, -0.0106, -0.0152, -0.0281, -0.0793])\n",
      "Q値の教師データ: tensor([-0.0092, -0.0938, -0.0278, -1.0000, -0.0334, -1.0000, -1.0000, -0.0562,\n",
      "        -0.0029, -0.0138, -0.0057, -0.0058, -0.0274, -0.0341, -0.0126, -0.0189,\n",
      "        -0.0365, -0.0247, -0.0177, -0.0152, -0.0182, -0.0191, -0.0050, -0.0270,\n",
      "        -0.0304, -0.0249, -0.0302, -0.0428, -0.0105, -0.0151, -0.0279, -0.0785])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0469,  0.0000, -0.0462, -0.0082, -0.0188, -0.0131, -0.0320, -0.0295,\n",
      "        -0.0102, -0.0197,  0.0000, -0.0218,  0.0000, -0.0610, -0.0034, -0.0098,\n",
      "        -0.0185, -0.0435, -0.0255, -0.0952, -0.0207, -0.0063, -0.0198,  0.0000,\n",
      "        -0.0196, -0.0351, -0.0149, -0.0241, -0.0081, -0.1010, -0.0284,  0.0000])\n",
      "Q値の教師データ: tensor([-0.0465, -1.0000, -0.0458, -0.0081, -0.0186, -0.0130, -0.0316, -0.0292,\n",
      "        -0.0101, -0.0195, -1.0000, -0.0216, -1.0000, -0.0604, -0.0033, -0.0097,\n",
      "        -0.0183, -0.0431, -0.0253, -0.0943, -0.0205, -0.0063, -0.0196, -1.0000,\n",
      "        -0.0195, -0.0348, -0.0147, -0.0239, -0.0080, -0.1000, -0.0281, -1.0000])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0616, -0.0314, -0.0175, -0.0147, -0.0119, -0.0143, -0.0929, -0.0515,\n",
      "        -0.0268, -0.0090, -0.0063, -0.0341, -0.0222, -0.0087, -0.0082, -0.0071,\n",
      "        -0.0151, -0.0057, -0.0144, -0.0201, -0.0204, -0.0077, -0.0062, -0.0134,\n",
      "        -0.0173, -0.0953, -0.0340, -0.1105, -0.0104, -0.0159, -0.0216, -0.0307])\n",
      "Q値の教師データ: tensor([-0.0610, -0.0311, -0.0174, -0.0146, -0.0118, -0.0142, -0.0920, -0.0510,\n",
      "        -0.0266, -0.0089, -0.0062, -0.0338, -0.0220, -0.0086, -0.0081, -0.0070,\n",
      "        -0.0150, -0.0057, -0.0143, -0.0199, -0.0202, -0.0076, -0.0061, -0.0133,\n",
      "        -0.0171, -0.0944, -0.0336, -0.1094, -0.0103, -0.0157, -0.0214, -0.0304])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0214, -0.0233, -0.0188, -0.0378, -0.0133, -0.0248, -0.0102, -0.0345,\n",
      "        -0.0292, -0.0156, -0.0282, -0.0933, -0.0126, -0.0203, -0.0082, -0.0204,\n",
      "        -0.0204, -0.0144, -0.0173, -0.0211, -0.0069, -0.0320, -0.0815, -0.0351,\n",
      "        -0.0289, -0.0331, -0.0076, -0.0665, -0.0336, -0.0092, -0.0192, -0.0137])\n",
      "Q値の教師データ: tensor([-0.0212, -0.0231, -0.0186, -0.0374, -0.0132, -0.0246, -0.0101, -0.0341,\n",
      "        -0.0289, -0.0154, -0.0279, -0.0924, -0.0125, -0.0201, -0.0081, -0.0202,\n",
      "        -0.0202, -0.0143, -0.0171, -0.0209, -0.0068, -0.0316, -0.0806, -0.0348,\n",
      "        -0.0287, -0.0328, -0.0076, -0.0659, -0.0332, -0.0091, -0.0190, -0.0135])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0625, -0.0060,  0.0003, -0.0049, -0.0254, -0.0176, -0.0072,  0.0000,\n",
      "        -0.0196, -0.0139, -0.0173, -0.0115, -0.0168, -0.0262, -0.0323, -0.0961,\n",
      "        -0.0292, -0.0214, -0.0207, -0.0251, -0.0179, -0.0575, -0.0061, -0.0062,\n",
      "         0.0000,  0.0000, -0.0216, -0.0135, -0.0174, -0.0083, -0.0282, -0.0355])\n",
      "Q値の教師データ: tensor([-6.1875e-02, -5.9166e-03,  3.2911e-04, -4.8158e-03, -2.5122e-02,\n",
      "        -1.7448e-02, -7.0852e-03, -1.0000e+00, -1.9361e-02, -1.3740e-02,\n",
      "        -1.7089e-02, -1.1366e-02, -1.6678e-02, -2.5957e-02, -3.1973e-02,\n",
      "        -9.5148e-02, -2.8921e-02, -2.1231e-02, -2.0468e-02, -2.4800e-02,\n",
      "        -1.7682e-02, -5.6916e-02, -6.0094e-03, -6.1092e-03, -1.0000e+00,\n",
      "        -1.0000e+00, -2.1433e-02, -1.3402e-02, -1.7177e-02, -8.1907e-03,\n",
      "        -2.7916e-02, -3.5158e-02])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0143, -0.0144,  0.0000, -0.1009, -0.0184, -0.0121, -0.0091, -0.0183,\n",
      "        -0.0157, -0.0357, -0.0207,  0.0000,  0.0000, -0.0089, -0.0218, -0.0278,\n",
      "        -0.0198, -0.0099, -0.0012, -0.0237, -0.0065, -0.0302, -0.0096,  0.0000,\n",
      "        -0.0143, -0.0086, -0.0083, -0.0757, -0.0090, -0.0076, -0.0158, -0.0280])\n",
      "Q値の教師データ: tensor([-0.0142, -0.0143, -1.0000, -0.0999, -0.0182, -0.0120, -0.0090, -0.0181,\n",
      "        -0.0156, -0.0353, -0.0205, -1.0000, -1.0000, -0.0088, -0.0215, -0.0275,\n",
      "        -0.0196, -0.0098, -0.0012, -0.0234, -0.0064, -0.0299, -0.0095, -1.0000,\n",
      "        -0.0142, -0.0085, -0.0082, -0.0750, -0.0089, -0.0075, -0.0157, -0.0277])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0355, -0.0144, -0.0250,  0.0000, -0.0290, -0.0072, -0.0184, -0.0073,\n",
      "        -0.0273, -0.0150, -0.0353, -0.0091, -0.0297, -0.0155, -0.0059, -0.0100,\n",
      "        -0.0113, -0.0217,  0.0000, -0.0092, -0.0156,  0.0000, -0.0485, -0.0070,\n",
      "        -0.0179, -0.0110, -0.0224, -0.0201, -0.0135, -0.0353, -0.0206, -0.1014])\n",
      "Q値の教師データ: tensor([-0.0352, -0.0142, -0.0248, -1.0000, -0.0288, -0.0071, -0.0182, -0.0073,\n",
      "        -0.0270, -0.0149, -0.0349, -0.0090, -0.0294, -0.0154, -0.0058, -0.0099,\n",
      "        -0.0112, -0.0215, -1.0000, -0.0091, -0.0155, -1.0000, -0.0480, -0.0069,\n",
      "        -0.0178, -0.0109, -0.0222, -0.0199, -0.0134, -0.0349, -0.0203, -0.1004])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0218, -0.0184, -0.0167, -0.0305, -0.0345, -0.0191, -0.0946, -0.0093,\n",
      "        -0.0302, -0.0080, -0.0232, -0.0286, -0.0187, -0.0599, -0.0293, -0.0251,\n",
      "        -0.0322, -0.0226, -0.0129, -0.0100, -0.0126, -0.0326, -0.0124, -0.0252,\n",
      "        -0.0765, -0.0679, -0.1040, -0.0975, -0.0372, -0.0078, -0.0186, -0.0807])\n",
      "Q値の教師データ: tensor([-0.0215, -0.0182, -0.0166, -0.0302, -0.0341, -0.0189, -0.0936, -0.0092,\n",
      "        -0.0299, -0.0079, -0.0230, -0.0283, -0.0185, -0.0593, -0.0290, -0.0249,\n",
      "        -0.0319, -0.0223, -0.0128, -0.0099, -0.0125, -0.0323, -0.0123, -0.0249,\n",
      "        -0.0758, -0.0672, -0.1030, -0.0966, -0.0368, -0.0078, -0.0184, -0.0799])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0129,  0.0000, -0.0593, -0.0570, -0.0348, -0.0201, -0.0306, -0.0075,\n",
      "         0.0000, -0.0194,  0.0000, -0.0142, -0.0094, -0.0354, -0.0376, -0.0284,\n",
      "        -0.0811, -0.0980, -0.0834, -0.0160, -0.1021, -0.0081, -0.0682, -0.0803,\n",
      "        -0.0187, -0.0483, -0.0323, -0.0146, -0.0118, -0.0364, -0.0288, -0.0187])\n",
      "Q値の教師データ: tensor([-0.0128, -1.0000, -0.0587, -0.0565, -0.0344, -0.0199, -0.0303, -0.0074,\n",
      "        -1.0000, -0.0192, -1.0000, -0.0141, -0.0093, -0.0350, -0.0373, -0.0281,\n",
      "        -0.0803, -0.0970, -0.0826, -0.0158, -0.1011, -0.0080, -0.0675, -0.0795,\n",
      "        -0.0185, -0.0479, -0.0320, -0.0144, -0.0117, -0.0361, -0.0285, -0.0185])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0218, -0.0347, -0.0138, -0.0193, -0.0311, -0.0199, -0.0298,  0.0000,\n",
      "         0.0000, -0.1055, -0.0208, -0.0094, -0.0132, -0.0109, -0.0456, -0.0100,\n",
      "        -0.0307, -0.0359, -0.0740, -0.0097, -0.0640, -0.0397, -0.0968, -0.0189,\n",
      "        -0.0221, -0.0165, -0.0349, -0.0177, -0.0838, -0.0135, -0.0220, -0.0237])\n",
      "Q値の教師データ: tensor([-0.0216, -0.0344, -0.0137, -0.0191, -0.0308, -0.0197, -0.0295, -1.0000,\n",
      "        -1.0000, -0.1044, -0.0205, -0.0093, -0.0131, -0.0108, -0.0452, -0.0099,\n",
      "        -0.0304, -0.0355, -0.0732, -0.0096, -0.0634, -0.0393, -0.0958, -0.0187,\n",
      "        -0.0219, -0.0163, -0.0345, -0.0175, -0.0829, -0.0134, -0.0218, -0.0234])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0190, -0.0306, -0.0240, -0.0350, -0.0369, -0.0100, -0.0248, -0.0495,\n",
      "        -0.0381, -0.0250, -0.0106, -0.0096, -0.0085, -0.0304, -0.0282, -0.0928,\n",
      "        -0.0831, -0.0599, -0.0103, -0.0221, -0.0217, -0.0139, -0.0375, -0.0105,\n",
      "        -0.0228, -0.0101, -0.0089, -0.0017, -0.0361, -0.0106, -0.0193, -0.0257])\n",
      "Q値の教師データ: tensor([-0.0188, -0.0303, -0.0237, -0.0347, -0.0365, -0.0099, -0.0246, -0.0490,\n",
      "        -0.0377, -0.0247, -0.0105, -0.0095, -0.0084, -0.0301, -0.0279, -0.0919,\n",
      "        -0.0823, -0.0593, -0.0102, -0.0219, -0.0215, -0.0138, -0.0371, -0.0104,\n",
      "        -0.0226, -0.0100, -0.0088, -0.0017, -0.0357, -0.0105, -0.0191, -0.0254])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0380, -0.0228, -0.0691, -0.1061, -0.0251, -0.0103, -0.0097, -0.0150,\n",
      "        -0.0089, -0.0312, -0.0188, -0.0161, -0.0499, -0.0222, -0.0153, -0.0231,\n",
      "        -0.0161, -0.0316, -0.0245, -0.0322, -0.0252,  0.0000, -0.0189, -0.0194,\n",
      "        -0.0301, -0.0182, -0.0230, -0.0119, -0.0115, -0.0387, -0.1057, -0.0231])\n",
      "Q値の教師データ: tensor([-0.0377, -0.0225, -0.0684, -0.1051, -0.0249, -0.0102, -0.0096, -0.0149,\n",
      "        -0.0088, -0.0309, -0.0186, -0.0160, -0.0494, -0.0220, -0.0152, -0.0229,\n",
      "        -0.0159, -0.0313, -0.0243, -0.0319, -0.0250, -1.0000, -0.0188, -0.0192,\n",
      "        -0.0298, -0.0180, -0.0227, -0.0118, -0.0114, -0.0383, -0.1046, -0.0228])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0184, -0.0381, -0.0180, -0.0359,  0.0000, -0.0379, -0.0207, -0.0121,\n",
      "        -0.0181, -0.1093, -0.0109, -0.0204, -0.0227, -0.0239, -0.0112,  0.0000,\n",
      "        -0.0344, -0.0307, -0.0216, -0.0245, -0.0254, -0.0234, -0.0928, -0.0169,\n",
      "        -0.0102, -0.0314, -0.0192, -0.0183, -0.0195, -0.0082, -0.0229, -0.0104])\n",
      "Q値の教師データ: tensor([-0.0182, -0.0377, -0.0178, -0.0355, -1.0000, -0.0375, -0.0205, -0.0119,\n",
      "        -0.0179, -0.1082, -0.0108, -0.0202, -0.0225, -0.0236, -0.0111, -1.0000,\n",
      "        -0.0341, -0.0304, -0.0214, -0.0242, -0.0252, -0.0232, -0.0918, -0.0167,\n",
      "        -0.0101, -0.0311, -0.0191, -0.0181, -0.0193, -0.0081, -0.0227, -0.0103])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0364, -0.0144, -0.0228,  0.0000, -0.0236, -0.0250, -0.0305,\n",
      "        -0.0167, -0.0036, -0.0103, -0.0325, -0.0492, -0.0149, -0.0318, -0.0296,\n",
      "         0.0000, -0.0460, -0.0206, -0.0227, -0.0160, -0.0292, -0.0096, -0.0238,\n",
      "         0.0000, -0.0194, -0.0347, -0.0210, -0.0323, -0.0314, -0.0174, -0.0231])\n",
      "Q値の教師データ: tensor([-1.0000, -0.0361, -0.0143, -0.0226, -1.0000, -0.0234, -0.0248, -0.0302,\n",
      "        -0.0165, -0.0036, -0.0102, -0.0322, -0.0487, -0.0147, -0.0315, -0.0293,\n",
      "        -1.0000, -0.0455, -0.0204, -0.0225, -0.0159, -0.0289, -0.0095, -0.0236,\n",
      "        -1.0000, -0.0192, -0.0343, -0.0208, -0.0320, -0.0311, -0.0172, -0.0228])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0189, -0.1005, -0.0234, -0.0234, -0.0161, -0.0085, -0.0256, -0.0275,\n",
      "        -0.0159, -0.0190, -0.0298, -0.0171, -0.0194, -0.0322, -0.0584, -0.0246,\n",
      "        -0.0463, -0.0324, -0.0118, -0.0133, -0.0239, -0.0147, -0.0255, -0.0198,\n",
      "        -0.1069, -0.0204, -0.0379, -0.0111, -0.0243, -0.0123, -0.0164, -0.0122])\n",
      "Q値の教師データ: tensor([-0.0187, -0.0995, -0.0232, -0.0232, -0.0160, -0.0084, -0.0254, -0.0272,\n",
      "        -0.0157, -0.0188, -0.0295, -0.0169, -0.0192, -0.0319, -0.0579, -0.0243,\n",
      "        -0.0458, -0.0321, -0.0117, -0.0131, -0.0237, -0.0145, -0.0252, -0.0196,\n",
      "        -0.1058, -0.0202, -0.0376, -0.0110, -0.0241, -0.0122, -0.0163, -0.0121])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0327, -0.0122, -0.0176, -0.0122, -0.0243, -0.0331, -0.0472, -0.0191,\n",
      "        -0.0313, -0.0210, -0.0279, -0.1162, -0.0118, -0.0225, -0.0174, -0.0253,\n",
      "        -0.0143, -0.0139, -0.0199, -0.0210, -0.0116,  0.0000, -0.0150, -0.0180,\n",
      "        -0.0336, -0.0268, -0.0402,  0.0000, -0.0403, -0.0248, -0.0143, -0.0262])\n",
      "Q値の教師データ: tensor([-0.0323, -0.0121, -0.0175, -0.0120, -0.0241, -0.0328, -0.0467, -0.0189,\n",
      "        -0.0310, -0.0207, -0.0276, -0.1151, -0.0117, -0.0223, -0.0172, -0.0251,\n",
      "        -0.0142, -0.0138, -0.0197, -0.0207, -0.0115, -1.0000, -0.0148, -0.0178,\n",
      "        -0.0333, -0.0266, -0.0398, -1.0000, -0.0399, -0.0245, -0.0142, -0.0259])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "        -1.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0823, -0.0175,  0.0000, -0.0117, -0.0117, -0.0167, -0.0354, -0.1105,\n",
      "        -0.0408, -0.0201, -0.0203,  0.0000, -0.0612, -0.0289, -0.0110, -0.0255,\n",
      "        -0.0210, -0.0467, -0.0196, -0.0132, -0.1010, -0.0391, -0.0212, -0.0228,\n",
      "         0.0000, -0.0236, -0.0176, -0.0240,  0.0000, -0.0217,  0.0000, -0.0266])\n",
      "Q値の教師データ: tensor([-0.0814, -0.0173, -1.0000, -0.0116, -0.0116, -0.0165, -0.0351, -0.1094,\n",
      "        -0.0404, -0.0199, -0.0201, -1.0000, -0.0605, -0.0286, -0.0109, -0.0253,\n",
      "        -0.0208, -0.0462, -0.0194, -0.0130, -0.1000, -0.0387, -0.0210, -0.0226,\n",
      "        -1.0000, -0.0234, -0.0175, -0.0237, -1.0000, -0.0215, -1.0000, -0.0263])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0267, -0.0166,  0.0000, -0.0306, -0.0299, -0.0399, -0.1079, -0.0215,\n",
      "        -0.0176,  0.0000, -0.0201,  0.0000, -0.0160, -0.0142, -0.0193, -0.0199,\n",
      "         0.0000, -0.0384, -0.0623, -0.0231, -0.0258, -0.0614, -0.0099, -0.0250,\n",
      "        -0.0165, -0.0247, -0.0219, -0.0101,  0.0000, -0.0205, -0.0252, -0.0269])\n",
      "Q値の教師データ: tensor([-0.0264, -0.0164, -1.0000, -0.0303, -0.0296, -0.0395, -0.1068, -0.0213,\n",
      "        -0.0174, -1.0000, -0.0199, -1.0000, -0.0159, -0.0141, -0.0191, -0.0197,\n",
      "        -1.0000, -0.0380, -0.0616, -0.0229, -0.0256, -0.0608, -0.0098, -0.0247,\n",
      "        -0.0163, -0.0245, -0.0217, -0.0100, -1.0000, -0.0203, -0.0249, -0.0266])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0185, -0.0415, -0.0389, -0.0206, -0.0080, -0.0324, -0.0273, -0.0197,\n",
      "        -0.0126, -0.0183, -0.0310, -0.0626, -0.0658, -0.0235, -0.0383, -0.0324,\n",
      "        -0.0149, -0.0130, -0.0220, -0.0240, -0.0308, -0.0231, -0.0145, -0.0197,\n",
      "        -0.0247, -0.0157, -0.0142,  0.0000, -0.0665, -0.0303, -0.0205, -0.0218])\n",
      "Q値の教師データ: tensor([-0.0183, -0.0411, -0.0385, -0.0204, -0.0079, -0.0321, -0.0270, -0.0195,\n",
      "        -0.0125, -0.0181, -0.0307, -0.0619, -0.0652, -0.0233, -0.0379, -0.0321,\n",
      "        -0.0147, -0.0128, -0.0217, -0.0237, -0.0305, -0.0228, -0.0144, -0.0195,\n",
      "        -0.0244, -0.0155, -0.0140, -1.0000, -0.0658, -0.0300, -0.0203, -0.0216])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0980, -0.0475, -0.0140, -0.0094, -0.0229, -0.0105, -0.0160,\n",
      "        -0.0239, -0.0293, -0.0263, -0.0137, -0.0628, -0.0393, -0.0224, -0.0178,\n",
      "        -0.0388, -0.0211, -0.0619, -0.0385,  0.0000, -0.0097, -0.0226, -0.0185,\n",
      "        -0.1089, -0.0311, -0.0367, -0.0865, -0.0196, -0.0243, -0.0281, -0.0401])\n",
      "Q値の教師データ: tensor([-1.0000, -0.0970, -0.0470, -0.0138, -0.0093, -0.0227, -0.0104, -0.0158,\n",
      "        -0.0237, -0.0290, -0.0261, -0.0135, -0.0622, -0.0389, -0.0222, -0.0176,\n",
      "        -0.0384, -0.0209, -0.0613, -0.0381, -1.0000, -0.0096, -0.0224, -0.0183,\n",
      "        -0.1079, -0.0308, -0.0363, -0.0856, -0.0194, -0.0240, -0.0278, -0.0397])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0998, -0.0196, -0.0371, -0.0187, -0.0167,  0.0000, -0.0258,\n",
      "        -0.0263, -0.0357, -0.0338, -0.0228, -0.0140,  0.0000, -0.0391, -0.0276,\n",
      "        -0.0153, -0.0412, -0.0857, -0.0820, -0.0238, -0.0096, -0.1126, -0.0423,\n",
      "        -0.0189, -0.0165, -0.0323, -0.0315, -0.0134, -0.0236, -0.0233, -0.0150])\n",
      "Q値の教師データ: tensor([-1.0000, -0.0988, -0.0194, -0.0367, -0.0186, -0.0165, -1.0000, -0.0256,\n",
      "        -0.0261, -0.0354, -0.0335, -0.0225, -0.0138, -1.0000, -0.0387, -0.0273,\n",
      "        -0.0151, -0.0408, -0.0849, -0.0812, -0.0236, -0.0095, -0.1114, -0.0419,\n",
      "        -0.0187, -0.0163, -0.0319, -0.0312, -0.0133, -0.0234, -0.0230, -0.0148])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0210, -0.0335, -0.0929, -0.0421, -0.0336, -0.0262, -0.0281, -0.0230,\n",
      "        -0.0203, -0.0407, -0.0148, -0.0174, -0.0083, -0.0390, -0.0135,  0.0000,\n",
      "        -0.0319, -0.0712, -0.0236, -0.0269, -0.0402, -0.0170,  0.0000, -0.0192,\n",
      "        -0.0290, -0.0378, -0.0224, -0.0625, -0.0507, -0.0041, -0.0253, -0.0803])\n",
      "Q値の教師データ: tensor([-0.0208, -0.0331, -0.0920, -0.0417, -0.0332, -0.0259, -0.0278, -0.0227,\n",
      "        -0.0201, -0.0403, -0.0147, -0.0172, -0.0083, -0.0386, -0.0134, -1.0000,\n",
      "        -0.0315, -0.0705, -0.0234, -0.0266, -0.0398, -0.0168, -1.0000, -0.0190,\n",
      "        -0.0287, -0.0374, -0.0222, -0.0618, -0.0502, -0.0041, -0.0251, -0.0795])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.1201, -0.0994, -0.0152, -0.0233, -0.0282, -0.0145, -0.0224, -0.0256,\n",
      "        -0.0292, -0.0717, -0.0029, -0.0269, -0.0256, -0.0320, -0.0087, -0.0715,\n",
      "        -0.0176, -0.0140, -0.0250, -0.0234, -0.0275, -0.0326, -0.0257, -0.0232,\n",
      "        -0.0162, -0.0291, -0.0178, -0.0236, -0.0285, -0.0238, -0.0230, -0.0282])\n",
      "Q値の教師データ: tensor([-0.1189, -0.0984, -0.0150, -0.0231, -0.0279, -0.0144, -0.0222, -0.0254,\n",
      "        -0.0289, -0.0709, -0.0028, -0.0266, -0.0254, -0.0317, -0.0086, -0.0707,\n",
      "        -0.0174, -0.0138, -0.0247, -0.0232, -0.0273, -0.0323, -0.0254, -0.0229,\n",
      "        -0.0161, -0.0288, -0.0176, -0.0234, -0.0282, -0.0236, -0.0228, -0.0279])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0418, -0.0176, -0.0145, -0.0679, -0.0289, -0.0196, -0.0262, -0.0219,\n",
      "        -0.0296, -0.0421, -0.0290, -0.0143, -0.0258, -0.0244, -0.0258, -0.0322,\n",
      "         0.0000, -0.0224, -0.0239, -0.0192, -0.0400, -0.0273, -0.0172, -0.0259,\n",
      "        -0.0179, -0.0308, -0.0527, -0.0166, -0.0255, -0.0295, -0.0878, -0.0152])\n",
      "Q値の教師データ: tensor([-0.0413, -0.0175, -0.0143, -0.0672, -0.0286, -0.0194, -0.0260, -0.0217,\n",
      "        -0.0293, -0.0417, -0.0287, -0.0142, -0.0256, -0.0241, -0.0256, -0.0319,\n",
      "        -1.0000, -0.0222, -0.0236, -0.0190, -0.0396, -0.0270, -0.0170, -0.0256,\n",
      "        -0.0177, -0.0304, -0.0522, -0.0164, -0.0252, -0.0292, -0.0869, -0.0151])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0160, -0.0208, -0.0630, -0.0191, -0.0414, -0.0399, -0.0674, -0.0230,\n",
      "        -0.0354, -0.0264, -0.0240, -0.0355, -0.0195, -0.0188, -0.0157, -0.0441,\n",
      "        -0.0777, -0.0147,  0.0000, -0.0221, -0.0371, -0.0938, -0.0382, -0.1079,\n",
      "        -0.0426, -0.0399, -0.0264, -0.0153, -0.0299, -0.0198, -0.0262, -0.0131])\n",
      "Q値の教師データ: tensor([-0.0158, -0.0205, -0.0624, -0.0189, -0.0410, -0.0395, -0.0667, -0.0228,\n",
      "        -0.0351, -0.0261, -0.0237, -0.0352, -0.0193, -0.0186, -0.0155, -0.0437,\n",
      "        -0.0770, -0.0146, -1.0000, -0.0218, -0.0368, -0.0928, -0.0378, -0.1069,\n",
      "        -0.0422, -0.0395, -0.0261, -0.0151, -0.0296, -0.0196, -0.0260, -0.0129])\n",
      "20 Episode: Finished after 31 steps：10試行の平均step数 = 23.4\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0183, -0.0280, -0.0491, -0.0156, -0.0201, -0.0417, -0.0047,  0.0000,\n",
      "        -0.0195, -0.0310, -0.0632, -0.0167, -0.0779, -0.0251, -0.0227, -0.0148,\n",
      "        -0.0192, -0.0965, -0.0871, -0.0168, -0.0198, -0.0418, -0.0313, -0.0882,\n",
      "        -0.0261, -0.0187, -0.0437, -0.0229, -0.0196, -0.0261, -0.0211, -0.0219])\n",
      "Q値の教師データ: tensor([-0.0181, -0.0277, -0.0486, -0.0155, -0.0199, -0.0413, -0.0047, -1.0000,\n",
      "        -0.0193, -0.0307, -0.0625, -0.0166, -0.0771, -0.0249, -0.0225, -0.0147,\n",
      "        -0.0190, -0.0955, -0.0862, -0.0166, -0.0196, -0.0414, -0.0310, -0.0873,\n",
      "        -0.0258, -0.0185, -0.0433, -0.0226, -0.0194, -0.0258, -0.0209, -0.0217])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0247, -0.0328, -0.0440,  0.0000, -0.0197, -0.0222, -0.0232, -0.0281,\n",
      "        -0.0172, -0.0159, -0.0310, -0.0358, -0.0163, -0.0232, -0.0362, -0.0204,\n",
      "        -0.0162, -0.0034, -0.0531, -0.0133, -0.0194, -0.0247, -0.0282, -0.0523,\n",
      "        -0.0299, -0.0859, -0.1085, -0.0164, -0.0972, -0.1117, -0.0256, -0.0239])\n",
      "Q値の教師データ: tensor([-0.0245, -0.0324, -0.0436, -1.0000, -0.0195, -0.0220, -0.0229, -0.0278,\n",
      "        -0.0171, -0.0157, -0.0307, -0.0354, -0.0161, -0.0229, -0.0358, -0.0202,\n",
      "        -0.0160, -0.0034, -0.0526, -0.0132, -0.0192, -0.0244, -0.0279, -0.0518,\n",
      "        -0.0296, -0.0850, -0.1074, -0.0163, -0.0962, -0.1106, -0.0253, -0.0237])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0363, -0.0285, -0.0201, -0.0182, -0.0360,  0.0000,  0.0000, -0.1040,\n",
      "        -0.0265,  0.0000, -0.0339, -0.0160, -0.0443, -0.0164, -0.0249, -0.0171,\n",
      "        -0.0225, -0.0318, -0.0315, -0.0497, -0.0219, -0.0165, -0.0201, -0.0211,\n",
      "        -0.0301, -0.0187, -0.0188, -0.0259, -0.0368, -0.0260, -0.0244, -0.0209])\n",
      "Q値の教師データ: tensor([-0.0360, -0.0282, -0.0199, -0.0180, -0.0356, -1.0000, -1.0000, -0.1029,\n",
      "        -0.0262, -1.0000, -0.0336, -0.0158, -0.0438, -0.0163, -0.0246, -0.0170,\n",
      "        -0.0223, -0.0315, -0.0312, -0.0492, -0.0217, -0.0163, -0.0199, -0.0208,\n",
      "        -0.0298, -0.0185, -0.0186, -0.0257, -0.0365, -0.0258, -0.0242, -0.0207])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0308, -0.0361, -0.0877, -0.0188, -0.0432, -0.0348, -0.0174, -0.0397,\n",
      "        -0.0219, -0.0195, -0.0065, -0.0317, -0.0263, -0.0251, -0.0162, -0.0249,\n",
      "         0.0000, -0.0245, -0.0209, -0.0107, -0.0351, -0.0170, -0.0382, -0.0252,\n",
      "        -0.0173, -0.0222, -0.0277, -0.0271, -0.0218, -0.0534, -0.0331, -0.1094])\n",
      "Q値の教師データ: tensor([-0.0305, -0.0357, -0.0868, -0.0187, -0.0428, -0.0344, -0.0172, -0.0393,\n",
      "        -0.0217, -0.0193, -0.0064, -0.0314, -0.0261, -0.0249, -0.0160, -0.0246,\n",
      "        -1.0000, -0.0243, -0.0207, -0.0106, -0.0347, -0.0169, -0.0379, -0.0250,\n",
      "        -0.0171, -0.0220, -0.0274, -0.0269, -0.0216, -0.0529, -0.0328, -0.1083])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0214, -0.0183, -0.0217, -0.0095, -0.0393,  0.0000, -0.0171,\n",
      "        -0.0272, -0.0428,  0.0000, -0.0423, -0.0161, -0.0316, -0.0173, -0.0264,\n",
      "         0.0000, -0.0431, -0.0334, -0.0204, -0.0268, -0.0242,  0.0000, -0.0141,\n",
      "        -0.0584, -0.0236, -0.0165, -0.0454, -0.0181, -0.0215, -0.0358, -0.0215])\n",
      "Q値の教師データ: tensor([-1.0000, -0.0212, -0.0181, -0.0215, -0.0094, -0.0389, -1.0000, -0.0169,\n",
      "        -0.0270, -0.0424, -1.0000, -0.0419, -0.0159, -0.0313, -0.0171, -0.0261,\n",
      "        -1.0000, -0.0427, -0.0331, -0.0202, -0.0265, -0.0239, -1.0000, -0.0139,\n",
      "        -0.0578, -0.0234, -0.0163, -0.0449, -0.0179, -0.0213, -0.0354, -0.0213])\n",
      "reward_batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([ 0.0000, -0.0456, -0.0415, -0.0199, -0.0351, -0.0286, -0.0210, -0.0586,\n",
      "        -0.1232, -0.0201, -0.0170, -0.0285, -0.0822, -0.0345, -0.0257, -0.0245,\n",
      "        -0.0247, -0.0244, -0.0335, -0.0183, -0.0191, -0.0241, -0.0233, -0.0788,\n",
      "        -0.0232, -0.1130, -0.0271, -0.0247, -0.0308, -0.0285, -0.0231, -0.0338])\n",
      "Q値の教師データ: tensor([-1.0000, -0.0451, -0.0411, -0.0197, -0.0348, -0.0283, -0.0208, -0.0580,\n",
      "        -0.1220, -0.0199, -0.0168, -0.0282, -0.0814, -0.0342, -0.0254, -0.0243,\n",
      "        -0.0244, -0.0242, -0.0332, -0.0181, -0.0189, -0.0238, -0.0231, -0.0780,\n",
      "        -0.0230, -0.1118, -0.0269, -0.0245, -0.0305, -0.0283, -0.0229, -0.0334])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0254, -0.0262, -0.0292, -0.0250,  0.0000,  0.0000, -0.0316, -0.0281,\n",
      "        -0.0190, -0.0276, -0.1028, -0.0205, -0.0287, -0.0098, -0.0322, -0.0380,\n",
      "        -0.0732, -0.0245, -0.0241, -0.0234, -0.0272,  0.0000, -0.0221, -0.0195,\n",
      "        -0.0119, -0.0172, -0.0288, -0.0237, -0.0404, -0.0344, -0.0174, -0.0182])\n",
      "Q値の教師データ: tensor([-0.0251, -0.0259, -0.0289, -0.0247, -1.0000, -1.0000, -0.0313, -0.0278,\n",
      "        -0.0188, -0.0273, -0.1018, -0.0203, -0.0284, -0.0097, -0.0319, -0.0376,\n",
      "        -0.0725, -0.0242, -0.0238, -0.0232, -0.0270, -1.0000, -0.0219, -0.0193,\n",
      "        -0.0118, -0.0170, -0.0285, -0.0235, -0.0400, -0.0341, -0.0173, -0.0181])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0275, -0.0245, -0.0213, -0.0074, -0.0736, -0.0213,  0.0000, -0.0872,\n",
      "        -0.0847, -0.0647, -0.1053, -0.0623, -0.0257, -0.0416, -0.0420, -0.0330,\n",
      "        -0.0149, -0.0214, -0.0251, -0.0167, -0.0237, -0.0326, -0.0141, -0.0384,\n",
      "        -0.0428, -0.0735, -0.0423, -0.0305, -0.0368, -0.0189, -0.0379, -0.0260])\n",
      "Q値の教師データ: tensor([-0.0272, -0.0243, -0.0211, -0.0074, -0.0729, -0.0211, -1.0000, -0.0863,\n",
      "        -0.0839, -0.0641, -0.1042, -0.0617, -0.0255, -0.0412, -0.0416, -0.0326,\n",
      "        -0.0148, -0.0211, -0.0249, -0.0165, -0.0235, -0.0323, -0.0140, -0.0380,\n",
      "        -0.0424, -0.0727, -0.0419, -0.0302, -0.0364, -0.0187, -0.0375, -0.0258])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0285, -0.0462, -0.0278, -0.0077, -0.0371, -0.0300, -0.0548, -0.0187,\n",
      "        -0.0425, -0.0247, -0.1035, -0.0235, -0.0960, -0.0295, -0.0256,  0.0000,\n",
      "        -0.0408, -0.0199, -0.0103, -0.0248, -0.0344, -0.0329, -0.0337, -0.0263,\n",
      "        -0.0266, -0.0331, -0.0251, -0.0258, -0.0658, -0.0257, -0.0203, -0.0286])\n",
      "Q値の教師データ: tensor([-0.0282, -0.0458, -0.0275, -0.0076, -0.0367, -0.0297, -0.0542, -0.0185,\n",
      "        -0.0421, -0.0244, -0.1025, -0.0232, -0.0950, -0.0292, -0.0254, -1.0000,\n",
      "        -0.0404, -0.0197, -0.0102, -0.0246, -0.0341, -0.0326, -0.0334, -0.0260,\n",
      "        -0.0263, -0.0328, -0.0249, -0.0255, -0.0652, -0.0254, -0.0201, -0.0283])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0246, -0.0176, -0.0281, -0.0354,  0.0000, -0.0146, -0.0250, -0.0259,\n",
      "        -0.0547, -0.0382, -0.0202, -0.0283, -0.0052, -0.0197, -0.0508, -0.0239,\n",
      "        -0.0310, -0.0251,  0.0000, -0.0347, -0.0195, -0.0347, -0.0463, -0.0236,\n",
      "        -0.0258, -0.0877, -0.0293, -0.0435, -0.0351, -0.0294, -0.0216, -0.0377])\n",
      "Q値の教師データ: tensor([-0.0243, -0.0174, -0.0278, -0.0350, -1.0000, -0.0145, -0.0248, -0.0257,\n",
      "        -0.0541, -0.0378, -0.0200, -0.0280, -0.0051, -0.0195, -0.0503, -0.0237,\n",
      "        -0.0307, -0.0248, -1.0000, -0.0344, -0.0193, -0.0343, -0.0458, -0.0233,\n",
      "        -0.0256, -0.0868, -0.0290, -0.0431, -0.0347, -0.0291, -0.0214, -0.0373])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0268, -0.0205, -0.0473, -0.0534, -0.0391, -0.0990, -0.1148, -0.0296,\n",
      "        -0.0553, -0.0254, -0.0294, -0.0132, -0.0230, -0.0428, -0.0445, -0.0225,\n",
      "        -0.0447, -0.0179, -0.0187, -0.0254, -0.0376, -0.0250, -0.0112, -0.0269,\n",
      "        -0.0213, -0.0311, -0.0156, -0.0550, -0.0331, -0.0157, -0.0230, -0.0236])\n",
      "Q値の教師データ: tensor([-0.0265, -0.0203, -0.0468, -0.0528, -0.0387, -0.0980, -0.1137, -0.0293,\n",
      "        -0.0547, -0.0251, -0.0291, -0.0131, -0.0228, -0.0424, -0.0441, -0.0222,\n",
      "        -0.0443, -0.0177, -0.0185, -0.0252, -0.0372, -0.0248, -0.0111, -0.0266,\n",
      "        -0.0211, -0.0308, -0.0155, -0.0544, -0.0327, -0.0155, -0.0228, -0.0233])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0260, -0.0307, -0.0159, -0.0281, -0.0476, -0.0436, -0.0261, -0.0855,\n",
      "        -0.0244, -0.0332, -0.0208, -0.0288, -0.0306, -0.0231, -0.0383, -0.0469,\n",
      "        -0.0220, -0.0279, -0.0293, -0.0357, -0.0355, -0.0437, -0.0298, -0.0183,\n",
      "        -0.0471, -0.0245, -0.0211, -0.0426,  0.0000, -0.0433, -0.0276, -0.0122])\n",
      "Q値の教師データ: tensor([-0.0257, -0.0304, -0.0157, -0.0278, -0.0471, -0.0432, -0.0258, -0.0847,\n",
      "        -0.0242, -0.0329, -0.0206, -0.0285, -0.0303, -0.0228, -0.0379, -0.0464,\n",
      "        -0.0218, -0.0276, -0.0290, -0.0354, -0.0352, -0.0432, -0.0295, -0.0181,\n",
      "        -0.0466, -0.0243, -0.0209, -0.0422, -1.0000, -0.0428, -0.0274, -0.0121])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0401,  0.0000, -0.0857, -0.1258, -0.0455, -0.0392,  0.0000, -0.0357,\n",
      "        -0.0316, -0.0882,  0.0000, -0.0183, -0.0298, -0.0197, -0.0184, -0.0224,\n",
      "        -0.0405, -0.0544, -0.0230, -0.0435, -0.0220, -0.0448, -0.0191, -0.0632,\n",
      "        -0.0341, -0.0465, -0.0256, -0.0384, -0.0371, -0.0210, -0.0223, -0.0214])\n",
      "Q値の教師データ: tensor([-0.0397, -1.0000, -0.0849, -0.1246, -0.0451, -0.0388, -1.0000, -0.0353,\n",
      "        -0.0313, -0.0874, -1.0000, -0.0181, -0.0295, -0.0195, -0.0182, -0.0221,\n",
      "        -0.0401, -0.0539, -0.0227, -0.0431, -0.0217, -0.0444, -0.0189, -0.0626,\n",
      "        -0.0338, -0.0460, -0.0254, -0.0380, -0.0367, -0.0208, -0.0221, -0.0212])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0287, -0.0274, -0.0248, -0.0382, -0.0204, -0.0222,  0.0000, -0.0295,\n",
      "        -0.0257, -0.0250,  0.0000, -0.0228, -0.0340, -0.0327, -0.0454,  0.0000,\n",
      "        -0.0232, -0.0289, -0.0306, -0.0335,  0.0000, -0.0351, -0.0185, -0.0369,\n",
      "        -0.0262, -0.0225, -0.0333, -0.0168, -0.0247, -0.0211, -0.0258, -0.0480])\n",
      "Q値の教師データ: tensor([-0.0284, -0.0271, -0.0245, -0.0379, -0.0202, -0.0219, -1.0000, -0.0292,\n",
      "        -0.0255, -0.0247, -1.0000, -0.0226, -0.0336, -0.0324, -0.0450, -1.0000,\n",
      "        -0.0229, -0.0286, -0.0303, -0.0331, -1.0000, -0.0347, -0.0183, -0.0365,\n",
      "        -0.0259, -0.0223, -0.0329, -0.0166, -0.0245, -0.0209, -0.0255, -0.0475])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0255, -0.0286, -0.0189, -0.0312, -0.0461, -0.0238, -0.0232, -0.0129,\n",
      "        -0.0194, -0.0222, -0.0205, -0.0292, -0.0274, -0.0270, -0.0304, -0.0276,\n",
      "        -0.0337, -0.0261, -0.0218, -0.0242, -0.0291, -0.0355, -0.0414, -0.0195,\n",
      "        -0.0254, -0.0463, -0.0281, -0.0843,  0.0000, -0.0078, -0.0265, -0.0409])\n",
      "Q値の教師データ: tensor([-0.0253, -0.0283, -0.0187, -0.0308, -0.0456, -0.0236, -0.0230, -0.0127,\n",
      "        -0.0192, -0.0220, -0.0203, -0.0290, -0.0271, -0.0267, -0.0301, -0.0273,\n",
      "        -0.0334, -0.0259, -0.0216, -0.0240, -0.0288, -0.0352, -0.0410, -0.0193,\n",
      "        -0.0252, -0.0459, -0.0278, -0.0835, -1.0000, -0.0078, -0.0262, -0.0405])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0294, -0.0221, -0.0171, -0.0221, -0.0204, -0.0327, -0.0755, -0.0454,\n",
      "         0.0000, -0.0351, -0.0151, -0.0355, -0.0299, -0.0277, -0.0298, -0.0285,\n",
      "         0.0000, -0.0258, -0.0400, -0.0716, -0.0234,  0.0000, -0.0226, -0.1167,\n",
      "        -0.0297, -0.0454, -0.0246, -0.0454, -0.0196, -0.0163, -0.0221, -0.0252])\n",
      "Q値の教師データ: tensor([-0.0291, -0.0219, -0.0169, -0.0219, -0.0202, -0.0323, -0.0748, -0.0449,\n",
      "        -1.0000, -0.0348, -0.0150, -0.0351, -0.0296, -0.0275, -0.0295, -0.0283,\n",
      "        -1.0000, -0.0255, -0.0396, -0.0709, -0.0232, -1.0000, -0.0224, -0.1155,\n",
      "        -0.0294, -0.0449, -0.0243, -0.0450, -0.0194, -0.0161, -0.0218, -0.0250])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.])\n",
      "次状態の最大Q値: tensor([-0.1140, -0.0335, -0.0307, -0.0226, -0.0448, -0.0442, -0.0198, -0.0454,\n",
      "        -0.0214, -0.0288, -0.0664, -0.1079, -0.0174, -0.0239, -0.0473, -0.0282,\n",
      "        -0.0309, -0.0174, -0.0280, -0.0442, -0.0531, -0.0293, -0.0303, -0.0388,\n",
      "        -0.0311, -0.0264, -0.0483, -0.0357, -0.0243, -0.0222,  0.0000, -0.0363])\n",
      "Q値の教師データ: tensor([-0.1128, -0.0332, -0.0304, -0.0224, -0.0443, -0.0437, -0.0196, -0.0450,\n",
      "        -0.0212, -0.0285, -0.0657, -0.1069, -0.0173, -0.0237, -0.0468, -0.0279,\n",
      "        -0.0305, -0.0173, -0.0277, -0.0437, -0.0526, -0.0290, -0.0300, -0.0384,\n",
      "        -0.0308, -0.0262, -0.0478, -0.0354, -0.0241, -0.0220, -1.0000, -0.0360])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0250, -0.0246, -0.0280, -0.0331, -0.0492, -0.1176, -0.0235,  0.0000,\n",
      "        -0.1096, -0.0469, -0.0316,  0.0000, -0.0381, -0.0222, -0.0299, -0.0233,\n",
      "        -0.0531, -0.0489, -0.0890, -0.0427, -0.0244, -0.0228, -0.0457, -0.0266,\n",
      "        -0.0241, -0.0244, -0.0197, -0.0322, -0.0474, -0.0364, -0.0225, -0.0441])\n",
      "Q値の教師データ: tensor([-0.0247, -0.0243, -0.0277, -0.0327, -0.0487, -0.1164, -0.0233, -1.0000,\n",
      "        -0.1085, -0.0464, -0.0313, -1.0000, -0.0377, -0.0220, -0.0296, -0.0231,\n",
      "        -0.0525, -0.0484, -0.0881, -0.0423, -0.0241, -0.0226, -0.0452, -0.0264,\n",
      "        -0.0239, -0.0242, -0.0195, -0.0319, -0.0469, -0.0361, -0.0222, -0.0436])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0161, -0.0234, -0.0300, -0.0198, -0.0415, -0.0278, -0.0280, -0.0448,\n",
      "        -0.0401, -0.0105, -0.1218, -0.0331, -0.0225, -0.0502, -0.0213, -0.0484,\n",
      "        -0.0247, -0.0266, -0.1088, -0.0310, -0.0676, -0.0415, -0.0696, -0.0273,\n",
      "        -0.0452, -0.0243, -0.0397,  0.0000, -0.0322, -0.0478, -0.0358, -0.0213])\n",
      "Q値の教師データ: tensor([-0.0159, -0.0232, -0.0297, -0.0196, -0.0411, -0.0276, -0.0277, -0.0444,\n",
      "        -0.0397, -0.0104, -0.1206, -0.0328, -0.0222, -0.0497, -0.0211, -0.0479,\n",
      "        -0.0244, -0.0264, -0.1077, -0.0307, -0.0669, -0.0411, -0.0689, -0.0270,\n",
      "        -0.0447, -0.0241, -0.0393, -1.0000, -0.0319, -0.0473, -0.0354, -0.0211])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0257, -0.0240, -0.0472, -0.0319,  0.0000, -0.0263, -0.0234, -0.0231,\n",
      "        -0.0273, -0.0231, -0.0483, -0.0287, -0.0176, -0.0347, -0.0327, -0.0415,\n",
      "         0.0000, -0.0373, -0.0214, -0.0291, -0.0425, -0.0246, -0.0454,  0.0000,\n",
      "        -0.0472, -0.0453,  0.0000, -0.0324, -0.0243, -0.0233, -0.0327, -0.0203])\n",
      "Q値の教師データ: tensor([-0.0255, -0.0238, -0.0467, -0.0316, -1.0000, -0.0260, -0.0232, -0.0229,\n",
      "        -0.0270, -0.0229, -0.0479, -0.0284, -0.0174, -0.0344, -0.0324, -0.0411,\n",
      "        -1.0000, -0.0369, -0.0212, -0.0288, -0.0421, -0.0243, -0.0449, -1.0000,\n",
      "        -0.0467, -0.0449, -1.0000, -0.0320, -0.0241, -0.0230, -0.0324, -0.0201])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0456, -0.0331, -0.0222, -0.0210,  0.0000, -0.0560,  0.0000, -0.0207,\n",
      "        -0.0242, -0.0327, -0.0279, -0.0223, -0.0224, -0.0390, -0.0243,  0.0000,\n",
      "        -0.0271, -0.0728, -0.0226, -0.0323, -0.0298, -0.0276, -0.0192, -0.0316,\n",
      "        -0.0300, -0.0251, -0.1301,  0.0000, -0.0500, -0.0288, -0.0476, -0.0332])\n",
      "Q値の教師データ: tensor([-0.0452, -0.0328, -0.0220, -0.0208, -1.0000, -0.0555, -1.0000, -0.0205,\n",
      "        -0.0240, -0.0324, -0.0276, -0.0220, -0.0221, -0.0386, -0.0241, -1.0000,\n",
      "        -0.0268, -0.0721, -0.0224, -0.0320, -0.0295, -0.0274, -0.0190, -0.0313,\n",
      "        -0.0297, -0.0249, -0.1288, -1.0000, -0.0495, -0.0285, -0.0472, -0.0329])\n",
      "reward_batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0211, -0.0483, -0.0275,  0.0000, -0.0282, -0.0319, -0.0281, -0.0248,\n",
      "        -0.0207,  0.0000, -0.0278, -0.0227, -0.0326, -0.0351, -0.0210, -0.0387,\n",
      "        -0.0207, -0.0330, -0.0248, -0.0299, -0.0378, -0.0207, -0.0279, -0.0443,\n",
      "        -0.0477, -0.0308, -0.0233, -0.0495,  0.0000, -0.0286, -0.1036, -0.1030])\n",
      "Q値の教師データ: tensor([-0.0208, -0.0478, -0.0272, -1.0000, -0.0279, -0.0316, -0.0278, -0.0245,\n",
      "        -0.0205, -1.0000, -0.0276, -0.0225, -0.0322, -0.0347, -0.0208, -0.0383,\n",
      "        -0.0205, -0.0327, -0.0246, -0.0296, -0.0374, -0.0205, -0.0276, -0.0439,\n",
      "        -0.0472, -0.0305, -0.0230, -0.0490, -1.0000, -0.0283, -0.1025, -0.1020])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0239, -0.0461, -0.0575, -0.0393, -0.0382, -0.0226, -0.0250, -0.0212,\n",
      "        -0.0321, -0.0589, -0.0224, -0.0328, -0.0260, -0.0282, -0.0438, -0.0362,\n",
      "        -0.0490, -0.0244, -0.0511,  0.0000, -0.0262, -0.0497, -0.0121, -0.0297,\n",
      "         0.0000, -0.0182, -0.0323, -0.0500, -0.0219,  0.0000, -0.0470, -0.0485])\n",
      "Q値の教師データ: tensor([-0.0236, -0.0457, -0.0570, -0.0389, -0.0378, -0.0223, -0.0247, -0.0210,\n",
      "        -0.0317, -0.0583, -0.0222, -0.0325, -0.0258, -0.0279, -0.0434, -0.0359,\n",
      "        -0.0485, -0.0241, -0.0506, -1.0000, -0.0260, -0.0492, -0.0119, -0.0294,\n",
      "        -1.0000, -0.0180, -0.0319, -0.0495, -0.0217, -1.0000, -0.0466, -0.0480])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0256, -0.0334, -0.0412, -0.0261, -0.0310,  0.0000, -0.0306, -0.0194,\n",
      "        -0.0511, -0.0260, -0.0284, -0.0951, -0.0366, -0.0296, -0.0477, -0.0308,\n",
      "        -0.0326, -0.0337, -0.0318, -0.0550, -0.0178, -0.1208,  0.0000, -0.0308,\n",
      "        -0.0341, -0.0430, -0.0271, -0.0392, -0.0327, -0.0313, -0.0346, -0.0687])\n",
      "Q値の教師データ: tensor([-0.0253, -0.0331, -0.0408, -0.0259, -0.0307, -1.0000, -0.0303, -0.0192,\n",
      "        -0.0506, -0.0257, -0.0282, -0.0941, -0.0362, -0.0293, -0.0472, -0.0305,\n",
      "        -0.0323, -0.0334, -0.0315, -0.0545, -0.0177, -0.1196, -1.0000, -0.0305,\n",
      "        -0.0337, -0.0426, -0.0268, -0.0388, -0.0323, -0.0309, -0.0342, -0.0680])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0473, -0.0507,  0.0000, -0.0313, -0.0421, -0.0307, -0.0456, -0.0232,\n",
      "        -0.0749, -0.0422, -0.0503, -0.0387, -0.0742, -0.0347, -0.0290, -0.0337,\n",
      "        -0.0293, -0.0583, -0.0239, -0.1080, -0.0267, -0.0439, -0.0453, -0.0347,\n",
      "        -0.0787, -0.0267, -0.0300, -0.0330, -0.0322, -0.0311, -0.0362,  0.0000])\n",
      "Q値の教師データ: tensor([-0.0469, -0.0502, -1.0000, -0.0309, -0.0417, -0.0303, -0.0451, -0.0229,\n",
      "        -0.0742, -0.0417, -0.0498, -0.0383, -0.0734, -0.0344, -0.0287, -0.0334,\n",
      "        -0.0290, -0.0577, -0.0237, -0.1070, -0.0264, -0.0434, -0.0449, -0.0343,\n",
      "        -0.0780, -0.0265, -0.0297, -0.0327, -0.0319, -0.0308, -0.0359, -1.0000])\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0527,  0.0000, -0.0394, -0.0325, -0.0234, -0.0271, -0.0245, -0.0272,\n",
      "        -0.0907, -0.0430, -0.0325, -0.0468, -0.0243, -0.0299, -0.0305, -0.0296,\n",
      "        -0.0436, -0.0292, -0.0323, -0.0296, -0.0397, -0.0508, -0.0279, -0.0249,\n",
      "        -0.0292, -0.0332, -0.0307, -0.0933, -0.0411, -0.0270, -0.0239, -0.1022])\n",
      "Q値の教師データ: tensor([-0.0522, -1.0000, -0.0390, -0.0322, -0.0232, -0.0268, -0.0243, -0.0269,\n",
      "        -0.0898, -0.0425, -0.0322, -0.0464, -0.0241, -0.0296, -0.0302, -0.0293,\n",
      "        -0.0432, -0.0289, -0.0320, -0.0293, -0.0393, -0.0503, -0.0276, -0.0247,\n",
      "        -0.0289, -0.0329, -0.0304, -0.0923, -0.0407, -0.0267, -0.0237, -0.1012])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0255, -0.0322, -0.0261, -0.0559, -0.0475, -0.0273, -0.0516, -0.0412,\n",
      "        -0.0425, -0.0363, -0.0401, -0.1125, -0.1058, -0.0316, -0.0283, -0.0228,\n",
      "        -0.0230, -0.0234, -0.0329, -0.0279, -0.0458, -0.0496, -0.0590, -0.0713,\n",
      "        -0.0328, -0.0308, -0.0936, -0.0303, -0.0320, -0.0266, -0.0299,  0.0000])\n",
      "Q値の教師データ: tensor([-0.0252, -0.0319, -0.0259, -0.0553, -0.0471, -0.0270, -0.0511, -0.0408,\n",
      "        -0.0421, -0.0359, -0.0397, -0.1114, -0.1048, -0.0313, -0.0280, -0.0226,\n",
      "        -0.0228, -0.0232, -0.0326, -0.0276, -0.0453, -0.0492, -0.0584, -0.0706,\n",
      "        -0.0324, -0.0305, -0.0927, -0.0300, -0.0317, -0.0263, -0.0296, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0335, -0.0483, -0.0401, -0.0335, -0.0303, -0.1129, -0.0406, -0.0856,\n",
      "        -0.0415, -0.0519, -0.0275, -0.0592, -0.0272, -0.0302, -0.0405, -0.0341,\n",
      "        -0.0259, -0.0327, -0.0200, -0.0615, -0.0230,  0.0000, -0.0797, -0.0354,\n",
      "        -0.0501, -0.0192, -0.0273, -0.0460, -0.0293, -0.0390, -0.0514, -0.0448])\n",
      "Q値の教師データ: tensor([-0.0332, -0.0478, -0.0397, -0.0331, -0.0300, -0.1117, -0.0402, -0.0847,\n",
      "        -0.0411, -0.0514, -0.0272, -0.0586, -0.0270, -0.0299, -0.0401, -0.0338,\n",
      "        -0.0257, -0.0324, -0.0198, -0.0609, -0.0227, -1.0000, -0.0789, -0.0350,\n",
      "        -0.0496, -0.0190, -0.0270, -0.0455, -0.0290, -0.0387, -0.0508, -0.0443])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0656, -0.0932, -0.0268, -0.0521, -0.0354, -0.0522, -0.0344, -0.0329,\n",
      "        -0.0488, -0.0407,  0.0000, -0.0285, -0.0483, -0.0314, -0.0403, -0.0519,\n",
      "        -0.0383, -0.0519,  0.0000, -0.0523, -0.0481, -0.0528, -0.0243, -0.0334,\n",
      "         0.0000, -0.0382, -0.0467, -0.0219, -0.0459, -0.1113, -0.0281, -0.0315])\n",
      "Q値の教師データ: tensor([-0.0649, -0.0923, -0.0265, -0.0516, -0.0350, -0.0517, -0.0341, -0.0325,\n",
      "        -0.0483, -0.0403, -1.0000, -0.0283, -0.0478, -0.0311, -0.0399, -0.0514,\n",
      "        -0.0379, -0.0514, -1.0000, -0.0518, -0.0476, -0.0523, -0.0241, -0.0330,\n",
      "        -1.0000, -0.0379, -0.0463, -0.0217, -0.0454, -0.1102, -0.0278, -0.0312])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0308, -0.0357, -0.0354, -0.0420,  0.0000, -0.0380, -0.0308, -0.0213,\n",
      "         0.0000, -0.0288, -0.0973, -0.0399, -0.0336, -0.0300, -0.0351, -0.0341,\n",
      "        -0.0492, -0.0289, -0.0706, -0.0473, -0.0259,  0.0000, -0.0802, -0.0441,\n",
      "        -0.0262, -0.0267,  0.0000, -0.0605, -0.0311, -0.0498, -0.0470, -0.0504])\n",
      "Q値の教師データ: tensor([-0.0305, -0.0353, -0.0351, -0.0416, -1.0000, -0.0376, -0.0305, -0.0211,\n",
      "        -1.0000, -0.0286, -0.0963, -0.0395, -0.0332, -0.0297, -0.0348, -0.0338,\n",
      "        -0.0488, -0.0286, -0.0699, -0.0468, -0.0256, -1.0000, -0.0794, -0.0436,\n",
      "        -0.0260, -0.0264, -1.0000, -0.0599, -0.0308, -0.0493, -0.0465, -0.0499])\n",
      "21 Episode: Finished after 30 steps：10試行の平均step数 = 25.5\n",
      "reward_batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0307,  0.0000, -0.0498, -0.0263, -0.0539, -0.0509, -0.0447, -0.0289,\n",
      "        -0.0250, -0.0412, -0.0311, -0.0524, -0.0341, -0.0297, -0.0442, -0.0413,\n",
      "        -0.0335, -0.0264, -0.0295,  0.0000, -0.0338, -0.0334, -0.0282, -0.0497,\n",
      "        -0.0256,  0.0000, -0.0409, -0.0609, -0.0435, -0.1357, -0.0263, -0.0351])\n",
      "Q値の教師データ: tensor([-0.0304, -1.0000, -0.0493, -0.0261, -0.0534, -0.0504, -0.0443, -0.0286,\n",
      "        -0.0247, -0.0408, -0.0308, -0.0519, -0.0337, -0.0294, -0.0438, -0.0409,\n",
      "        -0.0332, -0.0261, -0.0292, -1.0000, -0.0335, -0.0330, -0.0279, -0.0492,\n",
      "        -0.0253, -1.0000, -0.0405, -0.0603, -0.0431, -0.1343, -0.0260, -0.0348])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.1245, -0.0278,  0.0000, -0.0868, -0.0301, -0.0563, -0.0693, -0.0321,\n",
      "        -0.0437, -0.0514, -0.0269, -0.0429, -0.0772, -0.0276, -0.0267, -0.0437,\n",
      "        -0.0266, -0.0288, -0.0226, -0.0211, -0.0285, -0.0287, -0.0350, -0.0365,\n",
      "        -0.0302,  0.0000, -0.0494, -0.0628, -0.1201, -0.0500, -0.0316, -0.0413])\n",
      "Q値の教師データ: tensor([-0.1232, -0.0275, -1.0000, -0.0859, -0.0298, -0.0558, -0.0687, -0.0318,\n",
      "        -0.0433, -0.0509, -0.0267, -0.0425, -0.0764, -0.0274, -0.0265, -0.0433,\n",
      "        -0.0264, -0.0285, -0.0224, -0.0209, -0.0282, -0.0284, -0.0346, -0.0361,\n",
      "        -0.0299, -1.0000, -0.0489, -0.0622, -0.1189, -0.0495, -0.0313, -0.0409])\n",
      "reward_batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0326, -0.0516,  0.0000, -0.0348, -0.0273, -0.1073, -0.0230, -0.0499,\n",
      "        -0.0261, -0.0368, -0.0297, -0.0338, -0.0502, -0.0311, -0.0324, -0.0316,\n",
      "        -0.0283, -0.0354, -0.0299, -0.0294, -0.0436, -0.0551, -0.0235, -0.0281,\n",
      "        -0.0467, -0.0363, -0.0261,  0.0000, -0.0282, -0.0576, -0.0222,  0.0000])\n",
      "Q値の教師データ: tensor([-0.0323, -0.0511, -1.0000, -0.0345, -0.0270, -0.1062, -0.0228, -0.0494,\n",
      "        -0.0258, -0.0365, -0.0294, -0.0335, -0.0497, -0.0308, -0.0321, -0.0313,\n",
      "        -0.0280, -0.0350, -0.0296, -0.0291, -0.0432, -0.0546, -0.0232, -0.0278,\n",
      "        -0.0463, -0.0359, -0.0258, -1.0000, -0.0279, -0.0570, -0.0220, -1.0000])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0265, -0.0342, -0.0773, -0.0305, -0.0433, -0.0394, -0.0273, -0.0935,\n",
      "        -0.0389, -0.0816, -0.0317, -0.0263, -0.0503, -0.0358, -0.0162, -0.0600,\n",
      "        -0.0271, -0.0552, -0.0541, -0.0367, -0.0252, -0.0328, -0.0441, -0.0277,\n",
      "        -0.0303, -0.0460, -0.0499, -0.0354, -0.0302, -0.0300, -0.0540, -0.0284])\n",
      "Q値の教師データ: tensor([-0.0262, -0.0339, -0.0765, -0.0302, -0.0429, -0.0390, -0.0271, -0.0926,\n",
      "        -0.0385, -0.0808, -0.0313, -0.0261, -0.0498, -0.0354, -0.0160, -0.0594,\n",
      "        -0.0268, -0.0547, -0.0535, -0.0364, -0.0249, -0.0325, -0.0437, -0.0274,\n",
      "        -0.0300, -0.0455, -0.0494, -0.0350, -0.0299, -0.0297, -0.0535, -0.0281])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0311, -0.0268, -0.0540, -0.1119, -0.0283,  0.0000, -0.0213, -0.0301,\n",
      "        -0.0457, -0.0371, -0.0337, -0.0993, -0.0365, -0.0361, -0.0340,  0.0000,\n",
      "        -0.0292, -0.0583, -0.0375, -0.0475, -0.0627, -0.0314, -0.0622, -0.0471,\n",
      "        -0.0526, -0.0320, -0.0399, -0.0327, -0.0327, -0.0355, -0.0300, -0.0335])\n",
      "Q値の教師データ: tensor([-0.0308, -0.0265, -0.0534, -0.1107, -0.0280, -1.0000, -0.0211, -0.0298,\n",
      "        -0.0452, -0.0367, -0.0334, -0.0983, -0.0361, -0.0357, -0.0337, -1.0000,\n",
      "        -0.0289, -0.0578, -0.0371, -0.0470, -0.0620, -0.0311, -0.0616, -0.0466,\n",
      "        -0.0521, -0.0317, -0.0395, -0.0324, -0.0324, -0.0352, -0.0297, -0.0332])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0511, -0.0493, -0.0822, -0.0302, -0.0371, -0.0589, -0.0277, -0.0293,\n",
      "         0.0000, -0.0344, -0.0676,  0.0000, -0.0280, -0.0453, -0.0375, -0.0318,\n",
      "        -0.0548, -0.0314, -0.0355, -0.0409, -0.0365,  0.0000, -0.0623, -0.0918,\n",
      "        -0.0357, -0.0280, -0.0331, -0.0140, -0.0226, -0.0984, -0.1305, -0.0307])\n",
      "Q値の教師データ: tensor([-0.0506, -0.0488, -0.0814, -0.0299, -0.0368, -0.0583, -0.0274, -0.0290,\n",
      "        -1.0000, -0.0341, -0.0669, -1.0000, -0.0277, -0.0448, -0.0371, -0.0314,\n",
      "        -0.0542, -0.0311, -0.0351, -0.0405, -0.0361, -1.0000, -0.0617, -0.0909,\n",
      "        -0.0354, -0.0277, -0.0328, -0.0139, -0.0224, -0.0974, -0.1292, -0.0304])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0528, -0.0471, -0.0459, -0.0311, -0.0332, -0.0320, -0.0276, -0.0489,\n",
      "        -0.0355,  0.0000, -0.0384, -0.0325, -0.0292, -0.0332, -0.0350, -0.0481,\n",
      "        -0.0372, -0.0390, -0.0564, -0.0413, -0.0418, -0.0308, -0.0353, -0.0357,\n",
      "        -0.0570, -0.0277,  0.0000, -0.0306, -0.0293, -0.0532, -0.0320, -0.0252])\n",
      "Q値の教師データ: tensor([-0.0522, -0.0467, -0.0454, -0.0308, -0.0329, -0.0316, -0.0273, -0.0484,\n",
      "        -0.0352, -1.0000, -0.0380, -0.0321, -0.0289, -0.0329, -0.0346, -0.0476,\n",
      "        -0.0368, -0.0386, -0.0559, -0.0409, -0.0414, -0.0305, -0.0350, -0.0353,\n",
      "        -0.0564, -0.0275, -1.0000, -0.0303, -0.0290, -0.0527, -0.0317, -0.0250])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0362, -0.0365, -0.0290, -0.0366, -0.1146, -0.0500, -0.0372, -0.0470,\n",
      "        -0.0352, -0.0761, -0.0366, -0.1064, -0.0449, -0.0468, -0.0379, -0.0476,\n",
      "        -0.0370, -0.0504, -0.0280, -0.0296, -0.0293, -0.0508, -0.0212, -0.0353,\n",
      "        -0.0427,  0.0000, -0.0372, -0.0487, -0.0279, -0.0342, -0.0572, -0.0827])\n",
      "Q値の教師データ: tensor([-0.0358, -0.0361, -0.0287, -0.0362, -0.1134, -0.0495, -0.0368, -0.0465,\n",
      "        -0.0348, -0.0753, -0.0363, -0.1053, -0.0444, -0.0464, -0.0375, -0.0472,\n",
      "        -0.0367, -0.0499, -0.0277, -0.0294, -0.0290, -0.0503, -0.0209, -0.0349,\n",
      "        -0.0422, -1.0000, -0.0369, -0.0482, -0.0277, -0.0339, -0.0566, -0.0819])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0370, -0.0390, -0.0394, -0.0321, -0.0544, -0.0369, -0.0430, -0.0202,\n",
      "        -0.0436, -0.0363, -0.0594, -0.0297, -0.0467, -0.0299, -0.0788, -0.0329,\n",
      "        -0.0340, -0.1185, -0.0581, -0.0554, -0.0496, -0.0404,  0.0000, -0.0748,\n",
      "        -0.0357, -0.0320, -0.0530, -0.0302, -0.0362, -0.0300, -0.0393, -0.0484])\n",
      "Q値の教師データ: tensor([-0.0366, -0.0386, -0.0390, -0.0318, -0.0538, -0.0365, -0.0426, -0.0200,\n",
      "        -0.0432, -0.0360, -0.0588, -0.0294, -0.0463, -0.0296, -0.0780, -0.0326,\n",
      "        -0.0336, -0.1173, -0.0575, -0.0549, -0.0491, -0.0400, -1.0000, -0.0741,\n",
      "        -0.0353, -0.0317, -0.0525, -0.0299, -0.0358, -0.0297, -0.0389, -0.0479])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0343, -0.0929, -0.0295, -0.0354, -0.0272, -0.0571, -0.0995, -0.0325,\n",
      "        -0.0557, -0.0525, -0.0283, -0.0303, -0.1138, -0.0488, -0.0328, -0.0204,\n",
      "        -0.0297, -0.0415, -0.0584, -0.0524, -0.0400, -0.0380, -0.0389, -0.0893,\n",
      "        -0.0416, -0.0387, -0.1069, -0.0393, -0.0284, -0.0685, -0.0510, -0.0328])\n",
      "Q値の教師データ: tensor([-0.0339, -0.0920, -0.0292, -0.0350, -0.0269, -0.0565, -0.0986, -0.0322,\n",
      "        -0.0551, -0.0520, -0.0280, -0.0300, -0.1127, -0.0483, -0.0325, -0.0202,\n",
      "        -0.0294, -0.0410, -0.0579, -0.0519, -0.0396, -0.0377, -0.0385, -0.0884,\n",
      "        -0.0412, -0.0383, -0.1058, -0.0389, -0.0282, -0.0679, -0.0505, -0.0324])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0371, -0.0347, -0.0375, -0.0495, -0.0353, -0.0334, -0.0478, -0.0533,\n",
      "        -0.0449, -0.0558, -0.0575, -0.0575, -0.0423, -0.0514, -0.0382, -0.0264,\n",
      "        -0.0458, -0.0509, -0.0290, -0.0399, -0.0366, -0.0391, -0.0565, -0.0321,\n",
      "        -0.0297, -0.0658, -0.0316, -0.0497, -0.0480, -0.0768, -0.0346, -0.0383])\n",
      "Q値の教師データ: tensor([-0.0367, -0.0343, -0.0371, -0.0490, -0.0349, -0.0330, -0.0473, -0.0527,\n",
      "        -0.0444, -0.0553, -0.0569, -0.0569, -0.0418, -0.0509, -0.0378, -0.0261,\n",
      "        -0.0454, -0.0504, -0.0287, -0.0395, -0.0362, -0.0387, -0.0559, -0.0318,\n",
      "        -0.0294, -0.0651, -0.0313, -0.0492, -0.0476, -0.0760, -0.0342, -0.0379])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0333, -0.0353, -0.0287, -0.0357, -0.0283, -0.0570, -0.0740,  0.0000,\n",
      "        -0.0432, -0.0362, -0.0305, -0.0368, -0.0300, -0.0309, -0.0347, -0.0348,\n",
      "        -0.0343, -0.1192, -0.0325, -0.0373, -0.0390, -0.0438, -0.0305, -0.0405,\n",
      "        -0.0322, -0.0397, -0.0564, -0.0298, -0.0322, -0.0577, -0.0384, -0.0444])\n",
      "Q値の教師データ: tensor([-0.0329, -0.0349, -0.0284, -0.0354, -0.0280, -0.0564, -0.0733, -1.0000,\n",
      "        -0.0428, -0.0358, -0.0301, -0.0364, -0.0297, -0.0305, -0.0344, -0.0345,\n",
      "        -0.0340, -0.1180, -0.0322, -0.0369, -0.0386, -0.0433, -0.0302, -0.0401,\n",
      "        -0.0319, -0.0393, -0.0558, -0.0295, -0.0319, -0.0571, -0.0381, -0.0440])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0545, -0.0568, -0.0364, -0.0309, -0.0640, -0.0386, -0.0571, -0.0564,\n",
      "        -0.0352, -0.0599, -0.0391, -0.0360, -0.0314, -0.0414, -0.0338, -0.0356,\n",
      "        -0.0535, -0.0291, -0.0355, -0.0446, -0.0384, -0.0316, -0.0533, -0.0985,\n",
      "        -0.0303, -0.0320, -0.0405, -0.0377, -0.0379, -0.1250, -0.0586, -0.0404])\n",
      "Q値の教師データ: tensor([-0.0539, -0.0562, -0.0360, -0.0306, -0.0633, -0.0382, -0.0565, -0.0558,\n",
      "        -0.0348, -0.0593, -0.0387, -0.0356, -0.0311, -0.0410, -0.0335, -0.0353,\n",
      "        -0.0530, -0.0289, -0.0351, -0.0442, -0.0380, -0.0313, -0.0528, -0.0975,\n",
      "        -0.0300, -0.0317, -0.0401, -0.0373, -0.0375, -0.1237, -0.0580, -0.0400])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.1242, -0.1160, -0.0360, -0.0448, -0.0297, -0.0311, -0.0389, -0.0595,\n",
      "        -0.0496,  0.0000, -0.0663, -0.0371, -0.0535, -0.0794, -0.1104, -0.0358,\n",
      "         0.0000, -0.0172, -0.0346, -0.0976,  0.0000, -0.0321, -0.0573, -0.0602,\n",
      "        -0.0322, -0.0405, -0.0369, -0.0491, -0.0496, -0.0476, -0.0641, -0.0369])\n",
      "Q値の教師データ: tensor([-0.1230, -0.1148, -0.0357, -0.0444, -0.0294, -0.0308, -0.0385, -0.0590,\n",
      "        -0.0491, -1.0000, -0.0657, -0.0367, -0.0530, -0.0786, -0.1093, -0.0354,\n",
      "        -1.0000, -0.0170, -0.0343, -0.0966, -1.0000, -0.0318, -0.0567, -0.0596,\n",
      "        -0.0319, -0.0401, -0.0365, -0.0486, -0.0491, -0.0471, -0.0635, -0.0365])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0311, -0.0303, -0.0332, -0.0353, -0.0377, -0.0553, -0.0430, -0.0533,\n",
      "        -0.0629, -0.0408, -0.0388, -0.0588, -0.0575,  0.0000, -0.0378, -0.0407,\n",
      "        -0.0160, -0.0487, -0.0359, -0.0519, -0.1245, -0.0298, -0.0497, -0.0292,\n",
      "        -0.0456, -0.0394, -0.0353, -0.0381, -0.0488, -0.0402, -0.0456, -0.0347])\n",
      "Q値の教師データ: tensor([-0.0307, -0.0300, -0.0328, -0.0350, -0.0373, -0.0547, -0.0425, -0.0527,\n",
      "        -0.0623, -0.0404, -0.0385, -0.0582, -0.0569, -1.0000, -0.0374, -0.0403,\n",
      "        -0.0158, -0.0482, -0.0355, -0.0514, -0.1232, -0.0295, -0.0492, -0.0290,\n",
      "        -0.0451, -0.0390, -0.0350, -0.0378, -0.0483, -0.0398, -0.0451, -0.0344])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0341, -0.0336, -0.0297, -0.0353, -0.0333, -0.0306, -0.0292,  0.0000,\n",
      "        -0.0400, -0.0265, -0.0329, -0.0375, -0.0570, -0.0511, -0.0300, -0.0395,\n",
      "        -0.0161, -0.0315,  0.0000, -0.0545, -0.0747, -0.0331, -0.0337,  0.0000,\n",
      "        -0.0294,  0.0000, -0.0589, -0.0405,  0.0000, -0.0392, -0.0625, -0.0979])\n",
      "Q値の教師データ: tensor([-0.0337, -0.0333, -0.0294, -0.0350, -0.0330, -0.0303, -0.0289, -1.0000,\n",
      "        -0.0396, -0.0262, -0.0326, -0.0371, -0.0564, -0.0506, -0.0297, -0.0391,\n",
      "        -0.0160, -0.0312, -1.0000, -0.0540, -0.0739, -0.0327, -0.0334, -1.0000,\n",
      "        -0.0291, -1.0000, -0.0583, -0.0401, -1.0000, -0.0388, -0.0619, -0.0969])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0579, -0.0376, -0.0566, -0.0466, -0.0591, -0.0343, -0.0310, -0.0572,\n",
      "        -0.0309, -0.0316, -0.0220, -0.0539, -0.0525, -0.0601, -0.0298, -0.0302,\n",
      "        -0.0335,  0.0000, -0.0404, -0.0291, -0.1111, -0.0191, -0.0489, -0.0256,\n",
      "         0.0000, -0.0351, -0.0316, -0.0302, -0.0458, -0.0385, -0.0294, -0.0368])\n",
      "Q値の教師データ: tensor([-0.0573, -0.0373, -0.0561, -0.0461, -0.0585, -0.0340, -0.0307, -0.0566,\n",
      "        -0.0306, -0.0313, -0.0218, -0.0533, -0.0520, -0.0595, -0.0295, -0.0299,\n",
      "        -0.0332, -1.0000, -0.0400, -0.0288, -0.1100, -0.0190, -0.0484, -0.0253,\n",
      "        -1.0000, -0.0348, -0.0313, -0.0299, -0.0454, -0.0381, -0.0291, -0.0365])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0780, -0.0486, -0.0329, -0.0504, -0.0392, -0.0605,  0.0000, -0.0550,\n",
      "        -0.0361, -0.0618, -0.0378, -0.0594, -0.0348, -0.0366, -0.0398, -0.0335,\n",
      "        -0.0314, -0.0429, -0.0518, -0.0317, -0.0166, -0.0295, -0.0407, -0.0406,\n",
      "        -0.0365,  0.0000, -0.0527, -0.0277, -0.0367, -0.0391, -0.0569, -0.0671])\n",
      "Q値の教師データ: tensor([-0.0773, -0.0481, -0.0326, -0.0498, -0.0388, -0.0599, -1.0000, -0.0545,\n",
      "        -0.0357, -0.0611, -0.0374, -0.0588, -0.0345, -0.0363, -0.0394, -0.0331,\n",
      "        -0.0311, -0.0424, -0.0513, -0.0314, -0.0164, -0.0292, -0.0403, -0.0402,\n",
      "        -0.0361, -1.0000, -0.0522, -0.0274, -0.0363, -0.0387, -0.0564, -0.0664])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0322, -0.0319, -0.0431, -0.0339, -0.0534, -0.0504, -0.0364, -0.0369,\n",
      "        -0.0500, -0.0290, -0.0366, -0.0541, -0.0433, -0.0575, -0.0806,  0.0000,\n",
      "        -0.0484, -0.0332, -0.0388, -0.0768, -0.0265, -0.1175, -0.0642, -0.0405,\n",
      "        -0.0414, -0.0331, -0.0577, -0.1210, -0.0367, -0.0313, -0.0310, -0.0536])\n",
      "Q値の教師データ: tensor([-0.0319, -0.0316, -0.0427, -0.0336, -0.0529, -0.0499, -0.0360, -0.0366,\n",
      "        -0.0495, -0.0288, -0.0362, -0.0536, -0.0429, -0.0569, -0.0798, -1.0000,\n",
      "        -0.0479, -0.0329, -0.0384, -0.0760, -0.0263, -0.1163, -0.0635, -0.0401,\n",
      "        -0.0410, -0.0328, -0.0571, -0.1198, -0.0363, -0.0310, -0.0307, -0.0531])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0582, -0.0417, -0.0337, -0.0324, -0.0311, -0.0390, -0.0325, -0.0355,\n",
      "        -0.0576, -0.0483, -0.0625, -0.0341, -0.0333, -0.0771, -0.0705, -0.0363,\n",
      "        -0.0530, -0.0506, -0.0354, -0.0411, -0.0399,  0.0000, -0.1312, -0.0329,\n",
      "         0.0000, -0.0486, -0.0409, -0.0342, -0.0398, -0.0258, -0.0394, -0.0485])\n",
      "Q値の教師データ: tensor([-0.0576, -0.0412, -0.0333, -0.0320, -0.0308, -0.0386, -0.0322, -0.0351,\n",
      "        -0.0571, -0.0478, -0.0618, -0.0338, -0.0330, -0.0764, -0.0698, -0.0360,\n",
      "        -0.0524, -0.0501, -0.0350, -0.0407, -0.0395, -1.0000, -0.1299, -0.0326,\n",
      "        -1.0000, -0.0481, -0.0405, -0.0338, -0.0394, -0.0256, -0.0390, -0.0480])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0302, -0.0519, -0.0469, -0.0344, -0.1023,  0.0000, -0.0551, -0.0559,\n",
      "        -0.0648, -0.0308, -0.0426, -0.0378, -0.0361, -0.0393, -0.0362, -0.0399,\n",
      "        -0.0428, -0.0353,  0.0000, -0.0343, -0.0466, -0.0375, -0.0464, -0.0418,\n",
      "        -0.0980, -0.0708, -0.0467,  0.0000, -0.0401, -0.0372, -0.0328, -0.0389])\n",
      "Q値の教師データ: tensor([-0.0299, -0.0514, -0.0464, -0.0340, -0.1013, -1.0000, -0.0545, -0.0553,\n",
      "        -0.0642, -0.0305, -0.0422, -0.0375, -0.0357, -0.0389, -0.0359, -0.0395,\n",
      "        -0.0424, -0.0350, -1.0000, -0.0340, -0.0461, -0.0371, -0.0459, -0.0414,\n",
      "        -0.0970, -0.0701, -0.0462, -1.0000, -0.0397, -0.0369, -0.0325, -0.0385])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0489, -0.0400, -0.0359, -0.0367, -0.0398,  0.0000, -0.0346, -0.0319,\n",
      "         0.0000, -0.0441, -0.0431, -0.0620, -0.0508, -0.0524, -0.0535, -0.0417,\n",
      "        -0.0244,  0.0000, -0.0566, -0.0598, -0.0336, -0.0352, -0.0359, -0.1323,\n",
      "        -0.0421, -0.0322, -0.0598, -0.0363, -0.0422, -0.0405, -0.0396, -0.0413])\n",
      "Q値の教師データ: tensor([-0.0485, -0.0396, -0.0355, -0.0363, -0.0394, -1.0000, -0.0342, -0.0316,\n",
      "        -1.0000, -0.0436, -0.0427, -0.0614, -0.0503, -0.0519, -0.0529, -0.0412,\n",
      "        -0.0242, -1.0000, -0.0561, -0.0592, -0.0332, -0.0349, -0.0355, -0.1310,\n",
      "        -0.0416, -0.0319, -0.0592, -0.0359, -0.0418, -0.0401, -0.0392, -0.0409])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.])\n",
      "次状態の最大Q値: tensor([-0.0479, -0.0423, -0.0797, -0.0446, -0.0342, -0.0466, -0.0763, -0.0285,\n",
      "        -0.0598, -0.0491, -0.0437,  0.0000, -0.0401, -0.0418, -0.0339, -0.0434,\n",
      "        -0.0418, -0.0359, -0.0362, -0.0289, -0.0604, -0.0471, -0.0431, -0.0644,\n",
      "        -0.0368, -0.0393, -0.0343, -0.1290, -0.0348, -0.0395, -0.0406,  0.0000])\n",
      "Q値の教師データ: tensor([-0.0474, -0.0419, -0.0789, -0.0442, -0.0338, -0.0462, -0.0755, -0.0282,\n",
      "        -0.0592, -0.0486, -0.0433, -1.0000, -0.0397, -0.0414, -0.0335, -0.0429,\n",
      "        -0.0414, -0.0355, -0.0358, -0.0286, -0.0598, -0.0466, -0.0427, -0.0638,\n",
      "        -0.0364, -0.0389, -0.0340, -0.1277, -0.0345, -0.0391, -0.0402, -1.0000])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0420, -0.0439, -0.0482, -0.0410,  0.0000, -0.0415, -0.0833, -0.0507,\n",
      "        -0.0430, -0.0410, -0.0489, -0.0269, -0.0571, -0.0343, -0.0549, -0.0635,\n",
      "        -0.0406,  0.0000,  0.0000, -0.0397, -0.0334,  0.0000, -0.0395, -0.0515,\n",
      "        -0.0460, -0.0537, -0.0421, -0.0315, -0.0327, -0.1035, -0.0450, -0.0379])\n",
      "Q値の教師データ: tensor([-0.0415, -0.0435, -0.0478, -0.0406, -1.0000, -0.0411, -0.0824, -0.0502,\n",
      "        -0.0426, -0.0405, -0.0484, -0.0267, -0.0565, -0.0339, -0.0543, -0.0628,\n",
      "        -0.0401, -1.0000, -1.0000, -0.0393, -0.0330, -1.0000, -0.0391, -0.0510,\n",
      "        -0.0456, -0.0532, -0.0417, -0.0312, -0.0324, -0.1025, -0.0445, -0.0375])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0384, -0.0380, -0.0616, -0.0555, -0.0361, -0.0379, -0.0474, -0.0560,\n",
      "        -0.0342, -0.0485, -0.0609, -0.0528, -0.0330, -0.0188, -0.0505, -0.0342,\n",
      "        -0.0606, -0.0481, -0.1388,  0.0000, -0.0492, -0.0273, -0.0574, -0.0404,\n",
      "        -0.0611, -0.0358, -0.0324, -0.0372, -0.0363, -0.0524, -0.0597, -0.0389])\n",
      "Q値の教師データ: tensor([-0.0380, -0.0377, -0.0610, -0.0550, -0.0357, -0.0375, -0.0469, -0.0554,\n",
      "        -0.0338, -0.0480, -0.0603, -0.0522, -0.0327, -0.0186, -0.0500, -0.0339,\n",
      "        -0.0600, -0.0476, -0.1374, -1.0000, -0.0487, -0.0270, -0.0569, -0.0400,\n",
      "        -0.0605, -0.0354, -0.0321, -0.0368, -0.0359, -0.0519, -0.0591, -0.0385])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0405, -0.0359, -0.0598, -0.0401, -0.1296, -0.0373, -0.0570, -0.0420,\n",
      "        -0.0406, -0.0403, -0.0443, -0.0511, -0.0412, -0.0617, -0.0438, -0.0390,\n",
      "        -0.0296, -0.0563, -0.0672, -0.0342, -0.1348, -0.0340, -0.0315, -0.1307,\n",
      "        -0.0336, -0.0527, -0.0563,  0.0000, -0.0397, -0.0577, -0.0362, -0.0338])\n",
      "Q値の教師データ: tensor([-0.0401, -0.0355, -0.0592, -0.0397, -0.1283, -0.0369, -0.0564, -0.0416,\n",
      "        -0.0402, -0.0399, -0.0438, -0.0506, -0.0408, -0.0611, -0.0433, -0.0386,\n",
      "        -0.0293, -0.0557, -0.0665, -0.0339, -0.1334, -0.0337, -0.0312, -0.1294,\n",
      "        -0.0332, -0.0522, -0.0558, -1.0000, -0.0393, -0.0572, -0.0358, -0.0334])\n",
      "reward_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "次状態の最大Q値: tensor([-0.0447, -0.0408, -0.0349, -0.1353, -0.0404, -0.0392, -0.0315, -0.1032,\n",
      "        -0.0369, -0.0420, -0.0419, -0.0430, -0.0341, -0.0518, -0.0345, -0.0324,\n",
      "        -0.0668, -0.0440, -0.0489, -0.0543, -0.0355, -0.0389, -0.1195, -0.0432,\n",
      "        -0.0782, -0.0194, -0.0372, -0.0221, -0.0384, -0.0346, -0.0418, -0.0560])\n",
      "Q値の教師データ: tensor([-0.0443, -0.0404, -0.0345, -0.1340, -0.0400, -0.0388, -0.0312, -0.1022,\n",
      "        -0.0366, -0.0416, -0.0414, -0.0426, -0.0338, -0.0513, -0.0341, -0.0321,\n",
      "        -0.0661, -0.0436, -0.0484, -0.0537, -0.0351, -0.0385, -0.1183, -0.0428,\n",
      "        -0.0774, -0.0192, -0.0368, -0.0219, -0.0381, -0.0342, -0.0414, -0.0554])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0412, -0.0348, -0.0362, -0.0405, -0.0672, -0.0629, -0.0563,  0.0000,\n",
      "        -0.1067, -0.0377, -0.0286, -0.0330, -0.0622, -0.0428, -0.0406, -0.0485,\n",
      "        -0.0730, -0.0641, -0.0435, -0.0488, -0.0366, -0.0554, -0.0637, -0.0491,\n",
      "        -0.0426, -0.0640, -0.0391, -0.0518, -0.0300, -0.0503, -0.0350, -0.0449])\n",
      "Q値の教師データ: tensor([-0.0408, -0.0344, -0.0359, -0.0401, -0.0665, -0.0622, -0.0557, -1.0000,\n",
      "        -0.1056, -0.0373, -0.0283, -0.0327, -0.0615, -0.0424, -0.0402, -0.0481,\n",
      "        -0.0723, -0.0635, -0.0431, -0.0483, -0.0363, -0.0549, -0.0630, -0.0486,\n",
      "        -0.0422, -0.0634, -0.0387, -0.0513, -0.0297, -0.0498, -0.0346, -0.0445])\n",
      "reward_batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "次状態の最大Q値: tensor([-0.0629, -0.0350, -0.0515, -0.0544, -0.0430, -0.0386,  0.0000, -0.0611,\n",
      "        -0.0412, -0.0544, -0.0508, -0.0464, -0.0375, -0.0381, -0.0576, -0.0593,\n",
      "        -0.0487, -0.0355, -0.0613, -0.0557, -0.0445, -0.0408, -0.0361, -0.0314,\n",
      "        -0.0658, -0.0420, -0.0356,  0.0000, -0.0446, -0.0413, -0.0485, -0.0373])\n",
      "Q値の教師データ: tensor([-0.0623, -0.0347, -0.0509, -0.0539, -0.0426, -0.0383, -1.0000, -0.0604,\n",
      "        -0.0408, -0.0539, -0.0503, -0.0460, -0.0371, -0.0377, -0.0570, -0.0587,\n",
      "        -0.0482, -0.0351, -0.0607, -0.0551, -0.0441, -0.0404, -0.0358, -0.0311,\n",
      "        -0.0651, -0.0416, -0.0353, -1.0000, -0.0442, -0.0408, -0.0480, -0.0369])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-4bf7f6ad7ab7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# main クラス\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcartpole_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcartpole_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-99-b2ed1e4022bb>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[1;31m# Experience ReplayでQ関数を更新する\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_q_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;31m# 観測の更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-98-64684d8b8c83>\u001b[0m in \u001b[0;36mupdate_q_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_q_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;34m'''Q関数を更新する'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-97-caa74c45dab5>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 勾配をリセット\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# バックプロパゲーションを計算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 結合パラメータを更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecide_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main クラス\n",
    "cartpole_env = Environment()\n",
    "cartpole_env.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
